<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>RoboComp</title>
 <link href="http://robocomp.github.io/atom.xml" rel="self"/>
 <link href="http://robocomp.github.io/"/>
 <updated>2016-05-11T03:35:36+05:30</updated>
 <id>http://robocomp.github.io</id>
 <author>
   <name>RoboComp</name>
   <email></email>
 </author>

 
 <entry>
   <title>GSoC 2016 Ideas</title>
   <link href="http://robocomp.github.io/website/2016/04/25/gsoc16ideas/"/>
   <updated>2016-04-25T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2016/04/25/gsoc16ideas</id>
   <content type="html">&lt;h2 id=&quot;robocomps-dsl-based-code-generator&quot;&gt;RoboComp’s DSL based code generator&lt;/h2&gt;

&lt;p&gt;RoboComp’s components are automatically generated using a tool named robocompdsl which takes as input a text file written in a domain-specific language (DSL) named (Component Description Specific Language). These files describe several characteristics of the components such as the network connections to other components, the programming language in which it is going to be written (currently C++ and Python languages are supported), the kind of user interface it will have (if any) and other options such as if the component will make use of third-party libraries. Given a CDSL file and a path, robocompdsl generates a source code tree ready to be compiled for the component described in the file. In this source code tree there are files which the user will not need to modify (these are called generic files) and files in which the user is supposed to type the code (specific files). Once a component has been generated, only generic files are overwritten upon re-generation, new specific files are created with an additional ‘.new’ extension. This tool allows users to create components very quickly, avoids frequent errors and contributes to keep high quality coding standards.&lt;/p&gt;

&lt;p&gt;Despite the undeniable benefits brought by robocompdsl, there are some important updates to make and new features to introduce. This is one of the goals of this GSoC’16 proposal. In particular, we propose these first three new features:&lt;/p&gt;

&lt;h2 id=&quot;qt5-and-ros-support&quot;&gt;1. Qt5 and ROS support&lt;/h2&gt;

&lt;p&gt;Qt5 support. Currently only Qt4, both in C++ and Python, are supported. Supporting Qt5 would be interesting because the new version of the library is now quite mature, has new features and someday Qt4 will be outdated. Also, there is already support for Qt5 in Python through PyQt5. The work needed for this activity will deal with the necessary modifications in the DSL parsing and code generation.&lt;/p&gt;

&lt;p&gt;ROS support (currently underway). ROS support is a pressing issue mainly due to the high popularity of ROS and the possibility to access a number of robot hardware elements that provide ROS interfaces and other interesting packages provided by research groups and companies. Also, it is the usual way of talking to virtual referees in robotic competitions. Supporting ROS within our framework will allow users to use ROS and RoboComp components seamlessly.&lt;/p&gt;

&lt;p&gt;Technologies involved:  C++, Python, Qt5, ROS&lt;/p&gt;

&lt;p&gt;Mentor: Luis J. Manso
Backup mentor: Marco A. Gutiérrez&lt;/p&gt;

&lt;h2 id=&quot;javascript-support&quot;&gt;2. Javascript support&lt;/h2&gt;

&lt;p&gt;NodeJS component code generation. An interesting diversion from current robotics technologies based on C++ and Python would be the use of Javascript as the language to code some highly concurrent components. Most components combine push-pull and RPC communication models to talk to other components in their graph of processes. Additionally, several more threads are normally used to handle the component’s internal workings. In this activity, the code generator will be extended to include Javascript running in a server as a new target language for components. The main restriction here is that ZeroC releases a version of the Ice middleware running on Node, since the web browser version is already out.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Technologies involved:  C++, Python, Qt5, OSG&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Mentor: Luis J. Manso&lt;/p&gt;

&lt;p&gt;Backup mentor: Marco A. Gutiérrez&lt;/p&gt;

&lt;h2 id=&quot;automatic-code-generation-for-state-machines&quot;&gt;3. Automatic code generation for State Machines&lt;/h2&gt;

&lt;p&gt;Currently, the code generator only generates code for the generic part of the component, including the communications middleware and the binary building machinery. In this activity we want to extend  robocompdsl to allow for the specification of a state machine, as the computing model driving the specific functionality of the component. The component definition language will now include instructions to define a set of states and transitions between them, so a functioning machine using Qt State Machine Framework is automatically instantiated when generated.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Technologies involved:  C++, Python, Qt&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Mentor: Pablo Bustos
Backup mentor: Luis J. Manso&lt;/p&gt;

&lt;h2 id=&quot;interesting-improvements-for-robocomps-inner-workings-dealing-with-installation-deployment-math-libraries-etc&quot;&gt;Interesting improvements for RoboComp’s inner workings dealing with installation, deployment, math libraries, etc.&lt;/h2&gt;

&lt;h2 id=&quot;facilitating-the-deployment-of-robocomp-on-different-platforms&quot;&gt;4. Facilitating the deployment of RoboComp on different platforms&lt;/h2&gt;

&lt;p&gt;Deployment of a complex framework on different, heterogeneous platforms is always a difficult problem. Also, horizontally scaling the deployment of components when more computational resources are needed is also a complicated issue. The idea of this activity is to ease the deployment of the Robocomp framework within several different platforms and to provide tools to decouple the logical graph of processes from the underlying hardware. In order to achieve that the thing needed to be done are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First, a refactoring of the CMake structure within RoboComp is needed so basic requirements for installation are reduced to a minimum. On the same page, this activity should ease the task for new developers to add new libraries, classes, tools or modules to the framework.&lt;/li&gt;
  &lt;li&gt;To facilitate the deployment on different operating systems, a fully functional Docker container for RoboComp will be developed so instant deployment will be available on heterogeneous platforms through the docker tool.&lt;/li&gt;
  &lt;li&gt;A semi-automatic procedure to update changes from the repository to the Docker container will be established.&lt;/li&gt;
  &lt;li&gt;To study and analyze virtual infrastructure solutions, such as Open Stack, to provide a virtualized infrastructure that can integrate the physical cluster onboard the robot and external servers for less responsive, but highly computationally demanding,  tasks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Technologies involved: cmake, docker, git&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Mentor: Marco A. Gutiérrez&lt;/p&gt;

&lt;p&gt;Backup Mentors: Luis J. Manso&lt;/p&gt;

&lt;h2 id=&quot;automating-the-uploading-of-binary-files-for-git-annex&quot;&gt;5. Automating the uploading of binary files for git-annex&lt;/h2&gt;

&lt;p&gt;The process of uploading binary files to git-annex is currently a tedious task and it ends up with developers trying to avoid the use of this tool and looking for alternatives to store binary files. A new alternative needs to be developed ether having a specific tool for that automates the addition of new bin files to git-annex or switching to a new tool that takes care of the binary files. First a tedious research in current alternatives and or proper solutions to the git-annex problem should be discussed. After the community selects one among the proposed solutions this must be implemented by the student and all previous binaries should be ported to the new solution.&lt;/p&gt;

&lt;p&gt;The final solution should basically be a command that would take the binary as an input upload it to the storage server and add the link to git-annex or the selected technology in charge of the binary files tracking.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Technologies involved: git, git-annex, C++, cmake&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Mentors: Rajath Kumar&lt;/p&gt;

&lt;p&gt;Backup mentor: Felipe Cid&lt;/p&gt;

&lt;h2 id=&quot;writing-a-name-and-port-service-for-running-components&quot;&gt;6. Writing a name and port service for running components&lt;/h2&gt;

&lt;p&gt;This activity deals with writing a new component in RoboComp that will provide a naming and port service. When starting, components will now contact this new service to advertise its name, available interfaces and request a valid port. Other components starting later will be able to query the service for component names and interfaces, and therefore being able to establish their proxy connections in real time, eliminating the need for complex and tedious configuration files.
Extensive tests with large networks of components will be performed and more advances query capabilities for the service will be explored.&lt;/p&gt;

&lt;p&gt;Technologies involved:  C++, Python, Qt&lt;/p&gt;

&lt;p&gt;Mentor: Luis J. Manso
Backup mentor: Marco A. Gutiérrez&lt;/p&gt;

&lt;h2 id=&quot;a-new-graphical-tool-for-deploying-components&quot;&gt;7. A new graphical tool for deploying components&lt;/h2&gt;

&lt;p&gt;Building on the existing python-based Manager tool in RoboComp, this activity will improve the current Python based design with more features. The new tool will facilitate the creation and modification of deployment files and will allow a much better access to the management interfaces of the componets. It will be based on Qt’s Graphics View Framework. With this new tool we expect that larger component networks with more than 50 componentes, will be easy to deploy and monitor at run time.
The current tool, Manager, reads an XML file with a description of all components involved in the deployment, their localization (IP and port) and configuration parameters. With this information the program starts all the processes and detaches from them once they are up and safe. From there on, the program’s UI displays a graph of the running components, their dependencies and some basic information about them. 
We would like to improve this program in several ways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Add a tools palette in the UI so new components can be created and dragged to the central canvas. Once there the XML file will be updated. Also, the components could be interconnected by the user as long as some basic syntactic and semantic rules are obeyed.&lt;/li&gt;
  &lt;li&gt;Add a new panel in the UI to allow the user to communicate with the components through the existing CommonBehavior interface. This interfaces is created for all components and provides a common way to query their running status, memory use, iteration main period, pause, resume, abort and to change in line some of the configuration parameters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Technologies involved:  Python, PySide, Qt&lt;/p&gt;

&lt;p&gt;Mentor: Pablo Bustos
Backup mentor: Luis J. Manso&lt;/p&gt;

&lt;h2 id=&quot;port-of-robocomps-math-library-qmat-to-eigen3&quot;&gt;8. Port of RoboComp’s math library, QMat, to Eigen3&lt;/h2&gt;
&lt;p&gt;Long before Eigen was out there we wrote QMat, a linear algebra library written as a wrapper to other existing math libraries such as IPP or GSL or even our own code. QMat has a lot o nice methods and functions that we have written as we needed them and that are specially suited for our other core library InnerModel. InnerModel is class that holds and allows access to the kinematic tree used by most of the components in RoboComp. This tree represents the state of the robot and the world perceived by it. It can be displayed in 3D using OpenSceneGraph (see activity 11) and used to perform many importan calculations in robotics.
In this activiy We would like to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;port QMat to Eigen&lt;/li&gt;
  &lt;li&gt;write a QMat/InnerModel without dependencies of RoboComp and that can be used as an independent piece of code for kinematic representations and calculations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Technologies involved:  C++, Eigen&lt;/p&gt;

&lt;p&gt;Mentor: Luis J. Manso
Backup mentor: Pablo Bustos&lt;/p&gt;

&lt;h2 id=&quot;and-finally-some-work-on-computer-vision-and-3d-graphics&quot;&gt;And finally, some work on computer vision and 3D graphics&lt;/h2&gt;

&lt;h2 id=&quot;object-detection-for-simulated-environments&quot;&gt;9. Object detection for simulated environments&lt;/h2&gt;

&lt;p&gt;We have several algorithms that are trained with information from real environments. Making this algorithms work in our simulation environment is not the straightforward task it should be. We would like to have a tool that takes as input several 3D objects and generates the infrastructure needed to train these algorithms. As an example provided to users different algorithms must be trained so their effectivity can be tested with the robocomp simulation environment. Development of some Neural Networks Structures might be required to improve the algorithms results.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Technologies involved: CNN, C++&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Mentors: Ramón Cintas, Felilpe Cid&lt;/p&gt;

&lt;p&gt;Backup Mentors: Marco A. Gutiérrez&lt;/p&gt;

&lt;h2 id=&quot;computer-vision-components-and-integration-with-opendetection&quot;&gt;10. Computer vision components and integration with openDetection&lt;/h2&gt;

&lt;p&gt;Open Detection is a library developed by a student during GSoC 2015. The idea was to develop a library to ease the development of computer vision tasks such as object recognition. This library is now a stand alone project but in robocomp we would like to keep a link with it and have some examples integrated in our components structure. An automated integration of this library should be performed such as it becomes transparent for the robocomp user. Also several components should be developed making use of the main features of this library so the computer vision algorithms from Open Detection can be used through different components interfaces.&lt;/p&gt;

&lt;p&gt;Along with this some extra components will be required to be developed as an abstraction for the main features of other Computer Vision Libraries such as OpenCV and the Point Cloud Library.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Technologies involved:  C++, OpenCV, Pointcloud Library, cmake&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Mentor: Kripasindhu Sarkar&lt;/p&gt;

&lt;p&gt;Backup mentor: Marco A. Gutiérrez&lt;/p&gt;

&lt;h2 id=&quot;d-visualization-of-internal-structures-in-real-time&quot;&gt;11. 3D visualization of internal structures in real time&lt;/h2&gt;

&lt;p&gt;Current Model graphs used in RoboComp mainly for cognitive world modeling are not clear to understand, especially when they are big, which they usually are. Making these graphs easier to read and their information easier to access is a key task that will ease the task of debugging and understanding these graphs and in consequence a robot’s mind. Specific research and tests with roboticists should be performed in order to find a way of displaying this information that helps anyone understand the whole graph. Once specific key assets are detected they should be implemented one by one  and properly tested with some roboticists to ensure they are widely accepted as a proper enhancement. Several of these features should be developed until the graph becomes easily readable and can be easily managed through the graphical interface provided in robocomp.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Techonolgies involved: python, Qt5, OSG, Active grammar-based Modeling (AGM)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Mentors: Luis J. Manso, Pablo Bustos&lt;/p&gt;

&lt;p&gt;Backup mentor: Ramon Cintas, Marco A. Gutiérrez&lt;/p&gt;

&lt;h2 id=&quot;gazebo-robocomp-integration&quot;&gt;12. Gazebo-RoboComp integration&lt;/h2&gt;

&lt;p&gt;Currently RoboComps uses a lightweight non-physics simulator based on OpenSceneGraph. In order to work on more demanding robots such as hexapods and drones we need to write an adpatation layer to a physics-based simulator like Gazebo. The idea is to write a set of plugins for Gazebo, each one providing one or several of the RoboComp low-level hardware interfaces. To test the new functionality, two controllers will be written for two robots in the lab, one for a Phantom X Hexapod  and one for a Parrot drone.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Techonologies involved: C++, Gazebo, RoboComp&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Mentors: Pablo Bustos, Luis Manso&lt;/p&gt;

&lt;p&gt;Backup mentor: Ramon Cintas&lt;/p&gt;

&lt;h2 id=&quot;robocomp-and-rtab-map-integration&quot;&gt;12. RoboComp and RTAB-Map Integration&lt;/h2&gt;

&lt;p&gt;The main objective of the proposal is to successfully incorporate within the RoboComp robotic framework RGB-D localization and mapping capabilities. Concretely, the proposal would take advantage of RTAB-Map (Real-Time Appearance-Based Mapping), a RGB-D Graph-Based SLAM solution. Taking into account that both RoboComp and RTAB-Map are coded in C++ and well documented, GSoC students are expected to carry out the integration after a small documentation stage with no explicit cross-platform drawbacks. RTAB-Map has been properly integrated in ROS, which can also serve as a reference for students.
Objectives of the proposal
Among all the functionalities offered by RTAB-Map, we are mainly interested on the mapping and localization based on RGB-D sensors such as Microsoft Kinect or Asus XTion. Generated maps should be stored using the file format of the Point Cloud Library, which would facilitate further developments relying on such interesting library including segmentation and recognition based on 3D features. Moreover, generated maps may serve for the generation of synthetic environments for the RCIS simulator. This process would allow RoboComp developers to perform more realistic simulations using scenes fully connected to real-world.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Expected results&lt;/strong&gt;
- Generation of RGB-D mapping RoboComp components 
- Generation of RGB-D localization RoboComp components
- Generation of maps stored in the PCL file format
- Export RGB-D maps perceived in real-world to simulated RSIM environments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Further developments&lt;/strong&gt;
While RTAB-Map algorithms mainly rely on visual descriptors, namely SIFT and SURF, this solution could be extended to include within its pipeline several 3D features already implemented in the PCL, such as SHOT or FPFH. This novelty would allow the SLAM algorithms to properly work under challenging situations with poor or even lack of illumination.
Students desired skills
- C++ programming
- Localization and mapping
- 3D graphics toolkits 
- 3D processing libraries
- RPC frameworks&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Useful links&lt;/strong&gt;
RoboComp: https://github.com/robocomp/robocomp
RTAB-MAP:http://introlab.github.io/rtabmap/
Point Cloud Library (PCL): http://pointclouds.org/
ICE:https://zeroc.com/products/ice
Open Scene Graph: http://www.openscenegraph.org/&lt;/p&gt;

&lt;p&gt;Mentors: Jesús Martínez, Cristina Romero&lt;/p&gt;

&lt;p&gt;Backup mentor: Ismael García-Varea&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Packaging FCL and libccd</title>
   <link href="http://robocomp.github.io/website/2015/08/21/nithin11/"/>
   <updated>2015-08-21T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/08/21/nithin11</id>
   <content type="html">&lt;p&gt;I assume that you have an debian package folder in your source directory. If not refer to the  intro to debian packaging tutorial.&lt;/p&gt;

&lt;p&gt;Now lets assume that you are going to upload the package for the first time into a ppa.&lt;/p&gt;

&lt;p&gt;###Creating source package
* Rename  the source directory into &lt;project_name&gt;-&lt;version&gt; eg libccd-2.01
* Crate a .tar.gz compressed file of the source directory, and place it outside source directory
* Rename the compressed file into &lt;project_name&gt;_&lt;version&gt;.orig.tar.gz
* Now run `debuild  -k&lt;gpg_key&gt; -S -sa ` if you want to include the whole source tar in upload
* Or run `debuild  -k&lt;gpg_key&gt; -S -sd ` if you don&#39;t want to include the whole source tar in upload&lt;/gpg_key&gt;&lt;/gpg_key&gt;&lt;/version&gt;&lt;/project_name&gt;&lt;/version&gt;&lt;/project_name&gt;&lt;/p&gt;

&lt;p&gt;###So when should you upload the source? 
when you are uploading for the first time (obviously) and whenever you make some changes to your source code. but as launchpad wont allow files with same name, you should increase your version number so that the source tar get a new name. For increasing the version number you should increase it in changelog for example in this case &lt;code class=&quot;highlighter-rouge&quot;&gt;fcl (1.0-0ppa0) vivid; urgency=low&lt;/code&gt; increase 1.0-0ppa0 to 1.1-0ppa0 also remember to rename all names accordingly (source folder and source tar)&lt;/p&gt;

&lt;p&gt;but if you have only changed some file in the debian directory. For example, edited changelog or added a dependency. In those cases you can skip the source upload but in such cases you have to increase your ppa number in your changelog  for example in this line  &lt;code class=&quot;highlighter-rouge&quot;&gt;fcl (1.0-0ppa0) vivid; urgency=low&lt;/code&gt; change 0ppa0 to 0ppa1.&lt;/p&gt;

&lt;p&gt;###uploading
Now once you have generated the .source_changes file use dput to upload&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dput ppa:&amp;lt;your-lp-id&amp;gt;/&amp;lt;name&amp;gt; &amp;lt;file_name&amp;gt;.source_changes
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;###NB
* if you want to add any dependency, edit the file debian/control add add it to &lt;code class=&quot;highlighter-rouge&quot;&gt;Depends&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;Build-Depends&lt;/code&gt; field
* if you want to change the target generation , edit the distribution name in first line of debian/changelog&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title><i>GSoC,</i> After Midterms</title>
   <link href="http://robocomp.github.io/website/2015/08/20/rajath3/"/>
   <updated>2015-08-20T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/08/20/rajath3</id>
   <content type="html">&lt;p&gt;At the start of the second term, I finished developing the version-2 of the website.&lt;/p&gt;

&lt;p&gt;Version-2 : https://github.com/robocomp/website/tree/version-2&lt;/p&gt;

&lt;p&gt;After learning from the previous-2 version built the Version-3(Current) of the website using Jekyll.&lt;/p&gt;

&lt;p&gt;Version-3 : https://github.com/robocomp/website/tree/gh-pages&lt;/p&gt;

&lt;p&gt;Website : www.robocomp.net&lt;/p&gt;

&lt;p&gt;Parallely I started developing simple components for robocomp which would introduce new users to the framework. I have implemented the components in both the languages which robocomp supports - c++ and python. Also have documented the same.&lt;/p&gt;

&lt;p&gt;C++ components : https://github.com/rajathkumarmp/RoboComp-Components&lt;/p&gt;

&lt;p&gt;Python components : https://github.com/rajathkumarmp/RoboComp-Python-Components&lt;/p&gt;

&lt;p&gt;Documentation : https://github.com/rajathkumarmp/RoboComp-Docs&lt;/p&gt;

&lt;p&gt;Future : I will be writing starter components for each of the available interface so that a new user can easily get started. After having executed most of the already available components in the robocomp organization, In the coming days I will be working on components using PCL and explore other possibilities with the framework. It has been a great learning experience so far and I am hungry for more. Cheers!&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Rajath Kumar M.P&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title><i>GSoC,</i> Symbolic planning techniques for recognizing objects domestic <p>#6</p> grasping object</title>
   <link href="http://robocomp.github.io/website/2015/08/20/mercedes6/"/>
   <updated>2015-08-20T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/08/20/mercedes6</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Grasping object&lt;/strong&gt; : This post will describe the planning system that implements Robocomp in order to provide to the robot a full functionality. In order for a robot be able to carry out a mission as “take the cup from the table and take it to the kitchen,” it needs something that in robotics calls &lt;code class=&quot;highlighter-rouge&quot;&gt;planner&lt;/code&gt;. In this post we move away the issue of inverse kinematics and we dive into the field of artificial intelligence, making a slight revision of existing planners and delving into the planner using robocomp.&lt;/p&gt;

&lt;p&gt;###Planning&lt;/p&gt;

&lt;p&gt;As the name suggests, the planning is to generate plans. Plans to be executed by a robot in order to reach a goal. These objectives are often complex and require the execution of a series of steps that must be organized in the best possible manner to achieve the objective with an effort and within a reasonable time.&lt;/p&gt;

&lt;p&gt;In order for a planner can build a series of plans, it needs an initial state of the world, a desired end state (or several) and a set of rules. For example, take the objective of frying an egg. We start with an initial world consisting of a kitchen with the necessary elements: an egg with a dozen eggs, a can of oil, a frying pan, a slotted spoon, a plate and a stove. And we have a set of rules, like &lt;code class=&quot;highlighter-rouge&quot;&gt;to pour oil&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;to water plants&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;to crack egg&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;to stir&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;to serve&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;to light fire&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;to put the fire out&lt;/code&gt;. The duty of the planner is to select and order those rules that allow us to go from the initial state of the world (with raw eggs) to the final state in which a fried egg is served  on a plate. Thus, the planner would eliminate the rule of “to water plants”, and would order the rest of rules as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;to light fire&lt;/code&gt;: in order to light the stove.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;to pour oil&lt;/code&gt;: to pour oil into the pan&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;to crack egg&lt;/code&gt;: to crack the egg and throw it into the pan&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;to stir&lt;/code&gt;: to catch the slotted spoon and to go stirring the oil for frying the egg well.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;to put the fire out&lt;/code&gt;: when the egg is fried the stove is turned off.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;to serve&lt;/code&gt;: to serve the fried egg on the plate&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With these steps (very simplified) we get that our robot prepares us a fried egg… Although the example is miserable, we can see that any action we do and that we find it easy to execute, for a robot is quite complicated. That is why planning is a very complex field of artificial intelligence.&lt;/p&gt;

&lt;p&gt;###Planning Domain Definition Language (PDDL)&lt;/p&gt;

&lt;p&gt;This section will introduce the reader slightly in the planning language more used in artificial intelligence: PDDL. It was created in 1998 by Drew McDermott and his team for use in that year’s edition of International Planning Competition. The aim of PDDL is to standardize the planning languages for greater reuse of planning domains. Its operation is relatively simple:&lt;/p&gt;

&lt;p&gt;Take for example, the following domain to create geometric shapes: We presume that in our initial world there is always a vertex node. The rules increase the number of vertices nodes to create geometric identities. For example, the &lt;code class=&quot;highlighter-rouge&quot;&gt;segment&lt;/code&gt; rule creates from the initial node a line segment, the rule &lt;code class=&quot;highlighter-rouge&quot;&gt;triangle&lt;/code&gt; creates from the line segment an equilateral triangle, the rule “square” creates from the equilateral triangle a square… and so on until an octagon. In the PDDL file, the rules would be:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(define (domain AGGL)
(:predicates
            (firstunknown ?u)
            (unknownorder ?ua ?ub)

            (isA ?n) # IS A NODE?
            (isB ?n)
            (isC ?n)
            (isD ?n)
            
            (union ?u ?v) # TWO NODES ?u AND ?v are united?
    )

    (:functions
            (total-cost)
    )

    (:action segment
            :parameters ( ?va ?vListAGMInternal ?vlist0 )
            :precondition (and (isA ?va) (firstunknown ?vlist0) (unknownorder ?vlist0 ?vListAGMInternal) (not(= ?vListAGMInternal ?vlist0)) )
            :effect (and (not (firstunknown ?vlist0)) (not (unknownorder ?vlist0 ?vListAGMInternal)) (firstunknown ?vListAGMInternal) (isB ?vlist0) (union ?va ?vlist0) (increase (total-cost) 1)
            )
    )

    (:action triangle
            :parameters ( ?va ?vb ?vListAGMInternal ?vlist0 )
            :precondition (and (isA ?va) (isB ?vb) (firstunknown ?vlist0) (unknownorder ?vlist0 ?vListAGMInternal) (not(= ?vListAGMInternal ?vlist0)) (union ?va ?vb) )
            :effect (and (not (firstunknown ?vlist0)) (not (unknownorder ?vlist0 ?vListAGMInternal)) (firstunknown ?vListAGMInternal) (isC ?vlist0) (union ?vb ?vlist0) (union ?vlist0 ?va) (increase (total-cost) 1)
            )
    )

    (:action square
            :parameters ( ?va ?vc ?vb ?vListAGMInternal ?vlist0 )
            :precondition (and (isA ?va) (isC ?vc) (isB ?vb) (firstunknown ?vlist0) (unknownorder ?vlist0 ?vListAGMInternal) (not(= ?vListAGMInternal ?vlist0)) (union ?va ?vb) (union ?vb ?vc) (union ?vc ?va) )
            :effect (and (not (firstunknown ?vlist0)) (not (unknownorder ?vlist0 ?vListAGMInternal)) (firstunknown ?vListAGMInternal) (isD ?vlist0) (union ?vc ?vlist0) (union ?vlist0 ?va) (not (union ?vc ?va)) (increase (total-cost) 1)
            )
    )
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To represent the initial world from which we start and the final world that we want to go, we should implement a file like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(define (problem myProblemPDDL)

    (:domain AGGL ) # planning domain ---&amp;gt; set of rules
    (:objects
            A_1
            unknown_0
            unknown_1
            unknown_2
            unknown_3
    )

    (:init
            (= (total-cost) 0)
            (firstunknown unknown_0)
            (unknownorder unknown_0 unknown_1)
            (unknownorder unknown_1 unknown_2)
            (unknownorder unknown_2 unknown_3)
            (isA A_1) # ----&amp;gt;  there is a initial vertex node
    )

    (:goal
            (exists ( ?A_1001 ?B_1002 ?C_1003 ?D_1004 ) # -----&amp;gt; we want a final world with a square
                    (and
                            (isA ?A_1001)
                            (isB ?B_1002)
                            (isC ?C_1003)
                            (isD ?D_1004)
                            (unido ?A_1001 ?B_1002)
                            (unido ?B_1002 ?C_1003)
                            (unido ?C_1003 ?D_1004)
                            (unido ?D_1004 ?A_1001)
                    )
            )
    )
    (:metric minimize (total-cost))
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This language proves to be relatively intuitive, easy to develop and test. However, in Robocomp we opted to do our own planning language: AGM.&lt;/p&gt;

&lt;p&gt;###Active Grammar-based Modeling&lt;/p&gt;

&lt;p&gt;AGM is the result of Luis Manso’s PhD thesis that “dealt with making software systems for robots more scalable, flexible and easier to develop using software engineering for robotics […] and enhancing active perception in robots using a grammar-based technique named active grammar-based modeling and a specially tailored novelty-detection algorithm named cognitive subtraction”&lt;a href=&quot;http://ljmanso.com/thesis.php&quot;&gt;1&lt;/a&gt;. To not extend much this post, you can check the working of the AGM on its &lt;a href=&quot;http://ljmanso.com/agm/&quot;&gt;official website&lt;/a&gt;. We will target only that AGM proves to be a useful graphical tool to program rules and  to test domains and problems, and its grammar has a reminiscent of the PDDL language (has a AGM to PDDL converter). Let’s look at the same example as before but now in AGM:&lt;/p&gt;

&lt;p&gt;This would be the set of rules in a .aggl file:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// START OF THE FILE:
segment : active(1)
{
    {
            a:A(-175,-15)
    }
    =&amp;gt;
    {
            a:A(-415,-5)
            b:B(-150,-5)
            a-&amp;gt;b(union)
    }
}

triangle : active(1)
{
    {
            a:A(-330,-10)
            b:B(-70,-15)
            a-&amp;gt;b(union)
    }
    =&amp;gt;
    {
            a:A(-440,170)
            c:C(-270,10)
            b:B(-70,170)
            a-&amp;gt;b(union)
            b-&amp;gt;c(union)
            c-&amp;gt;a(union)
    }
}

square : active(1)
{
    {
            a:A(-385,80)
            c:C(-245,-125)
            b:B(-105,80)
            a-&amp;gt;b(union)
            b-&amp;gt;c(union)
            c-&amp;gt;a(union)
    }
    =&amp;gt;
    {
            a:A(-460,125)
            c:C(-155,-45)
            b:B(-150,125)
            d:D(-460,-40)
            a-&amp;gt;b(union)
            b-&amp;gt;c(union)
            c-&amp;gt;d(union)
            d-&amp;gt;a(union)
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The initial world model is stored in a xml file:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;AGMModel&amp;gt;
    &amp;lt;symbol id=&quot;1&quot; type=&quot;A&quot; /&amp;gt;  #There is only a vertex node (symbol) with the identifier &quot;1&quot; and te type &quot;A&quot;
&amp;lt;/AGMModel&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The goal or target world is stored in another xml file:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;AGMModel&amp;gt;
    # NODES:
    &amp;lt;symbol id=&quot;a&quot; type=&quot;A&quot; /&amp;gt;
    &amp;lt;symbol id=&quot;b&quot; type=&quot;B&quot; /&amp;gt;
    &amp;lt;symbol id=&quot;c&quot; type=&quot;C&quot; /&amp;gt;
    &amp;lt;symbol id=&quot;d&quot; type=&quot;D&quot; /&amp;gt;
    # LINKS BETWEEN NODES (RELATIONSHIPS)
    &amp;lt;link src=&quot;a&quot; dst=&quot;b&quot; label=&quot;union&quot; /&amp;gt;
    &amp;lt;link src=&quot;b&quot; dst=&quot;c&quot; label=&quot;union&quot; /&amp;gt;
    &amp;lt;link src=&quot;c&quot; dst=&quot;d&quot; label=&quot;union&quot; /&amp;gt;
    &amp;lt;link src=&quot;d&quot; dst=&quot;a&quot; label=&quot;union&quot; /&amp;gt;
&amp;lt;/AGMModel&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;###Component architecture&lt;/p&gt;

&lt;p&gt;Explained more or less the planners, we will explain the architecture of components developed by robocomp for the robot to be able to carry out actions. Oversimplifying the question in order to that the reader make a clear idea of how the architecture works, we can say that this architecture is divided into three levels:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/mercedes92/VisualIKExperiment/blob/master/images/Arquitectura.png?raw=true&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First we need a problem domain (a set of rules), like problems in a home environment and a representation of the environment (a model of the robot and its environment).&lt;/li&gt;
  &lt;li&gt;In the top we have the AGM planner. Given an initial world and an objective world, it is in charge of generating a plan to reach the goal with the domain defined.&lt;/li&gt;
  &lt;li&gt;In the second label, we have a special component, the &lt;code class=&quot;highlighter-rouge&quot;&gt;executive&lt;/code&gt;. Basically he is responsible for transmitting the plan generated by the planner to the lower components. These components perform their actions and alter the representation of the environment. so the executive will have to call the scheduler to verify that these changes on the environment are correct and possible. If the changes are verified, the executive will update the representation.&lt;/li&gt;
  &lt;li&gt;In the third label we have the agent components. These are the components that receive the orders of the executive. They uses the operations of the lower component to change the representation. When they finish their execution, they publish the changes in order to be analyzed by the executive.&lt;/li&gt;
  &lt;li&gt;In the botton we have the basic components. They are those who perform basic calculations. For example, our &lt;code class=&quot;highlighter-rouge&quot;&gt;inversekinematic&lt;/code&gt; component, our &lt;code class=&quot;highlighter-rouge&quot;&gt;visualik&lt;/code&gt; component and our &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt; component are in this level.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So the challenge that we have to complete is to create an agent that uses our three inverse kinematics components in order to reach a goal: that the robot take a cup. This component is called &lt;code class=&quot;highlighter-rouge&quot;&gt;graspingAgent&lt;/code&gt; and is currently being developed by the laboratory. I would like to delve into its operation, but we still needs to implement many things and GSoC is coming to an end :(&lt;/p&gt;

&lt;p&gt;It’s a shame to have to say goodbye with the work so close to being finished. But it doesn’t matters, the next year the robot will be serving coffee to Robolab guys XD.&lt;/p&gt;

&lt;p&gt;Bye!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://photos.gograph.com/thumbs/CSP/CSP705/k24410287.jpg&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>GSoC, Computer vision components and libraries management -Open Detection <p>#4</p></title>
   <link href="http://robocomp.github.io/website/2015/08/19/kripasindhu_sarkar_blog_4/"/>
   <updated>2015-08-19T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/08/19/kripasindhu_sarkar_blog_4</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Experience&lt;/strong&gt; 
It has been a quite a ride through this Google Summer of Code. Writing a completely new library with complete building framework/rest framework for documentation or tutorial/and Doxygen framework for auto documentation-class diagrams was challenging. It was equally exciting as well. Because of this I believe that I have acquired quite a good ‘library maintainer/admin’ skills. I will continue contributing into this library after the finish of this GSoC.&lt;/p&gt;

&lt;p&gt;After the challenges in building library support tools, the next major challenge was library design. Since, there was no previous ‘coding flavor/design’ I had to come up with the polymorphic and repeatable class design. There was so much confusion in choosing one alternative among the so many choices of good design. While the progress of the library I had to redesign the framework, and and remove structures, include namespaces and several things. I initially had thought that the design will be pretty much stable after the first month, which was not true. I still feel that the main design may need to be changed in the future to have a more logical structure. I hope this constant effort of keeping a good design will be helpful for the library users, and in the end, won’t end up with an unstructured library like OpenCV. In the future I’ll always keep an eye open for the design in general.&lt;/p&gt;

&lt;p&gt;Now coming to the actual work of implementing different algorithm, I had really good experience in getting my hands dirty with variety of popular object detection methods. This was exactly what I had thought from this project. Taking out three months and working on the popular methods of your research topic is probably important and I hope that I’ll make a good us of this in the future.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Learning&lt;/strong&gt; 
Here I’ll point out some of the tools and resources I have used while building the library.&lt;/p&gt;

&lt;p&gt;####CMake building framework:
I read and used ideas from the CMakeLists files of &lt;strong&gt;VTK, pcl&lt;/strong&gt; and &lt;strong&gt;OpenCV&lt;/strong&gt; (not that much); and chose stuffs among them depending what I thought was interesting and also modified them according to my needs. But in general, I used PCL cmake framework as my major reference.&lt;/p&gt;

&lt;p&gt;####Library design:
I have always found the design of &lt;strong&gt;VTK&lt;/strong&gt;, beautiful; so chose it to get the polymorphic design of the classes. Even though fully templated policy based design is faster than polymorphic classes, I chose the polymorphic one as it gives way more flexibility without cluttering the design (ideas taken from vtk). I still am not sure if the performance loss through this design will affect in the future.&lt;/p&gt;

&lt;p&gt;####Algorithms:
I had a good background in PCL, so Point Cloud based detection was not difficult and used their wonderful 3d_recognition_framework in my backend. Detection based on 2D features is my own contribution and other 2D global detectors (like HOG) was pretty straightforward from OpenCV. 
One good learning was the implementation of HOG Trainer which I worked on few open source code and modified them according to my own needs (like hard negative training etc).&lt;/p&gt;

&lt;p&gt;##Future
This project does not ends with the end of GSoC. I plan to do the following in the short term future:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Host a website as a homepage and links to documentation/tutorials etc.&lt;/li&gt;
  &lt;li&gt;Identify how is the website automatically updated with a push in previous libraries like PCL etc. They probably use some demon which runs everyday. Learn and implement it.&lt;/li&gt;
  &lt;li&gt;Improve the documentations, write some descriptive tutorials.&lt;/li&gt;
  &lt;li&gt;Properly launch the library, with its website; first among the colleagues of our lab; then in public blogs.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

</content>
 </entry>
 
 <entry>
   <title>GSoC, Computer vision components and libraries management - Open Detction <p>#3</p></title>
   <link href="http://robocomp.github.io/website/2015/08/19/kripasindhu_sarkar_blog_3/"/>
   <updated>2015-08-19T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/08/19/kripasindhu_sarkar_blog_3</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Contributions after Mid-term&lt;/strong&gt;
Following are the contributions towards the project &lt;em&gt;after&lt;/em&gt; the mid-term evaluations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;HOG based detection (with demo).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Face-recognition: training as well as recognition (with demo).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cascade based detection (with demo).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Functionality for confidence in a detection (to handle uncertainty).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That concluded the functionality of whatever planned for this summer of code. Other than the planned work, I included some other new functionality which came in between and I thought was required. The other additions are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Addition of two types of detection functions for every detection class.&lt;/p&gt;

    &lt;p&gt;a. &lt;em&gt;detectOmni&lt;/em&gt; : This is the function whose purpose is to detect an object in an entire scene. Thus, other than the type of detection we also have information about the &lt;em&gt;location&lt;/em&gt; of the detection w.r.t. the scene. This was the only function previously implemented.&lt;/p&gt;

    &lt;p&gt;b. &lt;em&gt;detect&lt;/em&gt; : The purpose of this function is to perform detection on a segmented scene or an ‘object candidate’. i.e. the entire scene is considered as an ‘object’ or an detection. This function was an extra addition for most of the classes. For global descriptor based classes like ODHOGDetector this function came very naturally.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Standardized the training database&lt;/strong&gt;:. Each detection class identifies its own database (which is a unique folder as of now). Training data of all classes are now put together under a single ‘trained_data’ directory and the rest is resolved by the class automatically.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The class &lt;strong&gt;ODDetectorMultiAlgo&lt;/strong&gt;: This is one of the most interesting class and interesting contribution in this project. The idea is to run multiple detection algorithm on a same segmented or unsegmented scene (i.e. run both &lt;em&gt;detect&lt;/em&gt; and &lt;em&gt;detectOmni&lt;/em&gt; function) and provide the result. That is, using this class one can do object detection/recognition using multiple algorithms and provide outcome of detections (eg. people detected by HOG, face detected by Cascade, bottle detected PnPRansac in a same image). Because of the nice polymorphic design of the detection classes, one can use the concept easily with any number of detection classes with their own parameters, but with &lt;em&gt;ODDetectorMultiAlgo&lt;/em&gt; one can only take benefit of the default parameters of the included classes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;HOG training: There are no standard training module for training HOG (in OpenCV or in other standard library) and people are mostly confused in training svm with the HOG detector available from OpenCV. This formed a motivation for completing the HOG pipeline and provide a training module. The crucial contribution includes &lt;em&gt;hard negative&lt;/em&gt; training mode where the training is repeated with hard examples or false positive windows as negative windows. I hope that this will be really a helpful contribution for the computer vision community.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;design-changes-learning-resources-and-overall-learning-experiences&quot;&gt;Design changes, learning resources, and overall learning experiences:&lt;/h2&gt;
&lt;p&gt;In the next blog I’ll add the different sources I used to design and implement the above tasks and the things I learnt in this process.&lt;/p&gt;

&lt;hr /&gt;

</content>
 </entry>
 
 <entry>
   <title><i>GSoC,</i> Symbolic planning techniques for recognizing objects domestic <p>#5</p> System Review</title>
   <link href="http://robocomp.github.io/website/2015/08/16/mercedes5/"/>
   <updated>2015-08-16T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/08/16/mercedes5</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Full object manipulation system&lt;/strong&gt; : In this post we will describe the full system developed for manipulating objects using a robotic arm. We start at the highest level, VisualIK (the correction system), and we will go down to the base of the system, the IK (which calculates the final angle of the joints of the robotic arm).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/mercedes92/VisualIKExperiment/blob/master/images/Dibujo%20sin%20t%C3%ADtulo.png?raw=true&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;All the components that will be described below implement the &lt;code class=&quot;highlighter-rouge&quot;&gt;InverseKinematics&lt;/code&gt; interface. This interface is defined by a series of data structures:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Pose6D&lt;/code&gt;: It represents a pose with three translations and three rotations [x, y, z, rx, ry, rz]&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;WeightVector&lt;/code&gt;: It represents the weight vector of each element of Pose6D. This will help us later in the Levenberg-Marquardt algorithm.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;TargetState&lt;/code&gt;: It is the state when the targets reach the end of their execution by ik. It has some information like the elapsed time, the final error in translations and rotations and the values of each joint.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Axis&lt;/code&gt;: This structure represents the Cartesian axes. It is necessary for certain special types of targets.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Motor&lt;/code&gt;: This structure stores the name and the angular value of a motor of the kinematic chain.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Also, the interface defines a number of methods:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;getTargetState&lt;/code&gt;: this method returns the state of one target, given the name of the robot part that executed the target and the numerical identifier of the target.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;getPartState&lt;/code&gt;: this method returns the state (if the part has pending targets or not) of one of the robot part (RIGHTARM, LEFTARM or HEAD).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;setTargetPose6D&lt;/code&gt;: This method is used to send targets to the ik. We need to indicate the part of the robot that will execute the target, the pose of the target and the weight vector of the pose. It returns the identifier of the target.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;setTargetAlignaxis&lt;/code&gt;: This method sends a special target for the IK. This target is achieved by aligning the end effector to the target axes. We must indicate the the part of the robot that will execute the target, the pose and the axes. It returns the identifier of the target.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;setTargetAdvanceAxis&lt;/code&gt;: This method sends another special target for the IK. The goal is that the end effector advance along a vector in the Cartesian system. We must indicate the part of the robot that will execute the target, the axes of the vector and the dist to advance. Like always, it return the identifier of the target.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;goHome&lt;/code&gt;: this method send a robot part to the idle position.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;stop&lt;/code&gt;: this method abort the execution of the targets of one of the robot part.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;setJoint&lt;/code&gt;: this method changes the angular value of one of the joint of the kinematic chain.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;setFingers&lt;/code&gt;: this method opens and closes the fingers of the end effector.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now, we will describe the basic operation of our components.&lt;/p&gt;

&lt;p&gt;###The &lt;code class=&quot;highlighter-rouge&quot;&gt;visualIK&lt;/code&gt; component&lt;/p&gt;

&lt;p&gt;The old &lt;a href=&quot;http://robocomp.github.io/website/2015/06/17/mercedes3.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;VisualBIK&lt;/code&gt; component&lt;/a&gt; has been reorganized, resulting in the current &lt;a href=&quot;https://github.com/robocomp/robocomp-ursus/tree/master/components/visualik&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;visualik&lt;/code&gt; component&lt;/a&gt;. The basic principle of operation of the component remains, adding some improvements and changing its communication with the &lt;code class=&quot;highlighter-rouge&quot;&gt;inversekinematics&lt;/code&gt; component, by the &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt; component.&lt;/p&gt;

&lt;p&gt;Its goal remains the same, correct errors produced by the inaccuracies of the joints in the IK. To this end, it bases its operation on a state machine:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;IDLE&lt;/code&gt;: It is the resting state of &lt;code class=&quot;highlighter-rouge&quot;&gt;visualik&lt;/code&gt;. The component is waiting to receive a target through one of the methods of its interface. When a target comes through the interface (stored in the &lt;code class=&quot;highlighter-rouge&quot;&gt;currentTarget&lt;/code&gt; variable), the state machine switches to INIT_BIK and prepares the global variables for the execution of correction.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;INIT_BIK&lt;/code&gt;: in this state, the visualik applies an initial correction to the target (Ec). This correction is based on experience in the correction of previous targets, so the first correction, having no previous experience, will be zero. Then, it sends the target to his proxy of kinematic, the &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt; component. Finally, the state of the machine is changed to &lt;code class=&quot;highlighter-rouge&quot;&gt;WAIT_BIK&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;WAIT_BIK&lt;/code&gt;: in this state, the &lt;code class=&quot;highlighter-rouge&quot;&gt;visualik&lt;/code&gt; waits the end of execution of the target by the &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt;. When the &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt; finishes executing the target, the &lt;code class=&quot;highlighter-rouge&quot;&gt;visualik&lt;/code&gt; changes his state to &lt;code class=&quot;highlighter-rouge&quot;&gt;CORRECT_ROTATION&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;CORRECT_ROTATION&lt;/code&gt;: It is the latest machine status. In this state is when the &lt;code class=&quot;highlighter-rouge&quot;&gt;visualik&lt;/code&gt; does all the calculations in order to correct the deviations and errors of the joints. The procedure is simple: a) by apriltags, the &lt;code class=&quot;highlighter-rouge&quot;&gt;visualik&lt;/code&gt; calculates the visual pose of the end effector; b) then, it computes the error vector between the visual pose and the target pose (Ev); c) with this error vector Ev, the &lt;code class=&quot;highlighter-rouge&quot;&gt;visualik&lt;/code&gt; corrects the target pose and sends the new position to the &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt; component; d) this process is repeated until the error achieved in translation and rotation is less than a predetermined threshold; e) finally the &lt;code class=&quot;highlighter-rouge&quot;&gt;visualik&lt;/code&gt; calculates the error vector between the original target and the last corrected target (Ec), with this error the component realizes the first correction in &lt;code class=&quot;highlighter-rouge&quot;&gt;INIT_BIK&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here there is a scheme of the procedure performed by the &lt;code class=&quot;highlighter-rouge&quot;&gt;visualik&lt;/code&gt;. It is very summarized to facilitate the understanding of the procedure by the reader:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1: procedure VISUAL CALIBRATION
2:    Xt = TargetArrives()
3:    Ct = Xt + Ec
4:    sendTargetToGIK(Ct)
5:    Xv = getAprilTagPose(robot)
6:    while (Ev = Xv- Xt) &amp;gt; threshold ^¬ timeOut() do
7:         Xi = getInternalPose(robot)
8:         Xc = Xi + Ev
9:         sendTargetToGIK(Xc)
10:        Xv = getAprilTagPose(robot)
11:   end while
12:   Ec = Xc - Xt
13: end procedure
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Like all components developed by robocomp, the &lt;code class=&quot;highlighter-rouge&quot;&gt;visualik&lt;/code&gt; needs a configuration file in which the components required (the &lt;a href=&quot;https://github.com/robocomp/robocomp-ursus/tree/master/components/ikGraphGenerator&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt;&lt;/a&gt;) and the components to subscribe (the &lt;a href=&quot;https://github.com/robocomp/robocomp-robolab/tree/master/components/apriltagsComp&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;apriltagsComp&lt;/code&gt;&lt;/a&gt;) and other configuration parameters are determined. In this case, a configuration file may have the following elements:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CommonBehavior.Endpoints=tcp -p 14537
# Endpoints for implemented interfaces:
InverseKinematics.Endpoints=tcp -p 10242

# Endpoints for interfaces to subscribe:
AprilTagsTopic.Endpoints=tcp -p 12938

# Proxies for required interfaces
InverseKinematicsProxy =inversekinematics:tcp -h localhost -p 10241 # `ikGraphGenerator`

InnerModel=/home/robocomp/robocomp/components/robocomp-ursus-rockin/etc/ficheros_Test_VisualBIK/ursus_bik.xml # the internal model of the robot environment 

# This property is used by the clients to connect to IceStorm.
TopicManager.Proxy=IceStorm/TopicManager:default -p 9999
Ice.Warn.Connections=0
Ice.Trace.Network=0
Ice.Trace.Protocol=0
Ice.ACM.Client=10
Ice.ACM.Server=10
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;###The &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt; component&lt;/p&gt;

&lt;p&gt;As we explain in the previous &lt;a href=&quot;http://robocomp.github.io/website/2015/08/13/mercedes4.html&quot;&gt;post&lt;/a&gt;, the &lt;a href=&quot;https://github.com/robocomp/robocomp-ursus/tree/master/components/ikGraphGenerator&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt;component&lt;/a&gt; creates and stores a graph representing the work 3D space of the arm, where each node stores the euclidean space pose of the end effector and the configuration of the joints that compose the arm. So the &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt; waits until the &lt;code class=&quot;highlighter-rouge&quot;&gt;visualik&lt;/code&gt; send a target to him through the &lt;code class=&quot;highlighter-rouge&quot;&gt;InverseKinematics&lt;/code&gt; interface. In this moment, the &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt; performs four steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;the component searches the node A whose pose is closest to the initial end effector pose and moves the arm there.&lt;/li&gt;
  &lt;li&gt;the component finds in the graph  the node B whose pose is closest to the target position, disabling those nodes which would make the robot’s arm collide with external objects.&lt;/li&gt;
  &lt;li&gt;the component computes the shortest path between the node A and the node B and moves the end effector among the nodes to reach the node B.&lt;/li&gt;
  &lt;li&gt;Finally, the component moves from the graph to the actual target sending the original target to the &lt;code class=&quot;highlighter-rouge&quot;&gt;inversekinematic&lt;/code&gt;component and taking the final values of the joints.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt;needs a config file too:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CommonBehavior.Endpoints=tcp -p 14536
# Endpoints for implemented interfaces:
InverseKinematics.Endpoints=tcp -p 10241

InnerModel=/home/robocomp/robocomp/components/robocomp-ursus/etc/ursus.xml # the internal model of the robot environment 

# Proxies for required interfaces
InverseKinematicsProxy = inversekinematics:tcp -h localhost -p 10240
JointMotorProxy = jointmotor:tcp -h localhost -p 20000

Ice.Warn.Connections=0
Ice.Trace.Network=0 
Ice.Trace.Protocol=0
Ice.ACM.Client=10
Ice.ACM.Server=10
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;###The &lt;code class=&quot;highlighter-rouge&quot;&gt;inversekinematic&lt;/code&gt; component&lt;/p&gt;

&lt;p&gt;Finally, we will explain the basic component of the system, the &lt;a href=&quot;https://github.com/robocomp/robocomp-ursus/tree/master/components/inversekinematics&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;inversekinematic&lt;/code&gt; component&lt;/a&gt;. As we said in the &lt;a href=&quot;http://robocomp.github.io/website/2015/06/15/mercedes2.html&quot;&gt;second post&lt;/a&gt;, this component receives three types of targets through the &lt;code class=&quot;highlighter-rouge&quot;&gt;InverseKinematics&lt;/code&gt; interface:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;POSE6D&lt;/code&gt;: the typical target with translations and rotations [tx, ty, tz, rx,ry, rz]. The end effector has to be positioned at coordinates (tx, ty, tz) of the target and align their rotation axes with the target, specified in (rx, ry, rz). This target arrives from the method &lt;code class=&quot;highlighter-rouge&quot;&gt;setTargetPose6D&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ADVANCEAXIS&lt;/code&gt;: its goal is to move the end effector of the robot along a vector. This target arrives from the method &lt;code class=&quot;highlighter-rouge&quot;&gt;setTargetAdvanceAxis&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ALIGNAXIS&lt;/code&gt;: Its goal is that the end effector has the axes aligned with the target. This target arrives from the method &lt;code class=&quot;highlighter-rouge&quot;&gt;setTargetAlignaxis&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Targets received are stored into the queues of the corresponding robot part and they are executed by order of arrival. When the ik ends the execution of one target, it stores the target into the solved targets queue:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/mercedes92/VisualIKExperiment/blob/master/images/iksystem.png?raw=true&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The config file of the &lt;code class=&quot;highlighter-rouge&quot;&gt;inversekinematic&lt;/code&gt;component is the next:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CommonBehavior.Endpoints=tcp -p 12207
# Endpoints for implemented interfaces:
InverseKinematics.Endpoints=tcp -p 10240

# Kinematic chain lists: they stores the joints names.
RIGHTARM=rightShoulder1;rightShoulder2;rightShoulder3;rightElbow;rightForeArm;rightWrist1;rightWrist2
RIGHTTIP=grabPositionHandR # end effector of the RIGHTARM

LEFTARM=leftShoulder1;leftShoulder2;leftShoulder3;leftElbow;leftForeArm;leftWrist1;leftWrist2
LEFTTIP=grabPositionHandL

HEAD=head_yaw_joint;head_pitch_joint
HEADTIP=rgbd_transform

InnerModel=/home/robocomp/robocomp/components/robocomp-ursus/etc/ursus.xml #Internal model of the robot environment

 # Proxies for required interfaces
JointMotorProxy = jointmotor:tcp -h localhost -p 20000

TopicManager.Proxy=IceStorm/TopicManager:default -p 9999
Ice.Warn.Connections=0
Ice.Trace.Network=0
Ice.Trace.Protocol=0
Ice.ACM.Client=10
Ice.ACM.Server=10
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;When &lt;code class=&quot;highlighter-rouge&quot;&gt;visualik&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt; components submit their targets to the immediately below component, they store the identifier of the target and are waiting until the target be resolved, calling the method &lt;code class=&quot;highlighter-rouge&quot;&gt;getTargetState&lt;/code&gt;. So, when the &lt;code class=&quot;highlighter-rouge&quot;&gt;inversekinematic&lt;/code&gt; component ends one target execution, the &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt; moves the arm with the values given by the &lt;code class=&quot;highlighter-rouge&quot;&gt;inversekinematic&lt;/code&gt; component and then, the &lt;code class=&quot;highlighter-rouge&quot;&gt;visualik&lt;/code&gt; continues with the corrections.&lt;/p&gt;

&lt;p&gt;Having explained the handling system, the next post will explain the planning system developed by ROBOLAB to plan the robot’s actions.&lt;/p&gt;

&lt;p&gt;Bye!&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title><i>GSoC,</i> Symbolic planning techniques for recognizing objects domestic <p>#4</p>, Inverse kinematics Graph Generator</title>
   <link href="http://robocomp.github.io/website/2015/08/13/mercedes4/"/>
   <updated>2015-08-13T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/08/13/mercedes4</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;ikGraphGenerator, an alternative to ik&lt;/strong&gt; : As the project has progressed, many improvements have emerged. One of them is the new component, &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt;. This component has been developed by Professor Luis Manso, and its goal is to remove weight to the inverse kinematics in the process of handling objects with a robotic effector.&lt;/p&gt;

&lt;p&gt;One of the problems of the inverse kinematics is that, given a target for a particular end-effector, it can calculate different solutions or values for each motor of the kinematic chain. For example, to pick up a cup, the IK can position the end effector at the target extending his arm more or less forcing or not the elbow or or separating more or less the shoulder. It doesn’t matter that the final position of the chain be more forced or more natural, the goal is reached and the solution is accepted.&lt;/p&gt;

&lt;p&gt;This is not convenient, since there is no way to control the trajectory of the arm. The control on the trajectory arm is crucial at certain times, for example when the robot avoids collisions between his arm and another objects (tables, chairs, walls, including his own body). The current &lt;code class=&quot;highlighter-rouge&quot;&gt;inversekinematics&lt;/code&gt; component does not allow us this control, so that we need the &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt; component.&lt;/p&gt;

&lt;p&gt;The main concept is to create a spatial graph where each node stores the pose of the point, [tx, ty, tz, rx, ry, rz], and the set of angular values for each motor of the kinematic chain (that has 7 DOFs). The links connecting the nodes whose positions are close to each other. In this way we can calculate paths between two different and separate points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/robocomp/robocomp-ursus/blob/master/components/ikGraphGenerator/etc/ikg.jpg?raw=true&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;###What does the &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt; component?&lt;/p&gt;

&lt;p&gt;Basically the &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt; realizes two functions:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It calculates the graph with random poses and their respective angular values. To do this, first it defines a spatial cube that represents the workspace of the robot arm. In this space a set of poses are selected and are sent to the &lt;code class=&quot;highlighter-rouge&quot;&gt;inversekinematic&lt;/code&gt; component as targets. The poses that are not achievable by the ik are automatically deleted. The resultant graph is stored in a file in order to use it in later calculations.&lt;/li&gt;
  &lt;li&gt;Once the graph has been calculated and stored, we can send a target to the &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt;. First the component searches the node &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; whose pose is closest to the initial end effector pose and the node &lt;code class=&quot;highlighter-rouge&quot;&gt;B&lt;/code&gt; whose pose is closest to the target position (to do this, the &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt; uses a fast low-dimension k-d tree). Then, the component calculates a path between node A and node B through the graph  using Dijkstra’s algorithm, so that the arm moves through the graph from the position marked by node A to the position marked by the node B. Finally, in order to achieve the final target, the &lt;code class=&quot;highlighter-rouge&quot;&gt;inversekinematic&lt;/code&gt; component is called to compute the final values of the joints, starting from the position marked by the node B.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/robocomp/robocomp-ursus/blob/master/components/ikGraphGenerator/etc/GIK.png?raw=true&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the next post I will describe how the whole system works with all the components, the &lt;code class=&quot;highlighter-rouge&quot;&gt;inversekinematic&lt;/code&gt;, the &lt;code class=&quot;highlighter-rouge&quot;&gt;ikGraphGenerator&lt;/code&gt; and the &lt;code class=&quot;highlighter-rouge&quot;&gt;visualik&lt;/code&gt; component.&lt;/p&gt;

&lt;p&gt;Bye!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Till now ... after midterm</title>
   <link href="http://robocomp.github.io/website/2015/08/08/nithin9/"/>
   <updated>2015-08-08T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/08/08/nithin9</id>
   <content type="html">&lt;p&gt;Hi all , In this post i will talk about what i have been working on after midterm evaluation. I have spend my time working mostly on packaging supporting libraries for Robocomp.This includes FCL and libccd. FCL is a library for performing three types of proximity queries on a pair of geometric models composed of triangles. libccd is a library for collision detection between two convex shapes.Technically Robocomp is only using FCL but libccd is an dependency of fcl, as i couldn’t find an updated ppa for it i decided to package it too. you can see those packages &lt;a href=&quot;https://launchpad.net/~imnmfotmal&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also i added ability for generating robocomp source packages for different distributions. Initially i added the option only for trusty, now we could generate packages for any distribution.I worked a bit on build tools also. Currently if someone created an workspace and made a github repo of it, some one else cant use it as we store the repo names in ~/.config directory, so i added an option to reinit an repo.&lt;/p&gt;

&lt;p&gt;well i guess thats all for now&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Setting up ppa</title>
   <link href="http://robocomp.github.io/website/2015/07/25/nithin10/"/>
   <updated>2015-07-25T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/07/25/nithin10</id>
   <content type="html">&lt;p&gt;##Setting up an ppa in launchpad&lt;/p&gt;

&lt;p&gt;After creating an launchpad account First you need to create and publish an OPENPGP key&lt;/p&gt;

&lt;p&gt;###Generating your key in Ubuntu
The easiest way to generate a new OpenPGP key in Ubuntu is to use the Passwords and Encryption Keys tool.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; open Passwords and Encryption Keys.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; Select File &amp;gt; New, select PGP Key and then follow the on-screen instructions.&lt;/p&gt;

&lt;p&gt;Now you’ll see your new key listed in the Passwords and Encryption Keys tool. (it may take some time)&lt;/p&gt;

&lt;p&gt;###Publishing your key&lt;/p&gt;

&lt;p&gt;Your key is useful only if other people can verify items that you sign. By publishing your key to a keyserver, which acts as a directory of people’s public keys, you can make your public key available to anyone else.Before you add your key to Launchpad, you need to push it to the Ubuntu keyserver.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; Open Passwords and Encryption Keys.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; Select the My Personal Keys tab, select your key.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt;  Select Remote &amp;gt; Sync and Publish Keys from the menu. Choose the Sync button. (You may need to add htp://keyserver.ubuntu.com to your key servers if you are not using Ubuntu.)&lt;/p&gt;

&lt;p&gt;It can take up to thirty minutes before your key is available to Launchpad. After that time, you’re ready to import your new key into Launchpad!&lt;/p&gt;

&lt;p&gt;OR you can direclty to go &lt;code class=&quot;highlighter-rouge&quot;&gt;http://keyserver.ubuntu.com/&lt;/code&gt; on your browser and add the PGP key there&lt;/p&gt;

&lt;p&gt;###Register your key in launchpad
fire up an terminal and run &lt;code class=&quot;highlighter-rouge&quot;&gt;gpg --fingerprint&lt;/code&gt; should give you fingerprints of all the keys. copy paste the required fingerprint into launchpad&lt;/p&gt;

&lt;p&gt;###Sign Ubunutu Code of Conduct
Download the ubuntu code of conduct form launchpad
&lt;code class=&quot;highlighter-rouge&quot;&gt;gpg --clearsign UbuntuCodeofConductFile&lt;/code&gt;  will sign the file
now copy the contents of the signed file and paste in launchpad&lt;/p&gt;

&lt;p&gt;###Wrapping Up
Now everything is set up. make sure you have some key in &lt;code class=&quot;highlighter-rouge&quot;&gt;OPENPGP Keys&lt;/code&gt; section and also the signed code of code of conduct as &lt;code class=&quot;highlighter-rouge&quot;&gt;Yes&lt;/code&gt; as shown.
&lt;img src=&quot;./launchpad.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;##Uploading package to ppa&lt;/p&gt;

&lt;p&gt;launchpad will only accept source packages and not binary.Launchpad will then build the packages. For building source packages we are using debuild which is a wrapper around the &lt;em&gt;dpkg-buildpackage + lintian&lt;/em&gt;. so you will need to install debuild and dput on your system;&lt;/p&gt;

&lt;p&gt;The source_package.cmake script is used to create debian source package.&lt;/p&gt;

&lt;p&gt;The main CMakeLists.txt file defines a target &lt;code class=&quot;highlighter-rouge&quot;&gt;spackage&lt;/code&gt; that builds the source package in build/Debian with &lt;code class=&quot;highlighter-rouge&quot;&gt;make spackage&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For uploading the package to ppa, First change the &lt;strong&gt;PPA_PGP_KEY&lt;/strong&gt; in &lt;a href=&quot;../cmake/package_details.cmake#L26&quot;&gt;package_details.cmake&lt;/a&gt; to details to the full-name of the PGP key  details registered with your ppa account For more details on setting up the pgp key see the &lt;a href=&quot;./setting_up_ppa.md&quot;&gt;tutorial&lt;/a&gt;.Then create a source package by building the target &lt;em&gt;spackage&lt;/em&gt;.Once the Source package is build successfully, upload it to your ppa by:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd Debian/
dput ppa:&amp;lt;lp-username&amp;gt;/&amp;lt;ppa-name&amp;gt; &amp;lt;packet-&amp;gt;source.changes
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;building of source package can be tested with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd Debian/robocomp-&amp;lt;version&amp;gt;
debuild -i -us -uc -S
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If you are uploading a new version of robocomp, change the version number  accordingly in the &lt;a href=&quot;../CMakeLists.txt#L31&quot;&gt;toplevel cmake&lt;/a&gt; before building, and then upload the source package as mentioned.&lt;/p&gt;

&lt;p&gt;###Note:&lt;/p&gt;

&lt;p&gt;If you want to upload another source package to ppa which doesn’t have any changes in the source but maybe in the debian files. you can build the spackage after commenting out &lt;code class=&quot;highlighter-rouge&quot;&gt;set(DEB_SOURCE_CHANGES &quot;CHANGED&quot; CACHE STRING &quot;source changed since last upload&quot;)&lt;/code&gt; in &lt;a href=&quot;../cmake/package_details.cmake#L27&quot;&gt;package_details.cmake&lt;/a&gt; so that the the script will only increase the ppa version number and won’t include the source package for uploading to ppa (which otherwise will give an error).&lt;/p&gt;

&lt;p&gt;##Installing robocomp from ppa&lt;/p&gt;

&lt;p&gt;First you will need to add the ppa in your sources, and then install robocomp package.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo add-apt-repository ppa:&amp;lt;lp-username&amp;gt;/robocomp
sudo apt-get update
sudo apt-get install robocomp
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;this will install robocomp along with basic components into /opt/robocomp.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title><i>GSoC,</i> Computer vision components and libraries management <p>#1</p></title>
   <link href="http://robocomp.github.io/website/2015/07/02/kripa1/"/>
   <updated>2015-07-02T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/07/02/kripa1</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;About me&lt;/strong&gt;:Hello, I am Kripasindhu Sarkar, a new PhD student at German Research Center for Artificial Intelligence (DFKI), Kaiserslautern working in the topic of Object Detection in simple and depth images. 
I am extremely interested in the topic of object detection and computer vision; specifically in solving the problem by using theories from human cognition and perception to simulate human way of visualizing the problem. 
But for now, I am focused on getting a very good grasp at the existing engineering (mostly) techniques in the field of computer vision and object detection. 
Before joining here as a PhD student I worked as a Software Engineer at Paypal for 2 years and, prior to that I did my masters and graduation from Indian Institute of Technology Kharagpur (IIT Kharagpur).&lt;/p&gt;

&lt;p&gt;##Computer vision components and libraries management&lt;/p&gt;

&lt;p&gt;The project is about designing and implementing a system for object detection and recognition in 3D point clouds and 2D images, and come up with a structured library with a good and easy-to-use APIs.
There has been a good amount of research in this direction and my work was to cherry-pick important ideas and present them as usable components. I’ll now explain in details the various methods I chose to
use as a part of this project.&lt;/p&gt;

&lt;p&gt;###Local feature based on 2D images
The idea is the find local features (like SIFT/SURF/ORB etc) in images of the object to be detected and the given test image. If enough matches are found between the descriptors of the to images an object is defined to be found. Important assumption is that the object to be detected must have textures. Advantage is that we get the complete 6 DOF of the object which might be useful for grasping. This comes in several flavors.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Planner objects: If we know the object is planner, we can directly compute its tomography (pose) after the match.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Random objects: If the object is of arbitrary shape it is quite difficult to detect an object with its pose but can be done in a tricky offline phase [1]. A 3D reconstruction is performed through bundle adjustments with the object to be detected to find the 2D - 3D correspondences. On the run time, given an input image, If enough matches are found, the object is detected with its full pose by solving PnP problem.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;###Dense feature based on 2D images:
The idea is the find features over a grid or a region of an image encoding the properties of that region and use that feature in some classification algorithm to perform detection. Naturally, we need to calculate dense feature over all possible region size over the image and apply the classifier; and thus it is bit slow as well. Also object pose is not identified in this type. Few of them are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;HOG based simple classification (well known).
Difficulty in implementation: Moderate; HOG implementation with multiscale detector is present in OpenCV; but the training has to be performed separately using 3rd party tool like libsvm/matlab etc (but is straightfwd).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;HOG based Part Based Model: This is the famous and legendary and state of art (not anymore) object detector which uses LSVM.
Difficulty in implementation: Difficult; OpenCV has the detection code, but not that good. Training LSVM is not straight fwd and we need to use the original Matlab implementation of the authors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Wevlet based face detector with adaboost: This is also well known face detector algorithm used widely.
Difficulty in implementation: Easy; though the concept is not that straight fwd, it is readily avilable in OpenCV.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;###Detection/Recognition on Depth Images
If we can get the Point Cloud with some laser scan or Kinect, there are plenty of algorithms to detect object with its pose. Again we have local feature based and global feature based algorithms described below:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Direct object with local and global features [4]:
Very similar to that of RGB image based algorithm with difference in the types of features. Local features have the advantage that preprocessing steps like segmentation is not required but tends to be slow. On the other hand we need to do segmentation to apply global features in the clusters. But once the segmentation (like identifying planes, etc and different clusters) of the scene is done, we can use the results subsequently. 
Difficulty in implementation: Easy; components of pipeline is available in PCL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Object matching using classifiers: 
Global features readily available in PCL and found it to have similar results to a current benchmark but faster (10 seconds for classification testing in the benchmark [2] which uses sliding window based classification on all scales using HOG like descriptors).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;##The library - Open Detection
It was decided later to have an independent library for Object Detection instead of integrating everything to Robocomp. The result is the inception of a separate library ‘Open Detection’.
The details of the design of the library is discussed in the next blog.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;[1] I. Gordon and D. G. Lowe, “What and where: 3d object recognition with accurate pose,” in Toward Category-Level Object Recognition, ser. Lecture Notes in Computer Science, J. Ponce, M. Hebert, C. Schmid, and A. Zisserman, Eds., vol. 4170. Springer, 2006, pp. 67–82.
[2] MOPED: A Scalable and Low Latency Object Recognition and Pose Estimation System
[3] Object Detection with Discriminatively Trained Part Based Models
[4] Aldoma, A.; Marton, Zoltan-Csaba; Tombari, F.; Wohlkinger, W.; Potthast, C.; Zeisl, B.; Rusu, R.B.; Gedikli, S.; Vincze, M., “Tutorial: Point Cloud Library: Three-Dimensional Object Recognition and 6 DOF Pose Estimation,” Robotics &amp;amp; Automation Magazine, IEEE , vol.19, no.3,&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title><i>GSoC,</i> Computer vision components and libraries management <p>#2</p></title>
   <link href="http://robocomp.github.io/website/2015/07/02/Kripa2/"/>
   <updated>2015-07-02T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/07/02/Kripa2</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Open Detection:&lt;/strong&gt; Following the idea that it is better to have an independent library for Object Detection than contributing directly to Robocomp, I created the new library ‘Open Detection’. It is available now in the following links&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Github link: https://github.com/krips89/opendetection&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Documentation link: http://krips89.github.io/opendetection_docs/index.html&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have tried to document/provide tutorial inside the library whenever possible. So instead of writing everything here in the blog I’ll just post links to the tutorials/documentations.&lt;/p&gt;

&lt;p&gt;###Installation Instructions
* Link: https://github.com/krips89/opendetection/blob/master/doc/tutorials/content/installation_instruction.rst&lt;/p&gt;

&lt;p&gt;###Library Design
The basic idea was to have a library with common and simple interface giving access to varies detection methods available here. After some thinking I came up with the design explained in the following tutorial:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://github.com/krips89/opendetection/blob/master/doc/tutorials/content/basic_structures.rst&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The class diagrams providing a good reference is provided here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;http://krips89.github.io/opendetection_docs/inherits.html&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;###Documentation 
I did not document extensively till now as building an independent library from the scratch took a long time. The other very important reason is that the design is till little vulnerable to changes.
I would wait little bit more for the design to be more concrete before I start documenting extensively.&lt;/p&gt;

&lt;p&gt;##Things finished 
Within this time frame I could finish the following tasks:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Design of the library.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Complete CMake infrastructure for modular building of the library from scratch.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;2D feature based object detection (both Training and Detection phase) with demo.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Global feature based object detection (both training and detection phase) with demo.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Auto generated Documentation using Doxygen (http://krips89.github.io/opendetection_docs/index.html).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sphinx based tutorial section to generate nice pages for tutorials and blogs like that of PCL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Few other Utility classes which fits the needs and design for the library.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;###Milestones and things learnt:
In the next blog I’ll add the different sources I used to design and implement the above tasks and the things I learnt in this process.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Kripasandhu Sarkar&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Build tools</title>
   <link href="http://robocomp.github.io/website/2015/06/26/nithin6/"/>
   <updated>2015-06-26T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/06/26/nithin6</id>
   <content type="html">&lt;p&gt;###rc_init_ws
This will initialize a Robocomp workspace in the current/specified directory.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rc_init_ws [path]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;###rcbuild
When invoked form workspace without any arguments if not inside source path, it will build all the non-ignored components inside the workspace,  if inside any component source directory it will build only that component. But if a component is specified it will build it.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; rcbuild [-h] [-i | --doc | --installdoc] [component]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;doc&lt;/code&gt; will generate documentation, &lt;code class=&quot;highlighter-rouge&quot;&gt;installdoc&lt;/code&gt; will install the docs to install path, &lt;code class=&quot;highlighter-rouge&quot;&gt;install&lt;/code&gt; will build and install the components. currently you can only generate docs for one component at a time.&lt;/p&gt;

&lt;p&gt;###rccomp
This dosent have much functions as of now. &lt;code class=&quot;highlighter-rouge&quot;&gt;rccomp list&lt;/code&gt; will list all the components.&lt;/p&gt;

&lt;p&gt;###rced
when invoked as component-name file-name. it will open the the file in the component. if multiple files with same name exists, it will give choices and will ask you to choose one. It uses the editor specified in $EDITOR by default, if not present it will use &lt;code class=&quot;highlighter-rouge&quot;&gt;vim&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rced [-h] component file
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;###rcrun
Using rcrun you can start, stop or force stop any component from anywhere. You can also start a component in debug mode, given you have the required &lt;em&gt;config file&lt;/em&gt; in the &lt;em&gt;etc&lt;/em&gt; directory. If you have specified a config file then rcrun will use it to start the component. By default rcrun will use the &lt;code class=&quot;highlighter-rouge&quot;&gt;config&lt;/code&gt; config file in &lt;code class=&quot;highlighter-rouge&quot;&gt;etc&lt;/code&gt; directory, if not found it will search for &lt;code class=&quot;highlighter-rouge&quot;&gt;generic_config&lt;/code&gt; if not found it will use any of config files present.If the debug flag is set, it will search for a config file that ends with &lt;em&gt;.debug&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rcrun [-h] [-s START |-st STOP | -fst FSTOP] [-d | -cf CFILE | -c CONFIG] [-is] [component]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;###rccd
Using this you can cd into the component directory given the component name.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rccd component
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title><i>GSoC,</i> Till Now..., (Before Midterms)</title>
   <link href="http://robocomp.github.io/website/2015/06/25/rajath1/"/>
   <updated>2015-06-25T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/06/25/rajath1</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; Built a website for robocomp using jekyll.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Progress:&lt;/strong&gt; Took the task of building a website for documenting the open source project &lt;em&gt;RoboComp&lt;/em&gt; for the first 4 weeks of Google Summer of Code 2015. The website should be able to segregate the posts into categories, Make it easy for users to post content and most importantly have a proper flow among the posts so that a new users will find it easy to learn the framework.&lt;/p&gt;

&lt;p&gt;Started building the website by using the &lt;a href=&quot;https://github.com/dbtek/dbyll&quot;&gt;dbyll theme&lt;/a&gt;. Messed around with the code a bit and had the website up for robocomp. Website &lt;a href=&quot;https://rajathkumar.github.io/robocomp&quot;&gt;link&lt;/a&gt;. After a few iterations the website was all good. While exploring on the same topic stumbled upon &lt;a href=&quot;https://rohanchandra.github.io/project/type/&quot;&gt;Type theme&lt;/a&gt;. Which was a jekyll based them more clean and elegant than the first one. Ported the entire website to the type theme and currently have shifted the &lt;a href=&quot;http://robocomp.github.io/website/&quot;&gt;website&lt;/a&gt; to the &lt;a href=&quot;https://github.com/robocomp&quot;&gt;robocomp organization on github&lt;/a&gt;. Currently have tweaked the website based on the suggetsions recieved by the mentors.&lt;/p&gt;

&lt;p&gt;Version-1 : https://github.com/robocomp/website/tree/version-1&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Future:&lt;/strong&gt; Will implement automatic segregation of posts based on categories the user mention. Extend the features of the website and add analytics, comments for blog posts etc. Give the website its final iterations based on the suggestions recieved.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Rajath Kumar M.P&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>New build system and workspace model in Robocomp <p>#2</p></title>
   <link href="http://robocomp.github.io/website/2015/06/25/nithin5/"/>
   <updated>2015-06-25T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/06/25/nithin5</id>
   <content type="html">&lt;p&gt;For managing these components we would need different utilities. So i started compiling a list of different utilities keeping a reference to other frameworks. Finally i have decide on my list&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;rc_init_ws&lt;/li&gt;
  &lt;li&gt;rcbuild&lt;/li&gt;
  &lt;li&gt;rced&lt;/li&gt;
  &lt;li&gt;rccd&lt;/li&gt;
  &lt;li&gt;rcrun&lt;/li&gt;
  &lt;li&gt;rccomp&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more information about the utilities see the &lt;a href=&quot;http://robocomp.github.io/website/2015/06/26/nithin6.html&quot;&gt;tutorial&lt;/a&gt; on build utilities. All the utilities are implemented using python except &lt;code class=&quot;highlighter-rouge&quot;&gt;rccd&lt;/code&gt; which is implemented as a shell function, As a subprocess cant affect its parents environment. So i was thinking, as anyway we will need to source a bash script, we could move the exporting of the environment variables also into that script.&lt;/p&gt;

&lt;p&gt;##Future work&lt;/p&gt;

&lt;p&gt;One useful feature that needs to be implemented is auto complete for the arguments. It would be a very useful feature as we don’t need need to know the exact component name, etc. Also some work on the manifest.xml has to be done. It was planned to contain basic info on packages like name, maintainer, dependencies etc. I didn’t do it right now because i was not really sure about the component dependencies, i will need to discuss about it a bit.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title><i>GSoC,</i> 2015 ideas</title>
   <link href="http://robocomp.github.io/website/2015/06/22/gsoc15/"/>
   <updated>2015-06-22T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/06/22/gsoc15</id>
   <content type="html">&lt;p&gt;1.- &lt;strong&gt;RoboComp tutorial, social management and documentation&lt;/strong&gt;: RoboComp’ sources has been ported to GitHub and we are building a new documentation repository there. We are using GitHub markdown language (GFM) write new docs and turorials. We want to build a set of short tutorials that guide the new users along several interconnected topics, such as component oriented programming, robotics, computer vision, robotics software modules integrating heterogeneous sources, cognitive architectures and testing and validating, all from inside RoboComp. These new tutorials will be developed using RoboComp’s robotics simulator, RCIS, so interactive examples can be created and used in the explanations. This package also includes work on automated installation scripts using CMake. Generic knowledge of linux systems, website and wiki administration is needed. This is a key task for our project as it would bring more attention to it as it will open the development to new people interested in the field.&lt;/p&gt;

&lt;p&gt;Required student level: intermediate programming and systems administration.&lt;/p&gt;

&lt;p&gt;2.- &lt;strong&gt;Computer vision components and libraries management&lt;/strong&gt;: RoboComp is being used to build a new cognitive architecture called RoboCog. Among the different modules already in progress, the object detection module is crucially important. We are pursuing an efficient 2D/3D vision pipeline that, working with the robot body control module, is able to localize, recognize, fit a pre-existing model and track a series of daily objects that the robot might encounter. Grasping would be one target of this pipeline, or even a means to complete recognition. There are currently many components implementing computer vision algorithms and intensive work done in pipeline construction. The key tasks on this idea would be to collaborate in the creation of high level tools to organize, document and test different pipelines for an specific task. These tools will be designed in collaboration with the mentors and tested in RoboComp’s RCIS simulator and real robots.&lt;/p&gt;

&lt;p&gt;Required student level: intermediate computer vision knowledge, C++ programming, basic CMake knowledge&lt;/p&gt;

&lt;p&gt;3.- &lt;strong&gt;RoboComp Building and deployment system design&lt;/strong&gt;: Current CMake building system in RoboComp is limited only to the core libraries, the RCIS simulator and some additional tools. A very useful task would be to come up with a more complex CMake structure that could build the entire system, including all finished components, without breaking current dependencies. Also, a few scripts will have to be built to compile individual components, run tests, search and inspect the source tree efficiently, check dependencies and documentation requirements. This code needs also to take into account the dependencies between components that can be stored in xml-like files -i,e, manifestos- within the component itself.&lt;/p&gt;

&lt;p&gt;Required student level: intermediate CMake knowledge, programming in C++, basic knowledge of shell scripting&lt;/p&gt;

&lt;p&gt;4.- &lt;strong&gt;Deployment generator and run-time monitoring&lt;/strong&gt;: When creating a specific robot architecture, many components have to be brought into a common deployment environment. Each component has its runtime configuration and network parameters that have to be declared in a common deployment file, from where the complete system can be brought to life. This task proposes the design of a domain specific language to facilitate the creation of shellscript deployment files that are syntactically and semantically correct. Once a net of RoboComp components is up and running, additional tools are needed to monitorize their execution through an existing default interface called CommonBehavior.This tool will use the DSL as input and will show a graphical representation of the running system. It will be written in Python and will extend important efforts already made in this direction.&lt;/p&gt;

&lt;p&gt;Required student level: intermediate programming with Python and introductory knowledge of formal languages&lt;/p&gt;

&lt;p&gt;For any questions, proposals, or comments please contact RoboComp’s org admin at:
marcogunex.es&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Robocomp Workspace Model</title>
   <link href="http://robocomp.github.io/website/2015/06/20/nithin4/"/>
   <updated>2015-06-20T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/06/20/nithin4</id>
   <content type="html">&lt;p&gt;The Robocomp workspace is for those who are developing components rather than the framework itself. The main advantage of having a workspace is that it will make the work-flow much easier. Workspace basically organizes the development. For example, currently for building or running a component you have to go to its directory, create a build directory and then use cmake, while by using workspace and build tools you could achieve the same in a single command.&lt;/p&gt;

&lt;p&gt;I have started the workspace model design keeping in mind the following points.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;you should be able to build all components at once, if necessary and also separately&lt;/li&gt;
  &lt;li&gt;the source tree should be kept clean&lt;/li&gt;
  &lt;li&gt;It should scalable and also existing components should be easily moved in&lt;/li&gt;
  &lt;li&gt;dependencies should be easily handled&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Referring to other similar workspace models i came up with the following model.&lt;/p&gt;

&lt;p&gt;###The recommended layout for development is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/robocomp/website/gh-pages/img/workspace_model.png&quot; alt=&quot; Robocomp workspace model&quot; title=&quot;Robocomp workspace model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;##Elements of workspace&lt;/p&gt;

&lt;p&gt;###Workspace
The workspace is the folder inside which you are going to be actively developing components. Keeping things in a folder with connected development helps keep separation of development models. In simple words, a workspace can be thought of as a group different components, for example Robocomp has some default components, you may as well create some components so in this case you Robocomp’s components can be in a workspace while your components in another. The config file in ~/.config/RoboComp/rc_worksapce.config maintains a list of all the registered workspaces.&lt;/p&gt;

&lt;p&gt;###Source space
The source space (a folder inside workspace) contains the source code of all the components in the workspace or this is where you will be developing. The source space is the folder where build tools will look for components. This folder is easily identified as it is where the toplevel.cmake is linked Robocomp installed folder and the name &lt;code class=&quot;highlighter-rouge&quot;&gt;src&lt;/code&gt;. Each component should be in a direct subdirectory. If the directory contains a file named &lt;em&gt;IGNORE_COMP&lt;/em&gt; the component will be ignored while building the workspace.&lt;/p&gt;

&lt;p&gt;###Build Space
The build space is the folder in which cmake is invoked and generates artifacts such as the CMakeCache. This need not be a direct sub directory of workspace, it can be any where. This is basically an &lt;em&gt;build&lt;/em&gt; directory of all the components.&lt;/p&gt;

&lt;p&gt;###Development Space
The development space is where build system generates the binaries and config files which are executable before installation.It will have a septate directory for each components. Each component directory contains a folder &lt;code class=&quot;highlighter-rouge&quot;&gt;bin&lt;/code&gt; which has the build binaries and a &lt;code class=&quot;highlighter-rouge&quot;&gt;etc&lt;/code&gt; directory which contain config files. This should be a direct subdirectory of workspace. Currently the &lt;code class=&quot;highlighter-rouge&quot;&gt;devel space is merged with the source space&lt;/code&gt; as you can seen in the layout graph.&lt;/p&gt;

&lt;p&gt;###Install Space
This is the default directory in to which the components in current workspace will get installed along with generated docs. This directory contains a file named &lt;em&gt;.rc_install&lt;/em&gt; which marks this as an install space. Please note that the robocomp install path &lt;em&gt;/opt/robocomp&lt;/em&gt; is also an install space by default.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title><i>GSoC,</i> Symbolic planning techniques for recognizing objects domestic <p>#3</p>, Visual Inverse Kinematics</title>
   <link href="http://robocomp.github.io/website/2015/06/17/mercedes3/"/>
   <updated>2015-06-17T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/06/17/mercedes3</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;Visual inverse kinematics, Basic understanding :&lt;/strong&gt; In the previous post we anticipate the problems caused by the gaps and inaccuracies of motors in the inverse kinematics of the robot. Now, in this third post we will talk about the solution implemented during the GSoC15 project.&lt;/p&gt;

&lt;p&gt;So, with the inverse kinematics component that we have implemented in Robocomp, we had the problem of inaccuracies and gaps in the robotic arm motors, problems that made the robot believed reach the target position without having actually achieved it. To solve this problem it was decided to implement a solution inside the visual field (which is what concerns us throughout this project), whose aim is to provide the inverse kinematics component a visual feedback that allows correct its mistakes. The operation of the algorithm is very simple and takes as its starting point the investigations of Seth Hutchinson, Greg Hager and Peter Corke, collected in &lt;code class=&quot;highlighter-rouge&quot;&gt;A Tutorial on Visual Servo Control&lt;/code&gt; [1].&lt;/p&gt;

&lt;p&gt;##’Looking’, then ‘moving’&lt;/p&gt;

&lt;p&gt;As Hutchinson, Hager and Corke reflect in their work:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Vision is a useful robotic sensor since it mimics the human sense of vision and allows for non-contact measurement of the environment. […] Typically visual sensing and manipulation are combined in a open-loop fashion, ‘looking’ then ‘moving’.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So the goal of &lt;code class=&quot;highlighter-rouge&quot;&gt;Visual servo control&lt;/code&gt; is to control the movement and location of the robot using visual techniques (detection and recognition of objects in an image). To get an idea how it works, we must have clear some fundamental concepts in this field&lt;/p&gt;

&lt;p&gt;###Kinematics of a robot&lt;/p&gt;

&lt;p&gt;We need to know what a kinematic chain is, what reference system and transformation coordinate are anda what algorithm is executed inside the robot kinematic. These concepts were explained in the second post of this collection. If you have doubts, consult it.&lt;/p&gt;

&lt;p&gt;If we link the kinematic chains concept with visual techniques (ie, now, in addition to the chain formed by the motors of the robotic arm, we have a camera in the chain looking one of the chain ends), we have two types of systems:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Endpoint open-loop (EOL): Systems which only observed the target object. These systems don’t need to look at his end effector so normally the camera is on the end effector (hand-eye).&lt;/li&gt;
  &lt;li&gt;Endpoint closed-loop (ECL): Systems which observed the target object and the end effector of the arm.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The visual inverse kinematics that we implemented in Robocomp uses this last configuration because is independent of hand-eye calibration errors (precisely, the clearances errors and inaccuracies that bother us in the inverse kinematics), although often requires solution of a more demanding vision problem, because we need to track the end effector.&lt;/p&gt;

&lt;p&gt;###Camera Projection Models&lt;/p&gt;

&lt;p&gt;We need to understand the geometric aspects of the imaging process if we want to understand how the information provided by the vision system is used to control the movement of the robot. The first thing to consider is that an image taken by a camera is always in 2D, so we’re losing spatial information (the depth of the scene).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://masters.donntu.org/2012/etf/nikitin/library/article10.files/image10.01.png&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To resolve this issue we have several options:
1. We can use multiple cameras that capture the studio space from different positions.
2. We can obtain multiple views with a single camera.
3. We can have previously stored the geometric relationship between certain characteristics of the target or the elements in the studio space.&lt;/p&gt;

&lt;p&gt;In any case, we must keep in mind certain things common to all cameras. For example the system of axes: the &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; axes form the basis of the image plane and the &lt;code class=&quot;highlighter-rouge&quot;&gt;Z&lt;/code&gt; axis is perpendicular to the image plane, along the optical axis of the camera. The origin is located on the &lt;code class=&quot;highlighter-rouge&quot;&gt;Z&lt;/code&gt; axis at a distance &lt;code class=&quot;highlighter-rouge&quot;&gt;λ&lt;/code&gt; of the image plane. That distance &lt;code class=&quot;highlighter-rouge&quot;&gt;λ&lt;/code&gt; is what we call focal length.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.hitl.washington.edu/artoolkit/documentation/images/ch03-17.gif&quot; alt=&quot;ALt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can map the position and the orientation of the end effector in space calculating the projective geometry of the camera. But this method, complicated in itself, increases their difficulty because we need &lt;code class=&quot;highlighter-rouge&quot;&gt;recognize&lt;/code&gt; the end effector in the picture, in addition to deriving the speed from the changes observed in each frame that the camera capture. For these reasons, in our visual inverse kinematic component, we use the algorithm proposed by Edwin Olson, &lt;code class=&quot;highlighter-rouge&quot;&gt;Apriltags&lt;/code&gt; [2] a visual fiducial system that uses a 2D barcode style &lt;code class=&quot;highlighter-rouge&quot;&gt;tag&lt;/code&gt; (binary, black and white synthetic brands), allowing full 6 DOF localization of features from a single image. Thus, if we put a apriltag in the end effector, we can get its position and orientation in a very simple way.&lt;/p&gt;

&lt;p&gt;##visualBIK component&lt;/p&gt;

&lt;p&gt;Having already some clear concepts, let us study how the component developed in this project, &lt;code class=&quot;highlighter-rouge&quot;&gt;visualBIK&lt;/code&gt;, works.&lt;/p&gt;

&lt;p&gt;Our component implements a simple state machine where waits the reception of a target position (a vector with traslations and rotations: [tx, ty, tz,    rx, ry, rz]) through its interface. When a target is received, the visualBIK send it to the inverse kinematics component like a &lt;code class=&quot;highlighter-rouge&quot;&gt;POSE6D&lt;/code&gt; target, and waits for him to finish running the target and placing the arm. As the end effector will be a little out of the target position (due to inaccuracies), the visualBIK will be prepared to correct this error:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It calculates the visual pose of the end effector (through apriltags, visualBIK receives the position of the end effector mark that the camera head sees).&lt;/li&gt;
  &lt;li&gt;After, it compute the error vector between the visual pose and the target pose.&lt;/li&gt;
  &lt;li&gt;With this error vector, visualBIK corrects the target pose and sends the new position to the inverse kinematics component.&lt;/li&gt;
  &lt;li&gt;This process is repeated until the error achieved in translation and rotation is less than a predetermined threshold.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this way we can correct the errors introduced by the inaccuracies of the joints.&lt;/p&gt;

&lt;p&gt;This component (like component inverse kinematics) is still in the testing phase and is more than likely suffer some changes that improve its operation.&lt;/p&gt;

&lt;p&gt;Bye!&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;[1] Hutchinson, S., Hager, G., Corke, P. &lt;code class=&quot;highlighter-rouge&quot;&gt;A Tutorial on Visual Servo Control&lt;/code&gt;, IEEE Trans. Robot. Automat., 12(5):651–670, Oct. 1996. Download in http://www-cvr.ai.uiuc.edu/~seth/ResPages/pdfs/HutHagCor96.pdf&lt;/p&gt;

&lt;p&gt;[2] OLson, E. &lt;code class=&quot;highlighter-rouge&quot;&gt;AprilTag: A robust and flexible visual fiducial system&lt;/code&gt;, Robotics and Automation (ICRA), 2011 IEEE International Conference on, 3400-3407&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title><i>GSoC,</i> Symbolic planning techniques for recognizing objects domestic <p>#2</p>, Inverse Kinematics</title>
   <link href="http://robocomp.github.io/website/2015/06/15/mercedes2/"/>
   <updated>2015-06-15T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/06/15/mercedes2</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;What is inverse kinematics?&lt;/strong&gt; : In this second post, although it may seem begin the house from the roof, let’s talk about how a robot moves its arms and hands in order to manipulate daily objects.&lt;/p&gt;

&lt;p&gt;The ultimate goal of this work is make the robot to be able to recognize certain daily objects in a house (for example a mug), and to manipulate these objects with its effectors (hands). To do this, one of the things we need to implement is the inverse kinematics of the robot. Although this is the last step, we start by inverse kinematics to be easier and more intuitive than object recognition (besides that we have almost finalized the cinematic component in Robocomp).&lt;/p&gt;

&lt;p&gt;###What does the inverse kinematics?&lt;/p&gt;

&lt;p&gt;A recurring problem in robotics is to give to robots a certain autonomy in terms of movement. Focusing on a practical and realistic example, as is the trajectory of a robotic arm from an initial position to a target point, the question is how does the robot move its arm from the starting pose to the final pose? or what values take its engines arm to reach the final position? This is the typical problem of inverse kinematics, which is responsible for calculating the angular values of a kinematic chain composed engines (joints) of the arm to reach a target position.&lt;/p&gt;

&lt;p&gt;But before we get down to work, we need to review a few concepts.&lt;/p&gt;

&lt;p&gt;###Previous concepts&lt;/p&gt;

&lt;p&gt;####kinematic chains&lt;/p&gt;

&lt;p&gt;The first concept that we should be clear is the &lt;code class=&quot;highlighter-rouge&quot;&gt;kinematic chain&lt;/code&gt;. The kinematic chain is a set of elements that produce motion, deforming the chain to adapt it to movement. Kinematic chains are composed of two elements:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;joints&lt;/code&gt;: joints or motors that produce the movement. Each joint gives a degree of freedom.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;links&lt;/code&gt;: rigid segments that connect the joints together.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;An example of kinematic chain in robotic is the arm of the robot, that is composed by all the motors that the robot has and the segments that connect this motors in order to create the arm form.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.sitenordeste.com/mecanica/images/cadena_cinematica.JPG&quot; alt=&quot;Alt text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;####Reference systems and Transformation coordinate.&lt;/p&gt;

&lt;p&gt;One of the problems of robot manipulators is to know where their structural elements are arranged in the space in which they move. We therefore need a referral system that puts or position the elements of the robot in the workspace. So, a &lt;code class=&quot;highlighter-rouge&quot;&gt;reference system&lt;/code&gt; is a set of agreements or conventions used by an observer to measure positions, rotations and other physical parameters of the system being studied. In our case, the arm of the robot is into the three-dimensional workspace (R³, with the axis X, Y and Z), where each components (for example, each joint) has one traslation (tx, ty, tz) and one rotation (rx, ry, rz). Therefore, the position of each component is given by a vector of six elements: &lt;code class=&quot;highlighter-rouge&quot;&gt;P=[tx, ty, tz,   rx, ry, rz]&lt;/code&gt; (the first three translational and three rotational recent). Normally, we represent the poses by homogeneous trasnformation matrices, which are of the form:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    | R  T |
P = | 0  1 |
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;where &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt; is the rotation matrix and &lt;code class=&quot;highlighter-rouge&quot;&gt;T&lt;/code&gt; the traslation coordenates.&lt;/p&gt;

&lt;p&gt;One of the kinematic problems is that each motor (which can be moved and/or rotated with respect to the previous motor of the chain) has his own reference system, so if we want to calculate the position of a particular point or joint, we will have to make a number of changes (&lt;code class=&quot;highlighter-rouge&quot;&gt;transformations&lt;/code&gt;) to move from one reference system to another. For example, if we have the newt arm:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;X_1--------------X_2--------X_3-----O
       M_1&amp;gt;2          M_2&amp;gt;3    M_3&amp;gt;O
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;where &lt;code class=&quot;highlighter-rouge&quot;&gt;X_n&lt;/code&gt; represents the position of the joints, &lt;code class=&quot;highlighter-rouge&quot;&gt;-&lt;/code&gt; is the link that connects the joints, &lt;code class=&quot;highlighter-rouge&quot;&gt;o&lt;/code&gt; is the end effector of the arm and &lt;code class=&quot;highlighter-rouge&quot;&gt;M_n&amp;gt;m&lt;/code&gt; are the transformation matrices to change the reference system n to the system m, and we want to calculate the position of the end effector in the reference system of the joint X_1, we have to calculate this equation:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Po_inX_1 = M_2&amp;gt;1 * M_3&amp;gt;2 * M_o&amp;gt;3 * Po_inO.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;###Problems to solve the inverse kinematics.&lt;/p&gt;

&lt;p&gt;If the forward kinematics is responsible for calculating the position of the end effector in a kinematic chain, given some angular values for the joints, the inverse kinematics is just the opposite: it is responsible for calculating the angle values of the joints so the end effector reaches a position. This last problem is much more difficult to solve. So difficult that we are forced to use generic mathematical methods that try to approach an optimal solution iteratively within a reasonable time. We have opted for an iterative method known as the &lt;code class=&quot;highlighter-rouge&quot;&gt;Levenberg-Marquardt&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;damped least squares&lt;/code&gt; algorithm. This method is used for solving nonlinear least squares problems where a solution to decrease an error function is sought.&lt;/p&gt;

&lt;p&gt;###Inverse kinematics in Robocomp&lt;/p&gt;

&lt;p&gt;As a result of the TFG, &lt;code class=&quot;highlighter-rouge&quot;&gt;Inverse kinematics in Social Robots&lt;/code&gt; [1], since 2014 Robocomp has a component [2] that is responsible for calculating the inverse kinematics of the social robot Ursus [3], developed by Robolab. This component has undergone a big evolution, since it was created last year to now, and is more than likely to continue evolving to achieve inverse kinematics each finer and in less time.&lt;/p&gt;

&lt;p&gt;Originally, this component receives three types of targets:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;POSE6D: It is the typical target with translations and rotations in the X, Y and Z axis. The end effector has to be positioned at coordinates (tx, ty, tz) of the target and align their rotation axes with the target, specified in (rx, ry, rz).&lt;/li&gt;
  &lt;li&gt;ADVANCEAXIS: its goal is to move the end effector of the robot along a vector. This is useful for improving the outcome of the above problem, for example, imagine that the hand has been a bit away from a mug. With this feature we can calculate the error vector between the end effector and the mug, and move the effector along the space to place it in an optimal position, near the mug.&lt;/li&gt;
  &lt;li&gt;ALIGNAXIS: Its goal is that the end effector is pointing to target without moving to it but rotated as the target. It may be useful in certain cases where we are more interested in oriented the end effector with the same rotation of the target.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To solve these various inverse kinematic problems, the component uses as main base the &lt;code class=&quot;highlighter-rouge&quot;&gt;Levenberg-Marquardt&lt;/code&gt; algorithm proposed in the article &lt;code class=&quot;highlighter-rouge&quot;&gt;SBA: A Software Package for Generic Sparse Bundle Adjustment&lt;/code&gt; by Lourakis and Argyros:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Input: A vector functon f: R^m → R^n with n≥m, a measurement vector x ∈ R^n and an initial parameters estimate p_0 ∈ R^m.
Output: A vector p+ ∈ R^m minimizing ||x-f(p)||^2.
Algorithm:
    k:=0;                 v:=2;                     p:=p0;
    A:=transposed(J)·J;   error:=x-f(p);            g:=transposed(J)·error;
    stop:=(||g||∞ ≤ ε1);  μ:=t*max_i=1,...,m (Aii)
    
    while(!stop) and (k&amp;lt;k_max)
         k:=k+1;
         repeat
               SOLVE (A+μ·I)·δ_p=g;
               if(||δ_p||≤ ε2·(||p||+ε2))
                    stop:=true;
               else
                    p_new:=p+δ_p
                    ρ:=(||error||^2-||x-f(p_new)||^2)/(transposed(δ_p)·(μ·δ_p+g));
                    if ρ&amp;gt;0
                        stop:=(||error||-||x-f(p_new)||&amp;lt;ε4·||error||);
                        p:=p_new;
                        A:=transposed(J)·J;    error:=x-f(p);    g:=transposed(J)·error;
                        stop:=(stop) or (||g||∞ ≤ ε1);
                        μ:=μ*max(1/3, 1-(2·ρ-1)^3);
                        v:=2;
                    else
                        μ:=μ*v;
                        v:=2*v;
                    endif
               endif
         until(ρ&amp;gt;0) or (stop)
         stop:=(||error||≤ ε3);
    endwhile
    p+:=p;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Where &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; is the hessian matrix, &lt;code class=&quot;highlighter-rouge&quot;&gt;J&lt;/code&gt; is the jacobian matrix, &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; is the gradient descent, &lt;code class=&quot;highlighter-rouge&quot;&gt;δ_p&lt;/code&gt; is the increments, &lt;code class=&quot;highlighter-rouge&quot;&gt;ρ&lt;/code&gt; is the ratio of profit that tells us if we are approaching a minimum or not, &lt;code class=&quot;highlighter-rouge&quot;&gt;μ&lt;/code&gt; is the damping factor, and &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;ε1, ε2, ε3, ε4&lt;/code&gt; are different thresholds. But the IK component of Robocomp adds several concepts to the original L-M algorithm, in order to complete the proper operation of the component:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Weight matrix: that controls the relevance between the translations (in meters) and rotations (in radians) of the target. So, where &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; was calculated as &lt;code class=&quot;highlighter-rouge&quot;&gt;transposed(J)·error&lt;/code&gt;, now &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; is &lt;code class=&quot;highlighter-rouge&quot;&gt;transposed(J)·(W·error)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Motors lock: when a motor reachs its minimun or maximun limit, we modified the jacobian matrix.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The new version of the inverse kinematics component simplifies the code of the old version and adds some more functionality:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Executes more than once a target. The inverse kinematic result is not the same if the start point of the effector is the robot’s home or a point B near tho the goal point.&lt;/li&gt;
  &lt;li&gt;Executes the traslations without the motors of the wrisht (only for Ursus). This makes possible to move the arm with stiff wrist, and then we can rotate easely the wrist when the end effectos is near the target.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Another improvement being studied is to include a small planner responsible for planning the trajectories of the robot arm, in order to facilitate the work of the IK component and reduce its execution time. However, one of the problems that the inverse kinematics can not solve by itself is the problem of gaps and imperfections of the robot. These gaps and inaccuracies make the robot move its arm toward the target position improperly, so that the robot “thinks” that the end effector has reached the target but in reality has fallen far short of the target pose.&lt;/p&gt;

&lt;p&gt;In order to solve this last problem, we need visual feedback to correct the errors and mistakes introduced for the gaps and inaccuracies in the kinematic chain. The visualBIK component, developed during this project, is responsible for solve this visual feedback and correct the inverse kinematic, but we’ll talk about it in the next post.&lt;/p&gt;

&lt;p&gt;Bye!&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;[1] Master Thesis, Universidad de Extremadura, Escuela Politécnica de Cáceres. Mercedes Paoletti Ávila. &lt;code class=&quot;highlighter-rouge&quot;&gt;Cinemática Inversa en Robots Sociales&lt;/code&gt;. Directed by Pablo Bustos and Luis Vicente Calderita. July 2014. Download in https://robolab.unex.es/index.php?option=com_remository&amp;amp;Itemid=53&amp;amp;func=startdown&amp;amp;id=143&lt;/p&gt;

&lt;p&gt;[2] inverse kinematics component repository: https://github.com/robocomp/robocomp-ursus/tree/master/components/inversekinematics&lt;/p&gt;

&lt;p&gt;[3] C. Suárez Mejías, C. Echevarría, P. Núñez, L. Manso, P. Bustos, S. Leal and C. Parra. &lt;code class=&quot;highlighter-rouge&quot;&gt;Ursus: A Robotic Assistant for Training of Patients with Motor Impairments&lt;/code&gt;. Book, Converging Clinical and Engineering Research on Neurorehabilitation, Springer series on BioSystems and BioRobotics, Editors, J.L Pons, D. Torricelli and Marta Pajaro. Springer, ISBN 978-3-642-34545-6, pages 249-254. January 2012. Download in https://robolab.unex.es/index.php?option=com_remository&amp;amp;Itemid=53&amp;amp;func=startdown&amp;amp;id=128&lt;/p&gt;

&lt;p&gt;[4] Lourakis, M. I., Argyros, A. (2009). &lt;code class=&quot;highlighter-rouge&quot;&gt;SBA: A Software Package for Generic Sparse Bundle Adjustment&lt;/code&gt;. Article of ACM Transactions on Mathematical Software, volume 36, issue 1, pages 1-30. Download in http://doi.acm.org/10.1145/1486527&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Debian Packaging 2</title>
   <link href="http://robocomp.github.io/website/2015/06/12/nithin8/"/>
   <updated>2015-06-12T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/06/12/nithin8</id>
   <content type="html">&lt;p&gt;##What is a binary package?
A binary package in a is an application package which contains (pre-built) executables, as opposed to source code. Basically a binary package is an archive which contains executables some other info like rules on how to install them, dependencies etc. debian binary package is also a type of binary package. You can use a package manger to install these packages.I have explained I have explained it in tutorial &lt;a href=&quot;http://robocomp.github.io/website/2015/05/23/nithin1.html&quot;&gt;Debian packaging&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;##Implementation in Robocomp
For binary packages i was left with two options. whether i could use the same cmake script i used for creating source packages or i could use the &lt;code class=&quot;highlighter-rouge&quot;&gt;CPACK&lt;/code&gt; packaging tool. Finally i decided to go with CPACK because - 1)less code , that means less messing up 2)its an well known tool so is expected to perform better than a script i am writing. CPACK has so many configuration options so i made a seperate cmake file &lt;code class=&quot;highlighter-rouge&quot;&gt;package_details.cmake&lt;/code&gt; for configuring cpack so that its easier for users to change any configuration. CPACK will add a target &lt;code class=&quot;highlighter-rouge&quot;&gt;package&lt;/code&gt; for generating binary package. so you could run &lt;code class=&quot;highlighter-rouge&quot;&gt;make package&lt;/code&gt; for generating the package.&lt;/p&gt;

&lt;p&gt;##Source packages and ppa
A Personal Package Archive (PPA) s a special software repository for uploading source packages to be built and published as an APT repository by Launchpad.So basically if a software has a ppa then users can just add the pa to their sources and they will be able to install the software package and will also get updates automatically. As alreasy mentioned we can only upload source packages into a ppa, by definition &lt;em&gt;Source packages provide you with all of the necessary files to compile or otherwise, build the desired piece of software.&lt;/em&gt; now the next question is how can we create source packages. I have explained it in tutorial &lt;a href=&quot;http://robocomp.github.io/website/2015/05/23/nithin1.html&quot;&gt;Debian packaging&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;##Implementation in Robocomp
For creating source package for robocomp i wrote a cmake module &lt;em&gt;source_package&lt;/em&gt;.The module will basically copy the source in to another directory (currently &lt;em&gt;Debian&lt;/em&gt; in build folder) and will create the source tar.Then it will create all debian/ files dynamically. The script will be executed when the target &lt;code class=&quot;highlighter-rouge&quot;&gt;spackage&lt;/code&gt; is made.&lt;/p&gt;

&lt;p&gt;After creating the source packages one trouble i faces was in setting up (registering) the PGP keys. Once you have created a launchpad account you should sign the Ubuntu Code of Conduct.Then you can upload the package using &lt;code class=&quot;highlighter-rouge&quot;&gt;dput&lt;/code&gt; utility.&lt;/p&gt;

&lt;p&gt;###NB
* Unfortunately CPACK has a bug in it, its not changing the control files permission correctly which throws a warning during installation. so i have create a bash script which will fix the control file permissions.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;You cant upload a package with same name into same ppa again, launchpad will reject it. so you need to change the package name and hence the version, every time you upload. This is automatically taken care off by the script.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If there are any changes to the source, then you should upload the whole source into the ppa. Well, any changes to fies in &lt;em&gt;debian&lt;/em&gt; folder is not considered a source change. In Robocomp its implemented in such a way that if there is any changes to the source, then you have to change the Robocomp version.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Right now we are not generating the changelog automatically. But that is a feature we could add, generating changelog from the git commit messages.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title><i>GSoC,</i> Symbolic planning techniques for recognizing objects domestic <p>#1</p></title>
   <link href="http://robocomp.github.io/website/2015/06/12/mercedes1/"/>
   <updated>2015-06-12T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/06/12/mercedes1</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;About Me&lt;/strong&gt; : Hello! My name is Mercedes Paoletti Ávila, and I would like to introduce me a little in this first post. I’m graduate in engineering from the University of Extremadura, and currently I study the Master in computer engineering and ICT management in the same University. Over the past two years I have been working in the Robotics Laboratory of the UEx, Robolab. There I developed my end grade work, “inverse kinematics in social robots”, using the robotic framework implemented by the laboratory, Robocomp, and now I’m doing my Master’s Thesis, that is about robots planners and is part of the project of LJ Manso, AGM.&lt;/p&gt;

&lt;p&gt;And just following this course of action, this project combines the two concepts (planning and inverse kinematics) for recognizing and manipulating objects, using the framework Robocomp.&lt;/p&gt;

&lt;p&gt;##Symbolic planning techniques for recognizing objects domestic&lt;/p&gt;

&lt;p&gt;The main object of this project is the application of symbolic techniques to build efficient pipelines in order to improve computer vision techniques, recognition and interpretation of domestic objects, to be finally executed on a domestic robot, so that the robot is able to move around a house and identify and interact with any objects located inside. The goal is that given a high-level task (eg. “grab a mug”), we can use symbolic planning techniques to build a pipeline of visual processing.&lt;/p&gt;

&lt;p&gt;In order to reach this goal, we need some previous concepts:&lt;/p&gt;

&lt;p&gt;1) We need a planner to organize the sequence of commands to be executed by the robot. We will use the AGM planner [1].&lt;/p&gt;

&lt;p&gt;2) We need a system to recognize marks or some objects. For now, we wil use the algorithm proposed by Edwin Olson, AprilTags [2]&lt;/p&gt;

&lt;p&gt;3) We also need a system that controls the movement of the robot and corrects the calibration errors and gaps of the engines. We will use the inverse kinematics component developed in Robocomp, which will add techniques of trajectory planning and visual feedback.&lt;/p&gt;

&lt;p&gt;All this provides a robust support that allows the robot to move freely within a given environment, such as a home, and interact with everyday objects that are in it.&lt;/p&gt;

&lt;p&gt;You can acces to the code of these components in the Robocomp repository: https://github.com/robocomp&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;[1] AGM documentation: http://ljmanso.com/agm/&lt;/p&gt;

&lt;p&gt;[2] AprilTags: http://april.eecs.umich.edu/papers/details.php?name=olson2011tags&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Maintaining your own repository of components in GitHub</title>
   <link href="http://robocomp.github.io/website/2015/05/23/using_github/"/>
   <updated>2015-05-23T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/05/23/using_github</id>
   <content type="html">&lt;p&gt;We recommend that you create a repository for your components (i.e. &lt;em&gt;mycomponents&lt;/em&gt; directory in the example before) in your GitHub account (or other similar site) and pull/clone it in &lt;em&gt;~/robocomp/components&lt;/em&gt; whenever yo need it. For example, if your GitHub account is &lt;em&gt;myaccount&lt;/em&gt;, first log in with your browser and create a new repository named &lt;em&gt;mycomponents&lt;/em&gt; following this instructions:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;https://help.github.com/articles/create-a-repo/
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now is good time to write down a short description of what your component does in the README.md file.&lt;/p&gt;

&lt;p&gt;Then we need to clean up the binary and generated files in &lt;em&gt;myfirstcomp&lt;/em&gt;. Note that this is not necessary if you upload the component to the repo just after creating it with DSLEditor and before you type &lt;em&gt;cmake .&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd ~/robocomp/components/mycomponents/myfirstcomp
make clean
sudo rm -r CMakeFiles
rm CMakeCache.txt
rm cmake_install.cmake
rm Makefile
rm *.kd*
rm src/moc*
sudo rm -r src/CMakeFiles
rm src/cmake_install.cmake
rm src/Makefile
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;now we are ready:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd ~/robocomp/components/mycomponents
git init
git remote add origin &quot;https://github.com/myaccount/mycomponents.git&quot;
git add mycomponents
git push -u origin master
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You can go now to GitHub and chek that your sources are there!&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Write a post for robocomp, A step by step guide.</title>
   <link href="http://robocomp.github.io/website/2015/05/23/post_on_webpage/"/>
   <updated>2015-05-23T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/05/23/post_on_webpage</id>
   <content type="html">&lt;p&gt;In this tutorial you will be learning about writing a post for robocomp. I assume that you are already familiar with contributing via Github if you are not then you can follow &lt;a href=&quot;http://rajathkumarmp.github.io/robocomp/tutorial/2015/05/23/contribute/&quot;&gt;this article&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;switch to &lt;code class=&quot;highlighter-rouge&quot;&gt;gh-pages&lt;/code&gt; branch
You can do this via github client or on the command line by navigating to the directory and executing the command&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;`git checkout gh-pages`
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;After checking out to the github pages branch in your navigate to the &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Here you will find all the posts.&lt;/p&gt;

&lt;p&gt;To write a new post. Create a new file and save it as &lt;code class=&quot;highlighter-rouge&quot;&gt;XYZ.md&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Note that you will be using Github markdown language.&lt;/p&gt;

&lt;p&gt;Once you save the file as &lt;code class=&quot;highlighter-rouge&quot;&gt;XYZ.md&lt;/code&gt;. It will be saved as draft and not published on to the website.&lt;/p&gt;

&lt;p&gt;At the header of every article/post you write. Always add this&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
layout:
title: 
---
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Layout can be &lt;code class=&quot;highlighter-rouge&quot;&gt;post&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;page&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;default&lt;/code&gt;. Always set the layout as &lt;code class=&quot;highlighter-rouge&quot;&gt;post&lt;/code&gt;. The title is the title of the post. Categories and Tags should be set accoridingly whichever is applicable. This is helpful in navigating or finding posts on same topic. Description is a short explanation or gist of the entire post.&lt;/p&gt;

&lt;p&gt;A sample header looks like this.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
layout: post
title: Write a post for robocomp, A step by step guide.
---
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;After adding the header you can proceed writing the post by using Github Markdown language. Now for the most important step. To publish the post or to change the post from draft to final you will have to rename the file to&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;`YYYY-MM-DD-XYZ.md`
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Commit and push it to the repo, you would have successfully published a post on to the robocomp’s website.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Creating a new component with eclipse based RoboComp's DSLEditor</title>
   <link href="http://robocomp.github.io/website/2015/05/23/component_creation_with_DSLEditor/"/>
   <updated>2015-05-23T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/05/23/component_creation_with_DSLEditor</id>
   <content type="html">&lt;p&gt;We will create now a new component that will connect to the RCIS simulator and run a simple controller for the robot, using the laser data. First we need to install the DSLEditor software that is runtime Eclipse application.&lt;/p&gt;

&lt;p&gt;Create another terminal in Yakuake and type:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd ~/robocomp/tools
python fetch_DSLEditor.py
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Select 32 or 64 bits according to your current linux installation. After a little while the DSLEditor will be installed under the &lt;em&gt;robocompDSL&lt;/em&gt; directory:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd roboCompDSL/DSLEditor
./DSLEditor
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Check that you have a &lt;em&gt;RoboComp&lt;/em&gt; tab in the upper bar of the DSLEditor window and that the &lt;em&gt;robocomp&lt;/em&gt; directory appears in the Project Explorer (left panel). If it does not, right click inside the &lt;em&gt;Project Explorer&lt;/em&gt; panel and select &lt;em&gt;import&lt;/em&gt;. Then select &lt;em&gt;General&lt;/em&gt; and then &lt;em&gt;Existing Projects into Workspace&lt;/em&gt;. Then select your &lt;em&gt;robocomp&lt;/em&gt; directory and push &lt;em&gt;Finish&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Now we need to bring up some handy tabs in the lower pane. Select the &lt;em&gt;Window&lt;/em&gt; tab in the upper bar, then &lt;em&gt;Show View&lt;/em&gt;, then &lt;em&gt;Other&lt;/em&gt; and again &lt;em&gt;Other&lt;/em&gt;. Select now &lt;em&gt;Interfaces&lt;/em&gt; and double-click on it. Go back to the main window.&lt;/p&gt;

&lt;p&gt;Now, in the left panel, unfold the &lt;em&gt;robocomp&lt;/em&gt; directory down to &lt;em&gt;robocomp/components/&lt;/em&gt; and then click on it with the right button. Select &lt;em&gt;New Folder&lt;/em&gt; and enter &lt;em&gt;mycomponents&lt;/em&gt; in the folder name. Do it again to create a new folder inside &lt;em&gt;mycomponents&lt;/em&gt; named &lt;em&gt;myfirstcomp&lt;/em&gt;. Select &lt;em&gt;myfirstcomp&lt;/em&gt; and then click on the &lt;em&gt;RoboComp&lt;/em&gt; tab in the upper bar of the main window. Select &lt;em&gt;Create CDSL file&lt;/em&gt; and fill the requested name with &lt;em&gt;MyFirstComp.cdsl&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The new file will open inside a syntax-sensitive editor in the central panel. Ctrl-space gives you syntactically correct options. You can see the skeleton of a new empty component. Look for the tab &lt;em&gt;Interfaces&lt;/em&gt; in the lower bar and select &lt;em&gt;DifferentialRobot.idsl&lt;/em&gt;. Click on the green cross at the right of the bar to include it and accept when prompted in a pop-up window. You will see something like:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import &quot;/robocomp/interfaces/IDSLs/DifferentialRobot.idsl&quot;;
Component PFLocalizerComp{
    Communications{
        };
        language Cpp;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Repeat the same steps to include &lt;em&gt;Laser.idsl&lt;/em&gt; and then add a &lt;em&gt;requires&lt;/em&gt; statement inside de &lt;em&gt;Communications&lt;/em&gt; section. The file now should look like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import &quot;/robocomp/interfaces/IDSLs/DifferentialRobot.idsl&quot;;
import &quot;/robocomp/interfaces/IDSLs/Laser.idsl&quot;;
Component MyFirstComp{
    Communications{
        requires DifferentialRobot, Laser;
    };
language Cpp;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Save the file and click in the upper bar on the &lt;em&gt;RoboComp&lt;/em&gt; tab. Select &lt;em&gt;Generate Code&lt;/em&gt;. After a little while the new source tree for your &lt;em&gt;MyFirstComp&lt;/em&gt; component will be created. You can go back now to Yakuake and create a new tab to compile it. Then:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd ~/robocomp/components/mycomponents/myfirstcomp
cmake .
make
bin/myfirstcomp --Ice.Config=etc/generic_config
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;and there it is! your component is running.&lt;/p&gt;

&lt;p&gt;What! Dissapointed? Yeah, I know it does nothing, but it runs and it is yours! Now let’s do some real programming.&lt;/p&gt;

&lt;p&gt;Stop the component with Ctrl Z and then type:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;killall -9 myfirstcomp
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now start your favorite IDE. KDevelop will do it just fine and you have it already installed. Open it in another tab, from Ubuntu menu or with Alt-F2. Then:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Click the *Project* tab in the upper bar
Select *Open/Import Project*
Navigate to ~/robocomp/components/mycomponents/myfirstcomp
Select *Makefile* and open the project
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;In the &lt;em&gt;Project&lt;/em&gt; panel to the left of the screen, navigate to &lt;em&gt;src&lt;/em&gt; and there select &lt;em&gt;specificworker.cpp&lt;/em&gt; and open it. Open also &lt;em&gt;specificworker.h&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Now replace the empty &lt;em&gt;void compute()&lt;/em&gt; method with this compact version of the classic AVOID-FORWARD-STOP architecture proposed by R. Brooks in the late 80’s:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void SpecificWorker::compute( )
{
    static	float rot = 0.1f;			// rads/sec
    static float adv = 100.f;			// mm/sec
    static float turnSwitch = 1;
    const float advIncLow = 0.8;		// mm/sec
    const float advIncHigh = 2.f;		// mm/sec
    const float rotInc = 0.25;			// rads/sec
    const float rotMax = 0.4;			// rads/sec
    const float advMax = 200;			// milimetres/sec
    const float distThreshold = 500; 	// milimetres
try
{
    RoboCompLaser::TLaserData ldata = laser_proxy-&amp;gt;getLaserData();
    std::sort( ldata.begin(), ldata.end(), [](RoboCompLaser::TData a, RoboCompLaser::TData b){ return     a.dist &amp;lt; b.dist; }) ;
    if( ldata.front().dist &amp;lt; distThreshold) 
    {
        adv = adv * advIncLow; 
        rot = rot + turnSwitch * rotInc;
        if( rot &amp;lt; -rotMax) rot = -rotMax;
        if( rot &amp;gt; rotMax) rot = rotMax;
        differentialrobot_proxy-&amp;gt;setSpeedBase(adv, rot);
    }
    else
    {
        adv = adv * advIncHigh; 
        if( adv &amp;gt; advMax) adv = advMax;
        rot = 0.f;
        differentialrobot_proxy-&amp;gt;setSpeedBase(adv, 0.f);		
        turnSwitch = -turnSwitch;
    }	
}
catch(const Ice::Exception &amp;amp;ex)
{
    std::cout &amp;lt;&amp;lt; ex &amp;lt;&amp;lt; std::endl;
}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To compile the fancy version of &lt;em&gt;std::sort&lt;/em&gt; you will have to first add this line at the end of the file &lt;em&gt;CMakeListsSpecific.txt&lt;/em&gt; located in the same &lt;em&gt;src&lt;/em&gt; directory:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ADD_DEFINITIONS( -std=c++11 )
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;and then type:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cmake .
make
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Hereafter, Press F8 in KDevelop to compile and link. Then, go to Yakuake and restart the component.&lt;/p&gt;

&lt;p&gt;Let us take InnerModel &lt;em&gt;simpleworld.xml&lt;/em&gt; as an example. Open a new tab in Yakuake and execute&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd robocomp/files/innermodel
rcis simpleworld.xml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now you should see 2 windows. Now in Yakuake go back to tab where you had compiled &lt;em&gt;myfirstcomp&lt;/em&gt; and run&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bin/myfirstcomp --Ice.Config=etc/generic_config
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You should see the robot maneouvring aroung the box. Now is when Robotics begin! Try to modify the code to let the robot go pass the blocking boxes.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>aprilTagsComp, Tutorial to simulate virtual apriltags</title>
   <link href="http://robocomp.github.io/website/2015/05/23/apritagstutorial/"/>
   <updated>2015-05-23T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/05/23/apritagstutorial</id>
   <content type="html">&lt;p&gt;If you haven’t already, Then do read about aprilTagsComp &lt;a href=&quot;apriltags.md&quot;&gt;here&lt;/a&gt; for better understanding. In this tutorial you will learn the actual functionality of apriltags.&lt;/p&gt;

&lt;p&gt;First make sure you have installed apriltags. Please follow the steps that is given in &lt;em&gt;INSTALL_APRILTAGS_LIB.TXT&lt;/em&gt;. Then move to the apriltagscomp folder&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd ~/robocomp/components/robocomp-robolab/components/apriltagsComp
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Compile by executing&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cmake.
make
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now that you have compiled the component and have the binary generated. Open a new tab in yakuake or terminal and execute&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd robocomp/files/innermodel
rcis simpleworld.xml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Here we will considering &lt;em&gt;simpleworld.xml&lt;/em&gt; as an example since it has virtual apriltags and a robot with a camera is present for simulation. After execution you should now see two windows, One showing the robot camera’s view pointing at one of the apriltags and the other with the site map showing the robot and two virtual apriltags.&lt;/p&gt;

&lt;p&gt;Now go back to the terminal where you had compiled the apriltagsComp and execute&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bin/apriltagscomp --Ice.Config=etc/generic_config
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You should now see the following output.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;user@username:~/robocomp/components/robocomp-robolab/components/apriltagsComp$ bin/apriltagscomp --Ice.Config=etc/generic_config
[/home/username/robocomp/classes/rapplication/rapplication.cpp]: Loading [camera:tcp -h localhost -p 10001] proxy at &#39;CameraProxy&#39;...
18:20:19:421::Info::apriltagscomp.cpp::139::/home/username/robocomp/components/robocomp-robolab/components/apriltagsComp/src/apriltagscomp.cpp::run::CameraProxy initialized Ok!
[/home/username/robocomp/classes/rapplication/rapplication.cpp]: Loading [rgbd:tcp -h localhost -p 10096] proxy at &#39;RGBDProxy&#39;...
18:20:19:421::Info::apriltagscomp.cpp::150::/home/username/robocomp/components/robocomp-robolab/components/apriltagsComp/src/apriltagscomp.cpp::run::RGBDProxy initialized Ok!
[/home/username/robocomp/classes/rapplication/rapplication.cpp]: Loading [rgbdbus:tcp -h localhost -p 10239] proxy at &#39;RGBDBusProxy&#39;...
18:20:19:421::Info::apriltagscomp.cpp::161::/home/username/robocomp/components/robocomp-robolab/components/apriltagsComp/src/apriltagscomp.cpp::run::RGBDBusProxy initialized Ok!
18:20:19:423::Debug::genericworker.cpp::53::/home/username/robocomp/components/robocomp-robolab/components/apriltagsComp/src/genericworker.cpp::setPeriod::Period changed100
18:20:19:423::Info::specificmonitor.cpp::56::/home/username/robocomp/components/robocomp-robolab/components/apriltagsComp/src/specificmonitor.cpp::initialize::Starting monitor ...
InputInterface RGBD
AprilTagsFamily tagCodes36h11
ID:0-10 0.17
ID:11-20 0.17
ID:21-30 0.17
InnerModelPath /home/robocomp/robocomp/files/innermodel/simpleworld.xml
RoboCompAprilTagsComp::AprilTagsComp started
InnerModelReader: reading /home/robocomp/robocomp/files/innermodel/simpleworld.xml
InnerModelRGBD: 0.000000 {10096}
&quot;/home/robocomp/robocomp/files/innermodel/simpleworld.xml&quot;   &quot;rgbd&quot; 
FOCAL LENGHT: 480 
  6.45862 fps
  7.17213 fps
  7.09036 fps
  6.94859 fps
  7.09561 fps
  7.01433 fps
  7.09569 fps
  7.02332 fps
  7.24122 fps
  7.17631 fps
  6.85862 fps
  7.00415 fps
  7.14883 fps
  6.92038 fps
  7.03233 fps
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>aprilTagsComp, Wrapping E. Olson's AprilTags in RoboComp</title>
   <link href="http://robocomp.github.io/website/2015/05/23/apriltags/"/>
   <updated>2015-05-23T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/05/23/apriltags</id>
   <content type="html">&lt;p&gt;AprilTags is an augmented reality tag system developed by E. Olson at the U. of Michigan, USA. A complete explanation and related papers can be found &lt;a href=&quot;http://april.eecs.umich.edu/wiki/index.php/AprilTags&quot;&gt;here&lt;/a&gt;. There is a C++ version written
 by Michael Kaes &lt;a href=&quot;http://people.csail.mit.edu/kaess/apriltags/&quot;&gt;here&lt;/a&gt; which is the one we use.&lt;/p&gt;

&lt;p&gt;April tags are AR tags designed to be easily detected by (robot) cameras. Understand them as a visual fiducial (artificial features) system that uses a 2D bar code style “tag”, allowing full 6 DOF localization of features from a single image. It is designed to encode smaller data (between 4 and 12 bits) and also these tags can be detected by the camera even at odd conditions. When the tag is seen by the camera, the algorithm computes the tag’s complete pose defining its own reference system relative to the camera (i.e Location of the tag is known with high accuracy). This reference system is defined as follows: If we look perpendicularly to a non rotated tag, The Z+ axis comes out towards us from the center of the tag plane, The X+ axis points leftwards and the Y+ axis points upwards (a left-hand reference system). The values computed by &lt;em&gt;apriltagsComp&lt;/em&gt; are the translation vector from the camera to the center of the tag’s reference system, and the three Euler angles that encode the relative orientation of the tag’s reference system wrt to the camera reference system.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;AprilTags.cdsl&lt;/em&gt; file specifies how &lt;em&gt;apriltagsComp&lt;/em&gt; has been generated and how it can be re-generated:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import &quot;/robocomp/interfaces/IDSLs/GetAprilTags.idsl&quot;;
import &quot;/robocomp/interfaces/IDSLs/AprilTags.idsl&quot;;
import &quot;/robocomp/interfaces/IDSLs/RGBD.idsl&quot;;
import &quot;/robocomp/interfaces/IDSLs/RGBDBus.idsl&quot;;
import &quot;/robocomp/interfaces/IDSLs/Camera.idsl&quot;;
Component AprilTagsComp{
    Communications{
            requires Camera, RGBDBus, RGBD;
            publishes AprilTags;
            implements GetAprilTags;
    };
    language Cpp;
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This files tells us that the component requires -will be calling- three RoboComp interfaces: Camera, RGBDBus y RGBD, which are normal and depth camera’s interfaces written in RoboComp’s IDSL language. You can find those files in &lt;em&gt;~/robocomp/interfaces/IDSLs&lt;/em&gt;. Also, the component will publish the data defined in the &lt;em&gt;AprilTags&lt;/em&gt; interface and will implement the &lt;em&gt;GetAprilTags&lt;/em&gt; interface. This means that using images provided by a component implementing the camera or RGBD interfaces, it will try to detect any tags in them and compute their 6D pose. Finally, it will publish a vector with all the tags id’s and poses to the Ice’s STORM broker, and also it will attend any direct requests (remote procedure calls) received from other components through the &lt;em&gt;GetAprilTags&lt;/em&gt; interface. So it is a rather serviceable and handy component!&lt;/p&gt;

&lt;p&gt;To access &lt;strong&gt;apriltagsComp&lt;/strong&gt; you need to install from &lt;em&gt;http://github.org/robocomp&lt;/em&gt; the repository named &lt;em&gt;robocomp-robolab&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd ~/robocomp/components
git clone https://github.com/robocomp/robocomp-robolab.git
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Once downloaded, &lt;em&gt;apriltagsComp&lt;/em&gt; can be found in:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~/robocomp/components/robocomp-robolab/components/apriltagsComp
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;First, read the &lt;em&gt;INSTALL_APRILTAGS_LIB.TXT&lt;/em&gt; file and follow instructions thereby. Once the library has been installed in /usr/local, we can proceed to compile the component:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd ~/robocomp/components/robocomp-robolab/components/apriltagsComp
cmake .
make
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We should have a binary now:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~/robocomp/components/robocomp-robolab/components/apriltagsComp/bin/apriltagscomp
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;##Configuration parameters
As any other component, &lt;em&gt;apriltagsComp&lt;/em&gt; needs a &lt;em&gt;config&lt;/em&gt; file to start. In&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;~/robocomp/components/robocomp-robolab/components/apriltagsComp/etc/generic_config
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;you can find an example of a configuration file. We can find there the following lines:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;GetAprilTagsComp.Endpoints=tcp -p 12210                     //Port where GetAprilTags iface is served
CommonBehavior.Endpoints=tcp -p 11258                       //Not of use for the user now
CameraProxy = camera:tcp -h localhost -p 10001              //Port where a camera is located
RGBDProxy = rgbd:tcp -h localhost -p 10096                  //Port where a RGBD camera is located
RGBDBusProxy = rgbdbus:tcp -h localhost -p 10239            //Port where a bus of RGBDs is located
AprilTagsProxy = apriltags:tcp -h localhost -p 10261        //Not of use for the user
TopicManager.Proxy=IceStorm/TopicManager:default -p 9999    //Port where STROM broker is located
InnerModelPath=/home/robocomp/robocomp/files/innermodel/simpleworld.xml


InputInterface = RGBD                                       //Current input iface to be used
AprilTagsFamily = tagCodes36h11                             //Tags family. See AprilTags paper
AprilTagsSize = 0.17                                        //Tag default real size in meters
ID:0-10 = 0.17   #tag size in meters                        //Tags numbers 1-10 real size in meters
ID:11-20 = 0.17   #tag size in meters                       //Tags numbers 11-20 real size in meters
ID:21-30 = 0.17   #tag size in meters                       //Tags numbers 21-30 real size in meters
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;AprilTagsFamily is a set of tags, There are different families like 36h10,25h9,16h5 however &lt;em&gt;tagCodes36h11&lt;/em&gt; is recommended. Each tag has an ID that is printed inside the surrounding square using Hamming code. Instructions to print tags and other tag families can be found &lt;a href=&quot;http://april.eecs.umich.edu/wiki/index.php/AprilTags&quot;&gt;here&lt;/a&gt;. The algorithm needs the real size of the tag to estimate its position and orientation in space. We can give the component tags of different sizes, As long as they correspond to different ranges of IDs, as specified in the configuration file above.&lt;/p&gt;

&lt;p&gt;##Starting the component
To start the component we need a real camera connected to the cameraV4lComp component or the RCIS simulator started with a file that includes virtual tags, such as &lt;em&gt;simpleworld.xml&lt;/em&gt;, Tutorial can be found &lt;a href=&quot;virtualapriltagstutorial.md&quot;&gt;here&lt;/a&gt;. Once RCIS is up and running, It will provide the RGBD.idsl interface (not Camera.idsl for now) at port 10096, which is what the configuration file states. To avoid changing the &lt;em&gt;generic_config&lt;/em&gt; file in the repository, We can copy it to the component’s home directory, So changes will remain untouched by future git pulls:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cp ~/robocomp/components/robocomp-robolab/components/apriltagsComp
cp /etc/generic_config config
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;So, to begin we type:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd ~/robocomp/components/robocomp-robolab/components/apriltagsComp
bin/apriltagscomp --Ice.Config=config
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If the robot’s camera is pointing towards one of the tags, You should see in the terminal lines showing the ID and pose of each visible tag.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Tutorials Directory</title>
   <link href="http://robocomp.github.io/website/2015/05/23/README/"/>
   <updated>2015-05-23T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/05/23/README</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;components.md&quot;&gt;A Brief introduction to Components&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;component_creation_with_DSLEditor.md&quot;&gt;Creation of a new component using RoboComp’s Eclipse based DSLEditor&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;using_github.md&quot;&gt;Maintaining your own repository of components&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;contribute/contribute.md&quot;&gt;How to contribute to RoboComp using the GitHub branching mechanism&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;robocompdsl.md&quot;&gt;Using the new &lt;strong&gt;robocompdsl&lt;/strong&gt; component generation command line tool&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/robocomp/robocomp-robolab/blob/master/components/apriltagsComp/README.md&quot;&gt;The E. Olson’s AprilTags component&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;robocompdsl_python.md&quot;&gt;Creating a Python component using &lt;strong&gt;robocompdsl&lt;/strong&gt; that subscribes to &lt;em&gt;apriltagsComp&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[InnerModel, RoboComp’s internal representation of reality]&lt;/p&gt;

&lt;p&gt;[RoboComp’s robots: Ursus, Loki and the others]&lt;/p&gt;

&lt;p&gt;[&lt;strong&gt;robocomp-robolab&lt;/strong&gt; components]&lt;/p&gt;

&lt;p&gt;[The BodyInverseKinematics (BIK) component]&lt;/p&gt;

&lt;p&gt;[The Navigation (TRAJ) component]&lt;/p&gt;

&lt;p&gt;[RoboCog, a Cognitive Architecture built with RoboComp]&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Chroot environment</title>
   <link href="http://robocomp.github.io/website/2015/05/23/How_To_Make_Chroot_Environment/"/>
   <updated>2015-05-23T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/05/23/How_To_Make_Chroot_Environment</id>
   <content type="html">&lt;p&gt;A chroot is a way of isolating applications from the rest of your computer, by putting them in a jail. This is particularly useful if you are testing an application which could potentially alter important system files, or which may be insecure.
A chroot is basically a special directory on your computer which prevents applications, if run from inside that directory, from accessing files outside the directory. In many ways, a chroot is like installing another operating system inside your existing operating system. 
The following are some possible uses of chroots:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Isolating insecure and unstable applications&lt;/li&gt;
  &lt;li&gt;Running 32-bit applications on 64-bit systems&lt;/li&gt;
  &lt;li&gt;Testing new packages before installing them on the production system&lt;/li&gt;
  &lt;li&gt;Running older versions of applications on more modern versions of Ubuntu&lt;/li&gt;
  &lt;li&gt;Building new packages, allowing careful control over the dependency packages which are installed&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This manual will follow the steps specified in the &lt;a href=&quot;https://help.ubuntu.com/community/BasicChroot&quot;&gt;official page of Ubuntu&lt;/a&gt;. And the system we will install as tutorial is Ubuntu 14.04 Trusty amd64.&lt;/p&gt;

&lt;p&gt;##Brief Explanation
Imagine you have your Robocomp version well installed and working really fine in your system (i.e. Ubuntu 14.04 amd64), but you need to upgrade your ICE or OpenCV or PCL or whatever third-party library to a new version. You don’t want to risk your well functional version of Robocomp and it’s dependencies removing the current version and installing the new one (this usually affects other packages and libraries), and you don’t have time enough to make a whole fresh installation in other partition or virtual machine, so the fastest solution is to create a jail containing the same distribution of your main system (Ubuntu 14.04 amd64) with chroot and test Robocomp with the new version of the library you need without touching your fine Robocomp installation.
Realize that creating a chrooted environment in your machine makes your system believe that your root directory (“/”) is in another place than the actual root of the system (like I explain on the wiki, the process in which you launch chroot believes that the root directory is in / while actually it is in /var/chroot/trusty_x64/, not letting you touch anything outside that directory and therefore not risking your current installation).
Another practical use for chroot is to test an especific program or library in a different distribution or architecture. For example, if you are working in Ubuntu 14.04 amd64 and you want to test if a library that you are using works fine in Debian Wheezy or Ubuntu 14.10 or Ubuntu 14.04 i386.&lt;/p&gt;

&lt;p&gt;##Creating a chroot&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;First of all we need to install the tools to make a chroot in out system.&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install debootstrap schroot&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create a folder where the chroot is going to be installed. You need to make the folder using administrator permission (with &lt;em&gt;sudo&lt;/em&gt; i.e). We will put the chroot up in &lt;em&gt;/var/chroot/trusty_x64&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo mkdir /var/chroot &amp;amp;&amp;amp; sudo mkdir /var/chroot/trusty_x64&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create a configuration file for schroot. For our example, we will create a file named trusty_x64.conf in &lt;em&gt;/etc/schroot/chroot.d/&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo gedit /etc/schroot/chroot.d/trusty_x64.conf&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;And write the following inside: (Change the &lt;USERNAME&gt; to actual username, example &quot;root-users=abhi&quot;&lt;/USERNAME&gt;&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;
 [trusty_x64]
 description=Ubuntu trusty 14.04 for amd64
 directory=/var/chroot/trusty_x64
 root-users=&amp;lt;USERNAME&amp;gt;
 type=directory
 users=testuser
&lt;/code&gt;
  - The first line is the name of the chroot thatis going to be created.
  - &lt;strong&gt;description&lt;/strong&gt; is a short description of the chroot.
  - &lt;strong&gt;directory&lt;/strong&gt; the path where the chroot is going to be installed. Note that is the same path that we specified in step 2.
  - &lt;strong&gt;root-users&lt;/strong&gt; list of users that are allowed in our chroot without password.
  - &lt;strong&gt;type&lt;/strong&gt;  The type of the chroot. Valid types are ‘plain’, ‘directory’, ‘file’, ‘block-device’ and ‘lvm-snapshot’. If empty or omitted, the default type is ‘plain’.
  - &lt;strong&gt;users&lt;/strong&gt; list of users that are allowed access to the chroot.&lt;/p&gt;

    &lt;p&gt;see &lt;a href=&quot;http://manpages.ubuntu.com/manpages/hardy/man5/schroot.conf.5.html&quot;&gt;schroot.config&lt;/a&gt; for further information.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Run Debootstrap. This step will download and unpack a basic ubuntu or debian system to the chroot directory we created in step 2.&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo debootstrap --variant=buildd --arch amd64 trusty /var/chroot/trusty_x64 http://archive.ubuntu.com/ubuntu&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;In our example, we are creating a chroot of an Ubuntu 14.04 64-bit distribution, but this command allows some different commands that can satisfy our needs, for instance, if we want to install the same distribution but the 32-bit version, we have to type:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo debootstrap --variant=buildd --arch i386 trusty /var/chroot/trusty http://archive.ubuntu.com/ubuntu&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;Note that we have to do the proper changes creating a different schroot configuration file (i.e. &lt;em&gt;/etc/schroot/chroot.d/trusty&lt;/em&gt;) and a different folder for the new chroot (i.e. &lt;em&gt;/var/chroot/trusty&lt;/em&gt;)&lt;/p&gt;

    &lt;p&gt;If we want to create a chroot for a Debian version (i.e. Debian Wheezy (stable)) we have to type:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo debootstrap --variant=buildd --arch amd64 wheezy /var/chroot/wheezy_x64 http://ftp.debian.org/debian&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Checking the chroot. To be sure that everything went ok, we can type the following command, that will list all the available chroot enviroments in out system.&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;schroot -l&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;If trusty_x64 appears, we can start working in our chrooted environment typing:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;schroot -c trusty_x64 -u root&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;The prompt of the chrooted environment should be like:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;(trusty_x64)root@abhi-Inspiron-7520:/home/abhi#&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; This step is not mandatory.
 &lt;strong&gt;NOTE&lt;/strong&gt; For convenience, the default schroot configuration rebinds the /home directory on the host system so that it appears in the chroot system. This could be unexpected because it means that you can accidentally delete or otherwise damage things in /home on the host system. To change this behaviour we can run the following command in the host system:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo gedit /etc/schroot/default/fstab&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;And comment the /home line:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;
# fstab: static file system information for chroots.
# Note that the mount point will be prefixed by the chroot path
# (CHROOT_PATH)
#
# &amp;lt;file system&amp;gt; &amp;lt;mount point&amp;gt;   &amp;lt;type&amp;gt;  &amp;lt;options&amp;gt;       &amp;lt;dump&amp;gt;  &amp;lt;pass&amp;gt;
/proc           /proc           none    rw,bind        0       0
/sys            /sys            none    rw,bind        0       0
/dev            /dev            none    rw,bind         0       0
/dev/pts        /dev/pts        none    rw,bind         0       0
#/home          /home           none    rw,bind         0       0
/tmp            /tmp            none    rw,bind         0       0
&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And that’s it! Now we have a whole very basic system in which we can test out programs and libraries.&lt;/p&gt;

&lt;p&gt;##Troubleshooting&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If you get locale warnings in the chroot like &lt;strong&gt;“Locale not supported by C library.”&lt;/strong&gt; or &lt;strong&gt;“perl: warning: Setting locale failed.”&lt;/strong&gt;, then try one or more of these commands:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    sudo dpkg-reconfigure locales
    sudo apt-get install language-pack-en
    sudo locale-gen en_US.UTF-8
    sudo dpkg-reconfigure locales
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;if the problem persist check out this &lt;a href=&quot;http://perlgeek.de/en/article/set-up-a-clean-utf8-environment&quot;&gt;page&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;To get access to the intertet within the chroot, you have to type:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo cp /etc/resolv.conf /var/chroot/trusty_x64/etc/resolv.conf&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You might want to have the proper sources.list in order to be able to install packages from Ubuntu official repositories like universe or multiverse, and the security updates. If you make a chroot installation, the sources.list will be the most basic one, like:&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;deb http://archive.ubuntu.com/ubuntu trusty main&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;You can generate a more complete sources.list file in this pages &lt;a href=&quot;http://repogen.simplylinux.ch/&quot;&gt;Ubuntu&lt;/a&gt; and &lt;a href=&quot;http://debgen.simplylinux.ch/&quot;&gt;Debian&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;##External Links&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://help.ubuntu.com/community/BasicChroot&quot;&gt;Ubuntu official chroot manual&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://help.ubuntu.com/community/DebootstrapChroot&quot;&gt;Ubuntu official deboostrap manual&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://perlgeek.de/en/article/set-up-a-clean-utf8-environment&quot;&gt;PerlGeek troubleshooting&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://manpages.ubuntu.com/manpages/hardy/man5/schroot.conf.5.html&quot;&gt;Schroot conf manual&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://manpages.ubuntu.com/manpages/trusty/en/man8/debootstrap.8.html&quot;&gt;Debootstap manual&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://repogen.simplylinux.ch/&quot;&gt;Sources.list for Ubuntu&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://debgen.simplylinux.ch/&quot;&gt;Sources.list for Debian&lt;/a&gt;&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Using robocompdsl, The command line component generator</title>
   <link href="http://robocomp.github.io/website/2015/05/23/robocompdsl/"/>
   <updated>2015-05-23T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/05/23/robocompdsl</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;robocompdsl&lt;/strong&gt; is the new tool used in RoboComp to automatically generate components and modify their main properties once they have been generated (e.g., communication requirements, UI type). It is one of the core tools of the framework so, if you installed RoboComp, you can start using it right away.&lt;/p&gt;

&lt;p&gt;This new version can only be used from the command line, but the languages used to define components and their interfaces remain mostly the same: &lt;strong&gt;CDSL&lt;/strong&gt; to specify components and &lt;strong&gt;IDSL&lt;/strong&gt; to specify interfaces. The only difference with the old RoboCompDSLEditor tool is that the reserved keywords (are now case independent). Take a look to the tutorial &lt;a href=&quot;components.md&quot;&gt;“a brief introduction to Components”&lt;/a&gt; for an introduction to the concept of component generation and the languages involved.&lt;/p&gt;

&lt;p&gt;There are three tasks we can acomplish using &lt;strong&gt;robocompdsl&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;generating a CDSL template file&lt;/li&gt;
  &lt;li&gt;generating the code for a previously existing CDSL file&lt;/li&gt;
  &lt;li&gt;regenerating the code for an already generated component.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;generating-a-cdsl-template-file&quot;&gt;Generating a CDSL template file&lt;/h2&gt;
&lt;p&gt;Even tough writing CDSL files is easy –their structure is simple and the number of reserved words is very limited– robocompdsl can generate template CDSL files to be used as a guide when writing CDSL files.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ robocompdsl path/to/mycomponent/mycomponent.cdsl
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This will generate a CDSL file with the following content:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import &quot;/robocomp/interfaces/IDSLs/import1.idsl&quot;;
import &quot;/robocomp/interfaces/IDSLs/import2.idsl&quot;;
 
Component CHANGETHECOMPONENTNAME
{
	Communications
	{
		implements interfaceName;
		requires otherName;
		subscribesTo topicToSubscribeTo;
		publishes topicToPublish;
	};
	language Cpp;
	gui Qt(QWidget);
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The CDSL language is described in the tutorial &lt;a href=&quot;components.md&quot;&gt;“A brief introduction to Components”&lt;/a&gt;. Just don’t forget to change the name of the component.&lt;/p&gt;

&lt;h2 id=&quot;generating-a-component-given-a-cdsl-file&quot;&gt;Generating a component given a CDSL file&lt;/h2&gt;
&lt;p&gt;Once we have our CDSL file we can generate the component’s source code running robocompdsl with the CDSL file as first argument and the directory where the code should be placed as the second argument.&lt;/p&gt;

&lt;p&gt;From the component’s directory:
    $ cd path/to/mycomponent
    $ robocompdsl mycomponent.cdsl .&lt;/p&gt;

&lt;p&gt;Or somewhere else:
    $ robocompdsl path/to/mycomponent/mycomponent.cdsl path/to/mycomponent&lt;/p&gt;

&lt;p&gt;These commands will generate the C++ or Python code in the specified directory.&lt;/p&gt;

&lt;h2 id=&quot;updating-the-source-code-of-a-component-after-modifying-its-cdsl-file&quot;&gt;Updating the source code of a component after modifying its CDSL file&lt;/h2&gt;
&lt;p&gt;Once we generated our component we might change our mind and decide to add a new connection to another interface or to publish a new topic. In these cases we can regenerate the code of the component just by changing the &lt;em&gt;.cdsl&lt;/em&gt; file and executing again the command.&lt;/p&gt;

&lt;p&gt;As you might have learned from the tutorial &lt;a href=&quot;components.md&quot;&gt;“A brief introduction to Components”&lt;/a&gt; RoboComp components are divided in specific code (files where you write your code) and generic code (autogenerated code which doesn’t need to be edited). Running robocompdsl again on the same directory will ony overwrite these generic files. To ensure robocompdsl doesn’t overwrite the changes you made to the specific files these are left unchanged, so the component might not compile after regeneration (e.g., you might need to add new methods).&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Packaging RoboComp</title>
   <link href="http://robocomp.github.io/website/2015/05/23/nithin3/"/>
   <updated>2015-05-23T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/05/23/nithin3</id>
   <content type="html">&lt;p&gt;###deb packages&lt;/p&gt;

&lt;p&gt;For creating a robocomp debian package :&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd ~/robocomp
mkdir build
cmake ..
make package
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;will create a .deb package in the build directory, which we can install using any packaging application like dpkg. To install the created package, just double click on it(open with Software Center) or in terminal type&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo dpkg -i &amp;lt;packagename&amp;gt;.deb
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;###source packages for ppa&lt;/p&gt;

&lt;p&gt;Launchpad will only accept source packages and not binary.Launchpad will then build the packages. For building source packages we are using debuild which is a wrapper around the &lt;em&gt;dpkg-buildpackage + lintian&lt;/em&gt;. so you will need to install debuild and dput on your system.The source_package.cmake script is used to create debian source package.&lt;/p&gt;

&lt;p&gt;The main CMakeLists.txt file defines a target &lt;code class=&quot;highlighter-rouge&quot;&gt;spackage&lt;/code&gt; that builds the source package in build/Debian with &lt;code class=&quot;highlighter-rouge&quot;&gt;make spackage&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For uploading the package to ppa, First change the &lt;strong&gt;PPA_PGP_KEY&lt;/strong&gt; in &lt;a href=&quot;../cmake/package_details.cmake#L26&quot;&gt;package_details.cmake&lt;/a&gt; to the contact of the PGP key  details registered with your ppa account.Then create a source package by building the target &lt;em&gt;spackage&lt;/em&gt;.Once the Source package is build successfully, upload it to your ppa by:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd Debian/
dput ppa:&amp;lt;lp-username&amp;gt;/&amp;lt;ppa-name&amp;gt; package-source.changes
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;building of source package can be tested with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd Debian/robocomp-&amp;lt;version&amp;gt;
debuild -i -us -uc
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;####Note:&lt;/p&gt;

&lt;p&gt;If you want to upload another source package to ppa which doesn’t have any changes in the source but maybe in the debian files. you can build the spackage after commenting out &lt;code class=&quot;highlighter-rouge&quot;&gt;set(DEB_SOURCE_CHANGES &quot;CHANGED&quot; CACHE STRING &quot;source changed since last upload&quot;)&lt;/code&gt; in &lt;a href=&quot;../cmake/package_details.cmake#L27&quot;&gt;package_details.cmake&lt;/a&gt; so that the the script will only increase the ppa version number and wont include the source package for uploading to ppa (which otherwise will give an error).&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title><i>GSoC,</i> Building and deployment system design</title>
   <link href="http://robocomp.github.io/website/2015/05/23/nithin2/"/>
   <updated>2015-05-23T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/05/23/nithin2</id>
   <content type="html">&lt;p&gt;###About Me:&lt;/p&gt;

&lt;p&gt;Hi all , I am Nithin Murali and i would like to introduce me a little in this post. I am pursuing my engineering degree on Electrical Engineering from Indian Institute of Technology Bombay. I am working on an Autonomous Underwater Vehicle which we are developing for the RObosub competition. We are developing in ROS framework. That was my first introduction to robotic frameworks. I have read about Robocomop before but a real chance to contribute to this ambitious framework was brought to me by GSOC 2015. I have Chosen the project &lt;em&gt;RoboComp Building and deployment system design&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;###About the project&lt;/p&gt;

&lt;p&gt;Currently the the build system in Robocomp is not very efficient. It is limited only to the core libraries and some additional tools. So we will have to seprately build all the components one by one. Also as the number of component increases it will be more difficult to manage all of them. So I am planning to come up with an workspace model for Robocomp. It would accompany with various tools which will ease handling of components.&lt;/p&gt;

&lt;p&gt;As of now the users have to build robocomp from source for using it. But there may be users who dont want to work on the framework but is only interested in developing new components. For such users it is important that that we should supply an compiled package (preferably debian package). Also package would be more accessible if we could provide an ppa for robocomp. So i am planning to package robocomp and also create a ppa for robocomp.&lt;/p&gt;

&lt;p&gt;Currently Robocomp dosent have any tests written nor is it using any testinf framework. So one of my task would be decide on a testing framework/stragery and write tests for existing framework.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Debian Packaging 1</title>
   <link href="http://robocomp.github.io/website/2015/05/23/nithin1/"/>
   <updated>2015-05-23T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/05/23/nithin1</id>
   <content type="html">&lt;p&gt;##What is a package
By definition &lt;em&gt;Debian packages are standard Unix ar archives that include two tar archives optionally compressed with gzip (zlib), Bzip2, lzma, or xz (lzma2): one archive holds the control information and another contains the program data.&lt;/em&gt;
All debain packages should follow certain conventions. The root source directory should contain a directory named &lt;em&gt;debian&lt;/em&gt;. This directory contains files which stores info about the package.
These are the required files under the debian directory&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;rules&lt;/strong&gt;&lt;br /&gt;
This is the maintainer script for the package building. This script is run by the packaging application to build and install the source into a &lt;em&gt;tmp&lt;/em&gt; directory in the debian folder. It has the following Targets&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;clean target&lt;/em&gt; : to clean all compiled, generated, and useless files in the build-tree.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;build target&lt;/em&gt; : to build the source into compiled programs and formatted documents in the build-tree.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;build-arch target&lt;/em&gt; : to build the source into arch-dependent compiled programs in the build-tree.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;build-indep target&lt;/em&gt; : to build the source into arch-independent formatted documents in the build-tree.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;binary target&lt;/em&gt; : to create all binary packages (effectively a combination of binary-arch and binary-indep targets)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;binary-arch target&lt;/em&gt; : to create arch-dependent (Architecture: any) binary packages in the parent directory.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;em&gt;binary-indep target&lt;/em&gt;: to create arch-independent (Architecture: all) binary packages in the parent directory.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;changelog&lt;/strong&gt;&lt;br /&gt;
This file contains the project changelog along with the project name , version and distribution and urgency of your package.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;compact&lt;/strong&gt;&lt;br /&gt;
The compact file defines the debhelper compatibility level.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;debian/control&lt;/strong&gt;&lt;br /&gt;
This file contains various values which dpkg, dselect, apt-get, apt-cache, aptitude, and other package management tools will use to manage the package. The control file describes the source and binary package, and gives some information about them, such as their names, who the package maintainer is, build and run dependencies and so on.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;copyright&lt;/strong&gt;&lt;br /&gt;
This file contains information about the copyright and license of the upstream sources&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;(pre/post)(inst/rm)&lt;/strong&gt;&lt;br /&gt;
This are the scipts which are run before or after installation or removal of package.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now once you have the source directory in the prescribed format. you will need a &lt;em&gt;.tar.gz&lt;/em&gt; archive of the source in the same folder.Then we can create a debian binary package using&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;debuild -i -us -uc -b
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Or a debian source package using&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;debuild -i -us -uc -S
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;p&gt;Nithin Murali&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Software components in RoboComp, A brief introduction</title>
   <link href="http://robocomp.github.io/website/2015/05/23/components/"/>
   <updated>2015-05-23T00:00:00+05:30</updated>
   <id>http://robocomp.github.io/2015/05/23/components</id>
   <content type="html">&lt;p&gt;Two major problems encountered when creating large, complex software are scalability and reusability. These problems become especially acute when it comes to writing the software that controls today robots. Robotics is the mixed bag of technology, where almost everything finds its way through. Also, Robotics is the place where our dreams of intelligent machines meet, in an endless attempt to build a truly useful tool for our daily lives. Because of this, we organize the software for our robots in big architectures that try to reproduce whatever we understand by intelligent behavior. The most audacious architectures are called &lt;em&gt;cognitive architectures&lt;/em&gt; and try to integrate all levels of behavior and reasoning needed to achieve intelligence. Some of them have been with us for more than 30 years, &lt;a href=&quot;http://soar.eecs.umich.edu/&quot;&gt;SOAR.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The problem with building these little monsters is that you need a very powerful underlying infrastructure that lets you build and modify software created by many people and that has to execute on real, moving machines. Also, everybody expect robots to be smarter than they really are and that is a lot of preassure. Components provide a new, developing technology that can be very helpful here. Components are &lt;em&gt;programs that communicate&lt;/em&gt; and as such, they are built with everything at hand, libraries, objects, threads, sockets, lambda functions and any other thing you can come up with to code a program. Also, components need a way to communicate among them and here is where communication middlewares get in. If you want to communicate programs written in different languages, running across the internet, executing on different hardware architectures -even browsers- then you cannot get along with a socket. You really need a middleware. Putting together these to elements, programs and a communication middleware, you almost come up with a component. One more thing is needed, a model for your components. You need to define what is a generic component and consequently how its internal structure is going to be, its directory and building ecosystem, how it has to be documented, its default behavior, how it will be deployed and its modes of communicating. There are several proposals that do exactly this, being the most famous &lt;a href=&quot;http://www.corba.org&quot;&gt;CORBA.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In RoboComp (2005-) we have created our own component model, inspired by the &lt;a href=&quot;http://orca-robotics.sourceforge.net/&quot;&gt;ORCA&lt;/a&gt; model and making it evolve to fit our needs along these years. As a middleware, RoboComp primarily uses &lt;a href=&quot;www.zeroc.com&quot;&gt;Ice&lt;/a&gt; and there is ongoing experimental work to make RoboComp middleware agnostic, so its components can be re-generated to use other middlewares such as &lt;a href=&quot;http://portals.omg.org/dds/&quot;&gt;DDS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;RoboComp’s components model is quite simple and we always try to simplify it even more. It can be best explained through two Domain Specific Languages (DSLs) that have been created to define a component at a very high level of abstraction. &lt;strong&gt;IDSL&lt;/strong&gt; stands for “Interface Definition Specific Language” and currently is a subset of Ice’s Slice interface language. With IDSL you write the data structures and functions that a component can implement, require, subscribe to or publish. A component can implement several interfaces, offering different views of its internal functioning. Also, the same interface can be implemented by many components. This is an example of a simple interface written in IDSL:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;module RoboCompSpeech
{
  interface Speech
  {
     bool say(string text,bool overwrite);
     bool isBusy();
  };
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;CDSL&lt;/strong&gt; stands for “Component Definition Specific Language” and allows the user to specifiy its name, accesible interfaces, communication connections, target language and other available modules or libraries that you want to include in the building scripts.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import &quot;/robocomp/interfaces/IDSLs/DifferentialRobot.idsl&quot;;
import &quot;/robocomp/interfaces/IDSLs/Laser.idsl&quot;;
Component prueba
{
    Communications
    {
      requires DifferentialRobot, Laser;
    };
    language cpp;
    gui Qt(QWidget);
};
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Using these two DSLs, RoboComp can generate the source code of the component using a tool designed to this end. The complete, functioning code of a component is created ready to be compiled and executed. We use a smart inheritance mechanism to separate the generic stuff from the user specific stuff and, based on it, the next time you generate a component, your code will remain untouched but access to new defined proxies will be there.&lt;/p&gt;

</content>
 </entry>
 

</feed>
