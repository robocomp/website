<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      RoboComp &middot; 
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/website/public/css/poole.css">
  <link rel="stylesheet" href="/website/public/css/syntax.css">
  <link rel="stylesheet" href="/website/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/websitepublic/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/websitepublic/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/website">
          RoboComp
        </a>
      </h1>
      <p class="lead">A simple robotics framework.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/website">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/Blog/">Blog</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/GSoC15/">GSoC'15</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/GSoC16/">GSoC'16</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/projects/">Projects</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/about/">About</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/website/contact/">Contact</a>
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/install/">Install</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
      <a class="sidebar-nav-item" href="http://robocomp.readthedocs.org">Tutorials</a>
      <a class="sidebar-nav-item" href="https://github.com/robocomp">GitHub project</a>
      
      <span class="sidebar-nav-item">Currently v1.0.0</span>
    </nav>

    <p>&copy; 2016. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <i><b>RoboComp</b> is an open-source Robotics framework providing the tools to create and modify software components that communicate through public interfaces. Components may require, subscribe, implement or publish interfaces in a seamless way. Building new components is done using two domain specific languages, IDSL and CDSL. With IDSL you define an interface and with CDSL you specify how the component will communicate with the world. With this information, a code generator creates C++ and/or Python sources, based on CMake, that compile and execute flawlessly. When some of these features have to be changed, the component can be easily regenerated and all the user specific code is preserved thanks to a simple inheritance mechanism.</i>


<hr>

<div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/08/19/kripasindhu_sarkar_blog_3/">
        GSoC, Computer vision components and libraries management - Open Detction <p>#3</p>
      </a>
    </h1>

    <span class="post-date">19 Aug 2015</span>

    <p><strong>Contributions after Mid-term</strong>
Following are the contributions towards the project <em>after</em> the mid-term evaluations:</p>

<ul>
  <li>
    <p>HOG based detection (with demo).</p>
  </li>
  <li>
    <p>Face-recognition: training as well as recognition (with demo).</p>
  </li>
  <li>
    <p>Cascade based detection (with demo).</p>
  </li>
  <li>
    <p>Functionality for confidence in a detection (to handle uncertainty).</p>
  </li>
</ul>

<p>That concluded the functionality of whatever planned for this summer of code. Other than the planned work, I included some other new functionality which came in between and I thought was required. The other additions are:</p>

<ul>
  <li>
    <p>Addition of two types of detection functions for every detection class.</p>

    <p>a. <em>detectOmni</em> : This is the function whose purpose is to detect an object in an entire scene. Thus, other than the type of detection we also have information about the <em>location</em> of the detection w.r.t. the scene. This was the only function previously implemented.</p>

    <p>b. <em>detect</em> : The purpose of this function is to perform detection on a segmented scene or an ‘object candidate’. i.e. the entire scene is considered as an ‘object’ or an detection. This function was an extra addition for most of the classes. For global descriptor based classes like ODHOGDetector this function came very naturally.</p>
  </li>
  <li>
    <p><strong>Standardized the training database</strong>:. Each detection class identifies its own database (which is a unique folder as of now). Training data of all classes are now put together under a single ‘trained_data’ directory and the rest is resolved by the class automatically.</p>
  </li>
  <li>
    <p>The class <strong>ODDetectorMultiAlgo</strong>: This is one of the most interesting class and interesting contribution in this project. The idea is to run multiple detection algorithm on a same segmented or unsegmented scene (i.e. run both <em>detect</em> and <em>detectOmni</em> function) and provide the result. That is, using this class one can do object detection/recognition using multiple algorithms and provide outcome of detections (eg. people detected by HOG, face detected by Cascade, bottle detected PnPRansac in a same image). Because of the nice polymorphic design of the detection classes, one can use the concept easily with any number of detection classes with their own parameters, but with <em>ODDetectorMultiAlgo</em> one can only take benefit of the default parameters of the included classes.</p>
  </li>
  <li>
    <p>HOG training: There are no standard training module for training HOG (in OpenCV or in other standard library) and people are mostly confused in training svm with the HOG detector available from OpenCV. This formed a motivation for completing the HOG pipeline and provide a training module. The crucial contribution includes <em>hard negative</em> training mode where the training is repeated with hard examples or false positive windows as negative windows. I hope that this will be really a helpful contribution for the computer vision community.</p>
  </li>
</ul>

<h2 id="design-changes-learning-resources-and-overall-learning-experiences">Design changes, learning resources, and overall learning experiences:</h2>
<p>In the next blog I’ll add the different sources I used to design and implement the above tasks and the things I learnt in this process.</p>

<hr />


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/08/16/mercedes5/">
        <i>GSoC,</i> Symbolic planning techniques for recognizing objects domestic <p>#5</p> System Review
      </a>
    </h1>

    <span class="post-date">16 Aug 2015</span>

    <p><strong>Full object manipulation system</strong> : In this post we will describe the full system developed for manipulating objects using a robotic arm. We start at the highest level, VisualIK (the correction system), and we will go down to the base of the system, the IK (which calculates the final angle of the joints of the robotic arm).</p>

<p><img src="https://github.com/mercedes92/VisualIKExperiment/blob/master/images/Dibujo%20sin%20t%C3%ADtulo.png?raw=true" alt="Alt text" /></p>

<p>All the components that will be described below implement the <code class="highlighter-rouge">InverseKinematics</code> interface. This interface is defined by a series of data structures:</p>

<ol>
  <li><code class="highlighter-rouge">Pose6D</code>: It represents a pose with three translations and three rotations [x, y, z, rx, ry, rz]</li>
  <li><code class="highlighter-rouge">WeightVector</code>: It represents the weight vector of each element of Pose6D. This will help us later in the Levenberg-Marquardt algorithm.</li>
  <li><code class="highlighter-rouge">TargetState</code>: It is the state when the targets reach the end of their execution by ik. It has some information like the elapsed time, the final error in translations and rotations and the values of each joint.</li>
  <li><code class="highlighter-rouge">Axis</code>: This structure represents the Cartesian axes. It is necessary for certain special types of targets.</li>
  <li><code class="highlighter-rouge">Motor</code>: This structure stores the name and the angular value of a motor of the kinematic chain.</li>
</ol>

<p>Also, the interface defines a number of methods:</p>

<ol>
  <li><code class="highlighter-rouge">getTargetState</code>: this method returns the state of one target, given the name of the robot part that executed the target and the numerical identifier of the target.</li>
  <li><code class="highlighter-rouge">getPartState</code>: this method returns the state (if the part has pending targets or not) of one of the robot part (RIGHTARM, LEFTARM or HEAD).</li>
  <li><code class="highlighter-rouge">setTargetPose6D</code>: This method is used to send targets to the ik. We need to indicate the part of the robot that will execute the target, the pose of the target and the weight vector of the pose. It returns the identifier of the target.</li>
  <li><code class="highlighter-rouge">setTargetAlignaxis</code>: This method sends a special target for the IK. This target is achieved by aligning the end effector to the target axes. We must indicate the the part of the robot that will execute the target, the pose and the axes. It returns the identifier of the target.</li>
  <li><code class="highlighter-rouge">setTargetAdvanceAxis</code>: This method sends another special target for the IK. The goal is that the end effector advance along a vector in the Cartesian system. We must indicate the part of the robot that will execute the target, the axes of the vector and the dist to advance. Like always, it return the identifier of the target.</li>
  <li><code class="highlighter-rouge">goHome</code>: this method send a robot part to the idle position.</li>
  <li><code class="highlighter-rouge">stop</code>: this method abort the execution of the targets of one of the robot part.</li>
  <li><code class="highlighter-rouge">setJoint</code>: this method changes the angular value of one of the joint of the kinematic chain.</li>
  <li><code class="highlighter-rouge">setFingers</code>: this method opens and closes the fingers of the end effector.</li>
</ol>

<p>Now, we will describe the basic operation of our components.</p>

<p>###The <code class="highlighter-rouge">visualIK</code> component</p>

<p>The old <a href="http://robocomp.github.io/website/2015/06/17/mercedes3.html"><code class="highlighter-rouge">VisualBIK</code> component</a> has been reorganized, resulting in the current <a href="https://github.com/robocomp/robocomp-ursus/tree/master/components/visualik"><code class="highlighter-rouge">visualik</code> component</a>. The basic principle of operation of the component remains, adding some improvements and changing its communication with the <code class="highlighter-rouge">inversekinematics</code> component, by the <code class="highlighter-rouge">ikGraphGenerator</code> component.</p>

<p>Its goal remains the same, correct errors produced by the inaccuracies of the joints in the IK. To this end, it bases its operation on a state machine:</p>

<ol>
  <li><code class="highlighter-rouge">IDLE</code>: It is the resting state of <code class="highlighter-rouge">visualik</code>. The component is waiting to receive a target through one of the methods of its interface. When a target comes through the interface (stored in the <code class="highlighter-rouge">currentTarget</code> variable), the state machine switches to INIT_BIK and prepares the global variables for the execution of correction.</li>
  <li><code class="highlighter-rouge">INIT_BIK</code>: in this state, the visualik applies an initial correction to the target (Ec). This correction is based on experience in the correction of previous targets, so the first correction, having no previous experience, will be zero. Then, it sends the target to his proxy of kinematic, the <code class="highlighter-rouge">ikGraphGenerator</code> component. Finally, the state of the machine is changed to <code class="highlighter-rouge">WAIT_BIK</code>.</li>
  <li><code class="highlighter-rouge">WAIT_BIK</code>: in this state, the <code class="highlighter-rouge">visualik</code> waits the end of execution of the target by the <code class="highlighter-rouge">ikGraphGenerator</code>. When the <code class="highlighter-rouge">ikGraphGenerator</code> finishes executing the target, the <code class="highlighter-rouge">visualik</code> changes his state to <code class="highlighter-rouge">CORRECT_ROTATION</code>.</li>
  <li><code class="highlighter-rouge">CORRECT_ROTATION</code>: It is the latest machine status. In this state is when the <code class="highlighter-rouge">visualik</code> does all the calculations in order to correct the deviations and errors of the joints. The procedure is simple: a) by apriltags, the <code class="highlighter-rouge">visualik</code> calculates the visual pose of the end effector; b) then, it computes the error vector between the visual pose and the target pose (Ev); c) with this error vector Ev, the <code class="highlighter-rouge">visualik</code> corrects the target pose and sends the new position to the <code class="highlighter-rouge">ikGraphGenerator</code> component; d) this process is repeated until the error achieved in translation and rotation is less than a predetermined threshold; e) finally the <code class="highlighter-rouge">visualik</code> calculates the error vector between the original target and the last corrected target (Ec), with this error the component realizes the first correction in <code class="highlighter-rouge">INIT_BIK</code>.</li>
</ol>

<p>Here there is a scheme of the procedure performed by the <code class="highlighter-rouge">visualik</code>. It is very summarized to facilitate the understanding of the procedure by the reader:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>1: procedure VISUAL CALIBRATION
2:    Xt = TargetArrives()
3:    Ct = Xt + Ec
4:    sendTargetToGIK(Ct)
5:    Xv = getAprilTagPose(robot)
6:    while (Ev = Xv- Xt) &gt; threshold ^¬ timeOut() do
7:         Xi = getInternalPose(robot)
8:         Xc = Xi + Ev
9:         sendTargetToGIK(Xc)
10:        Xv = getAprilTagPose(robot)
11:   end while
12:   Ec = Xc - Xt
13: end procedure
</code></pre>
</div>

<p>Like all components developed by robocomp, the <code class="highlighter-rouge">visualik</code> needs a configuration file in which the components required (the <a href="https://github.com/robocomp/robocomp-ursus/tree/master/components/ikGraphGenerator"><code class="highlighter-rouge">ikGraphGenerator</code></a>) and the components to subscribe (the <a href="https://github.com/robocomp/robocomp-robolab/tree/master/components/apriltagsComp"><code class="highlighter-rouge">apriltagsComp</code></a>) and other configuration parameters are determined. In this case, a configuration file may have the following elements:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>CommonBehavior.Endpoints=tcp -p 14537
# Endpoints for implemented interfaces:
InverseKinematics.Endpoints=tcp -p 10242

# Endpoints for interfaces to subscribe:
AprilTagsTopic.Endpoints=tcp -p 12938

# Proxies for required interfaces
InverseKinematicsProxy =inversekinematics:tcp -h localhost -p 10241 # `ikGraphGenerator`

InnerModel=/home/robocomp/robocomp/components/robocomp-ursus-rockin/etc/ficheros_Test_VisualBIK/ursus_bik.xml # the internal model of the robot environment 

# This property is used by the clients to connect to IceStorm.
TopicManager.Proxy=IceStorm/TopicManager:default -p 9999
Ice.Warn.Connections=0
Ice.Trace.Network=0
Ice.Trace.Protocol=0
Ice.ACM.Client=10
Ice.ACM.Server=10
</code></pre>
</div>

<p>###The <code class="highlighter-rouge">ikGraphGenerator</code> component</p>

<p>As we explain in the previous <a href="http://robocomp.github.io/website/2015/08/13/mercedes4.html">post</a>, the <a href="https://github.com/robocomp/robocomp-ursus/tree/master/components/ikGraphGenerator"><code class="highlighter-rouge">ikGraphGenerator</code>component</a> creates and stores a graph representing the work 3D space of the arm, where each node stores the euclidean space pose of the end effector and the configuration of the joints that compose the arm. So the <code class="highlighter-rouge">ikGraphGenerator</code> waits until the <code class="highlighter-rouge">visualik</code> send a target to him through the <code class="highlighter-rouge">InverseKinematics</code> interface. In this moment, the <code class="highlighter-rouge">ikGraphGenerator</code> performs four steps:</p>

<ol>
  <li>the component searches the node A whose pose is closest to the initial end effector pose and moves the arm there.</li>
  <li>the component finds in the graph  the node B whose pose is closest to the target position, disabling those nodes which would make the robot’s arm collide with external objects.</li>
  <li>the component computes the shortest path between the node A and the node B and moves the end effector among the nodes to reach the node B.</li>
  <li>Finally, the component moves from the graph to the actual target sending the original target to the <code class="highlighter-rouge">inversekinematic</code>component and taking the final values of the joints.</li>
</ol>

<p>The <code class="highlighter-rouge">ikGraphGenerator</code>needs a config file too:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>CommonBehavior.Endpoints=tcp -p 14536
# Endpoints for implemented interfaces:
InverseKinematics.Endpoints=tcp -p 10241

InnerModel=/home/robocomp/robocomp/components/robocomp-ursus/etc/ursus.xml # the internal model of the robot environment 

# Proxies for required interfaces
InverseKinematicsProxy = inversekinematics:tcp -h localhost -p 10240
JointMotorProxy = jointmotor:tcp -h localhost -p 20000

Ice.Warn.Connections=0
Ice.Trace.Network=0 
Ice.Trace.Protocol=0
Ice.ACM.Client=10
Ice.ACM.Server=10
</code></pre>
</div>

<p>###The <code class="highlighter-rouge">inversekinematic</code> component</p>

<p>Finally, we will explain the basic component of the system, the <a href="https://github.com/robocomp/robocomp-ursus/tree/master/components/inversekinematics"><code class="highlighter-rouge">inversekinematic</code> component</a>. As we said in the <a href="http://robocomp.github.io/website/2015/06/15/mercedes2.html">second post</a>, this component receives three types of targets through the <code class="highlighter-rouge">InverseKinematics</code> interface:</p>

<ol>
  <li><code class="highlighter-rouge">POSE6D</code>: the typical target with translations and rotations [tx, ty, tz, rx,ry, rz]. The end effector has to be positioned at coordinates (tx, ty, tz) of the target and align their rotation axes with the target, specified in (rx, ry, rz). This target arrives from the method <code class="highlighter-rouge">setTargetPose6D</code>.</li>
  <li><code class="highlighter-rouge">ADVANCEAXIS</code>: its goal is to move the end effector of the robot along a vector. This target arrives from the method <code class="highlighter-rouge">setTargetAdvanceAxis</code>.</li>
  <li><code class="highlighter-rouge">ALIGNAXIS</code>: Its goal is that the end effector has the axes aligned with the target. This target arrives from the method <code class="highlighter-rouge">setTargetAlignaxis</code>.</li>
</ol>

<p>Targets received are stored into the queues of the corresponding robot part and they are executed by order of arrival. When the ik ends the execution of one target, it stores the target into the solved targets queue:</p>

<p><img src="https://github.com/mercedes92/VisualIKExperiment/blob/master/images/iksystem.png?raw=true" alt="Alt text" /></p>

<p>The config file of the <code class="highlighter-rouge">inversekinematic</code>component is the next:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>CommonBehavior.Endpoints=tcp -p 12207
# Endpoints for implemented interfaces:
InverseKinematics.Endpoints=tcp -p 10240

# Kinematic chain lists: they stores the joints names.
RIGHTARM=rightShoulder1;rightShoulder2;rightShoulder3;rightElbow;rightForeArm;rightWrist1;rightWrist2
RIGHTTIP=grabPositionHandR # end effector of the RIGHTARM

LEFTARM=leftShoulder1;leftShoulder2;leftShoulder3;leftElbow;leftForeArm;leftWrist1;leftWrist2
LEFTTIP=grabPositionHandL

HEAD=head_yaw_joint;head_pitch_joint
HEADTIP=rgbd_transform

InnerModel=/home/robocomp/robocomp/components/robocomp-ursus/etc/ursus.xml #Internal model of the robot environment

 # Proxies for required interfaces
JointMotorProxy = jointmotor:tcp -h localhost -p 20000

TopicManager.Proxy=IceStorm/TopicManager:default -p 9999
Ice.Warn.Connections=0
Ice.Trace.Network=0
Ice.Trace.Protocol=0
Ice.ACM.Client=10
Ice.ACM.Server=10
</code></pre>
</div>

<p>When <code class="highlighter-rouge">visualik</code> and <code class="highlighter-rouge">ikGraphGenerator</code> components submit their targets to the immediately below component, they store the identifier of the target and are waiting until the target be resolved, calling the method <code class="highlighter-rouge">getTargetState</code>. So, when the <code class="highlighter-rouge">inversekinematic</code> component ends one target execution, the <code class="highlighter-rouge">ikGraphGenerator</code> moves the arm with the values given by the <code class="highlighter-rouge">inversekinematic</code> component and then, the <code class="highlighter-rouge">visualik</code> continues with the corrections.</p>

<p>Having explained the handling system, the next post will explain the planning system developed by ROBOLAB to plan the robot’s actions.</p>

<p>Bye!</p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/08/13/mercedes4/">
        <i>GSoC,</i> Symbolic planning techniques for recognizing objects domestic <p>#4</p>, Inverse kinematics Graph Generator
      </a>
    </h1>

    <span class="post-date">13 Aug 2015</span>

    <p><strong>ikGraphGenerator, an alternative to ik</strong> : As the project has progressed, many improvements have emerged. One of them is the new component, <code class="highlighter-rouge">ikGraphGenerator</code>. This component has been developed by Professor Luis Manso, and its goal is to remove weight to the inverse kinematics in the process of handling objects with a robotic effector.</p>

<p>One of the problems of the inverse kinematics is that, given a target for a particular end-effector, it can calculate different solutions or values for each motor of the kinematic chain. For example, to pick up a cup, the IK can position the end effector at the target extending his arm more or less forcing or not the elbow or or separating more or less the shoulder. It doesn’t matter that the final position of the chain be more forced or more natural, the goal is reached and the solution is accepted.</p>

<p>This is not convenient, since there is no way to control the trajectory of the arm. The control on the trajectory arm is crucial at certain times, for example when the robot avoids collisions between his arm and another objects (tables, chairs, walls, including his own body). The current <code class="highlighter-rouge">inversekinematics</code> component does not allow us this control, so that we need the <code class="highlighter-rouge">ikGraphGenerator</code> component.</p>

<p>The main concept is to create a spatial graph where each node stores the pose of the point, [tx, ty, tz, rx, ry, rz], and the set of angular values for each motor of the kinematic chain (that has 7 DOFs). The links connecting the nodes whose positions are close to each other. In this way we can calculate paths between two different and separate points.</p>

<p><img src="https://github.com/robocomp/robocomp-ursus/blob/master/components/ikGraphGenerator/etc/ikg.jpg?raw=true" alt="Alt text" /></p>

<p>###What does the <code class="highlighter-rouge">ikGraphGenerator</code> component?</p>

<p>Basically the <code class="highlighter-rouge">ikGraphGenerator</code> realizes two functions:</p>

<ol>
  <li>It calculates the graph with random poses and their respective angular values. To do this, first it defines a spatial cube that represents the workspace of the robot arm. In this space a set of poses are selected and are sent to the <code class="highlighter-rouge">inversekinematic</code> component as targets. The poses that are not achievable by the ik are automatically deleted. The resultant graph is stored in a file in order to use it in later calculations.</li>
  <li>Once the graph has been calculated and stored, we can send a target to the <code class="highlighter-rouge">ikGraphGenerator</code>. First the component searches the node <code class="highlighter-rouge">A</code> whose pose is closest to the initial end effector pose and the node <code class="highlighter-rouge">B</code> whose pose is closest to the target position (to do this, the <code class="highlighter-rouge">ikGraphGenerator</code> uses a fast low-dimension k-d tree). Then, the component calculates a path between node A and node B through the graph  using Dijkstra’s algorithm, so that the arm moves through the graph from the position marked by node A to the position marked by the node B. Finally, in order to achieve the final target, the <code class="highlighter-rouge">inversekinematic</code> component is called to compute the final values of the joints, starting from the position marked by the node B.</li>
</ol>

<p><img src="https://github.com/robocomp/robocomp-ursus/blob/master/components/ikGraphGenerator/etc/GIK.png?raw=true" alt="Alt text" /></p>

<p>In the next post I will describe how the whole system works with all the components, the <code class="highlighter-rouge">inversekinematic</code>, the <code class="highlighter-rouge">ikGraphGenerator</code> and the <code class="highlighter-rouge">visualik</code> component.</p>

<p>Bye!</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/08/08/nithin9/">
        Till now ... after midterm
      </a>
    </h1>

    <span class="post-date">08 Aug 2015</span>

    <p>Hi all , In this post i will talk about what i have been working on after midterm evaluation. I have spend my time working mostly on packaging supporting libraries for Robocomp.This includes FCL and libccd. FCL is a library for performing three types of proximity queries on a pair of geometric models composed of triangles. libccd is a library for collision detection between two convex shapes.Technically Robocomp is only using FCL but libccd is an dependency of fcl, as i couldn’t find an updated ppa for it i decided to package it too. you can see those packages <a href="https://launchpad.net/~imnmfotmal">here</a></p>

<p>Also i added ability for generating robocomp source packages for different distributions. Initially i added the option only for trusty, now we could generate packages for any distribution.I worked a bit on build tools also. Currently if someone created an workspace and made a github repo of it, some one else cant use it as we store the repo names in ~/.config directory, so i added an option to reinit an repo.</p>

<p>well i guess thats all for now</p>

<hr />
<p>Nithin Murali</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/07/25/nithin10/">
        Setting up ppa
      </a>
    </h1>

    <span class="post-date">25 Jul 2015</span>

    <p>##Setting up an ppa in launchpad</p>

<p>After creating an launchpad account First you need to create and publish an OPENPGP key</p>

<p>###Generating your key in Ubuntu
The easiest way to generate a new OpenPGP key in Ubuntu is to use the Passwords and Encryption Keys tool.</p>

<p><strong>Step 1</strong> open Passwords and Encryption Keys.</p>

<p><strong>Step 2</strong> Select File &gt; New, select PGP Key and then follow the on-screen instructions.</p>

<p>Now you’ll see your new key listed in the Passwords and Encryption Keys tool. (it may take some time)</p>

<p>###Publishing your key</p>

<p>Your key is useful only if other people can verify items that you sign. By publishing your key to a keyserver, which acts as a directory of people’s public keys, you can make your public key available to anyone else.Before you add your key to Launchpad, you need to push it to the Ubuntu keyserver.</p>

<p><strong>Step 1</strong> Open Passwords and Encryption Keys.</p>

<p><strong>Step 2</strong> Select the My Personal Keys tab, select your key.</p>

<p><strong>Step 3</strong>  Select Remote &gt; Sync and Publish Keys from the menu. Choose the Sync button. (You may need to add htp://keyserver.ubuntu.com to your key servers if you are not using Ubuntu.)</p>

<p>It can take up to thirty minutes before your key is available to Launchpad. After that time, you’re ready to import your new key into Launchpad!</p>

<p>OR you can direclty to go <code class="highlighter-rouge">http://keyserver.ubuntu.com/</code> on your browser and add the PGP key there</p>

<p>###Register your key in launchpad
fire up an terminal and run <code class="highlighter-rouge">gpg --fingerprint</code> should give you fingerprints of all the keys. copy paste the required fingerprint into launchpad</p>

<p>###Sign Ubunutu Code of Conduct
Download the ubuntu code of conduct form launchpad
<code class="highlighter-rouge">gpg --clearsign UbuntuCodeofConductFile</code>  will sign the file
now copy the contents of the signed file and paste in launchpad</p>

<p>###Wrapping Up
Now everything is set up. make sure you have some key in <code class="highlighter-rouge">OPENPGP Keys</code> section and also the signed code of code of conduct as <code class="highlighter-rouge">Yes</code> as shown.
<img src="./launchpad.png" alt="" /></p>

<p>##Uploading package to ppa</p>

<p>launchpad will only accept source packages and not binary.Launchpad will then build the packages. For building source packages we are using debuild which is a wrapper around the <em>dpkg-buildpackage + lintian</em>. so you will need to install debuild and dput on your system;</p>

<p>The source_package.cmake script is used to create debian source package.</p>

<p>The main CMakeLists.txt file defines a target <code class="highlighter-rouge">spackage</code> that builds the source package in build/Debian with <code class="highlighter-rouge">make spackage</code></p>

<p>For uploading the package to ppa, First change the <strong>PPA_PGP_KEY</strong> in <a href="../cmake/package_details.cmake#L26">package_details.cmake</a> to details to the full-name of the PGP key  details registered with your ppa account For more details on setting up the pgp key see the <a href="./setting_up_ppa.md">tutorial</a>.Then create a source package by building the target <em>spackage</em>.Once the Source package is build successfully, upload it to your ppa by:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cd Debian/
dput ppa:&lt;lp-username&gt;/&lt;ppa-name&gt; &lt;packet-&gt;source.changes
</code></pre>
</div>

<p>building of source package can be tested with:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cd Debian/robocomp-&lt;version&gt;
debuild -i -us -uc -S
</code></pre>
</div>

<p>If you are uploading a new version of robocomp, change the version number  accordingly in the <a href="../CMakeLists.txt#L31">toplevel cmake</a> before building, and then upload the source package as mentioned.</p>

<p>###Note:</p>

<p>If you want to upload another source package to ppa which doesn’t have any changes in the source but maybe in the debian files. you can build the spackage after commenting out <code class="highlighter-rouge">set(DEB_SOURCE_CHANGES "CHANGED" CACHE STRING "source changed since last upload")</code> in <a href="../cmake/package_details.cmake#L27">package_details.cmake</a> so that the the script will only increase the ppa version number and won’t include the source package for uploading to ppa (which otherwise will give an error).</p>

<p>##Installing robocomp from ppa</p>

<p>First you will need to add the ppa in your sources, and then install robocomp package.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>sudo add-apt-repository ppa:&lt;lp-username&gt;/robocomp
sudo apt-get update
sudo apt-get install robocomp
</code></pre>
</div>

<p>this will install robocomp along with basic components into /opt/robocomp.</p>

<hr />
<p>Nithin Murali</p>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/website/page3">Older</a>
  
  
    
      <a class="pagination-item newer" href="/website">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
