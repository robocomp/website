<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      RoboComp &middot; 
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/website/public/css/poole.css">
  <link rel="stylesheet" href="/website/public/css/syntax.css">
  <link rel="stylesheet" href="/website/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/websitepublic/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/websitepublic/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body>

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
        <a href="/website">
          RoboComp
        </a>
      </h1>
      <p class="lead">A simple robotics framework.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/website">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/Blog/">Blog</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/GSoC15/">GSoC'15</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/GSoC16/">GSoC'16</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/projects/">Projects</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/about/">About</a>
          
        
      
        
      
        
          
            <a class="sidebar-nav-item" href="/website/contact/">Contact</a>
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/website/install/">Install</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
      <a class="sidebar-nav-item" href="http://robocomp.readthedocs.org">Tutorials</a>
      <a class="sidebar-nav-item" href="https://github.com/robocomp">GitHub project</a>
      
      <span class="sidebar-nav-item">Currently v1.0.0</span>
    </nav>

    <p>&copy; 2016. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <i><b>RoboComp</b> is an open-source Robotics framework providing the tools to create and modify software components that communicate through public interfaces. Components may require, subscribe, implement or publish interfaces in a seamless way. Building new components is done using two domain specific languages, IDSL and CDSL. With IDSL you define an interface and with CDSL you specify how the component will communicate with the world. With this information, a code generator creates C++ and/or Python sources, based on CMake, that compile and execute flawlessly. When some of these features have to be changed, the component can be easily regenerated and all the user specific code is preserved thanks to a simple inheritance mechanism.</i>


<hr>

<div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/06/22/gsoc15/">
        <i>GSoC,</i> 2015 ideas
      </a>
    </h1>

    <span class="post-date">22 Jun 2015</span>

    <p>1.- <strong>RoboComp tutorial, social management and documentation</strong>: RoboComp’ sources has been ported to GitHub and we are building a new documentation repository there. We are using GitHub markdown language (GFM) write new docs and turorials. We want to build a set of short tutorials that guide the new users along several interconnected topics, such as component oriented programming, robotics, computer vision, robotics software modules integrating heterogeneous sources, cognitive architectures and testing and validating, all from inside RoboComp. These new tutorials will be developed using RoboComp’s robotics simulator, RCIS, so interactive examples can be created and used in the explanations. This package also includes work on automated installation scripts using CMake. Generic knowledge of linux systems, website and wiki administration is needed. This is a key task for our project as it would bring more attention to it as it will open the development to new people interested in the field.</p>

<p>Required student level: intermediate programming and systems administration.</p>

<p>2.- <strong>Computer vision components and libraries management</strong>: RoboComp is being used to build a new cognitive architecture called RoboCog. Among the different modules already in progress, the object detection module is crucially important. We are pursuing an efficient 2D/3D vision pipeline that, working with the robot body control module, is able to localize, recognize, fit a pre-existing model and track a series of daily objects that the robot might encounter. Grasping would be one target of this pipeline, or even a means to complete recognition. There are currently many components implementing computer vision algorithms and intensive work done in pipeline construction. The key tasks on this idea would be to collaborate in the creation of high level tools to organize, document and test different pipelines for an specific task. These tools will be designed in collaboration with the mentors and tested in RoboComp’s RCIS simulator and real robots.</p>

<p>Required student level: intermediate computer vision knowledge, C++ programming, basic CMake knowledge</p>

<p>3.- <strong>RoboComp Building and deployment system design</strong>: Current CMake building system in RoboComp is limited only to the core libraries, the RCIS simulator and some additional tools. A very useful task would be to come up with a more complex CMake structure that could build the entire system, including all finished components, without breaking current dependencies. Also, a few scripts will have to be built to compile individual components, run tests, search and inspect the source tree efficiently, check dependencies and documentation requirements. This code needs also to take into account the dependencies between components that can be stored in xml-like files -i,e, manifestos- within the component itself.</p>

<p>Required student level: intermediate CMake knowledge, programming in C++, basic knowledge of shell scripting</p>

<p>4.- <strong>Deployment generator and run-time monitoring</strong>: When creating a specific robot architecture, many components have to be brought into a common deployment environment. Each component has its runtime configuration and network parameters that have to be declared in a common deployment file, from where the complete system can be brought to life. This task proposes the design of a domain specific language to facilitate the creation of shellscript deployment files that are syntactically and semantically correct. Once a net of RoboComp components is up and running, additional tools are needed to monitorize their execution through an existing default interface called CommonBehavior.This tool will use the DSL as input and will show a graphical representation of the running system. It will be written in Python and will extend important efforts already made in this direction.</p>

<p>Required student level: intermediate programming with Python and introductory knowledge of formal languages</p>

<p>For any questions, proposals, or comments please contact RoboComp’s org admin at:
marcogunex.es</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/06/20/nithin4/">
        Robocomp Workspace Model
      </a>
    </h1>

    <span class="post-date">20 Jun 2015</span>

    <p>The Robocomp workspace is for those who are developing components rather than the framework itself. The main advantage of having a workspace is that it will make the work-flow much easier. Workspace basically organizes the development. For example, currently for building or running a component you have to go to its directory, create a build directory and then use cmake, while by using workspace and build tools you could achieve the same in a single command.</p>

<p>I have started the workspace model design keeping in mind the following points.</p>

<ul>
  <li>you should be able to build all components at once, if necessary and also separately</li>
  <li>the source tree should be kept clean</li>
  <li>It should scalable and also existing components should be easily moved in</li>
  <li>dependencies should be easily handled</li>
</ul>

<p>Referring to other similar workspace models i came up with the following model.</p>

<p>###The recommended layout for development is as follows:</p>

<p><img src="https://raw.githubusercontent.com/robocomp/website/gh-pages/img/workspace_model.png" alt=" Robocomp workspace model" title="Robocomp workspace model" /></p>

<p>##Elements of workspace</p>

<p>###Workspace
The workspace is the folder inside which you are going to be actively developing components. Keeping things in a folder with connected development helps keep separation of development models. In simple words, a workspace can be thought of as a group different components, for example Robocomp has some default components, you may as well create some components so in this case you Robocomp’s components can be in a workspace while your components in another. The config file in ~/.config/RoboComp/rc_worksapce.config maintains a list of all the registered workspaces.</p>

<p>###Source space
The source space (a folder inside workspace) contains the source code of all the components in the workspace or this is where you will be developing. The source space is the folder where build tools will look for components. This folder is easily identified as it is where the toplevel.cmake is linked Robocomp installed folder and the name <code class="highlighter-rouge">src</code>. Each component should be in a direct subdirectory. If the directory contains a file named <em>IGNORE_COMP</em> the component will be ignored while building the workspace.</p>

<p>###Build Space
The build space is the folder in which cmake is invoked and generates artifacts such as the CMakeCache. This need not be a direct sub directory of workspace, it can be any where. This is basically an <em>build</em> directory of all the components.</p>

<p>###Development Space
The development space is where build system generates the binaries and config files which are executable before installation.It will have a septate directory for each components. Each component directory contains a folder <code class="highlighter-rouge">bin</code> which has the build binaries and a <code class="highlighter-rouge">etc</code> directory which contain config files. This should be a direct subdirectory of workspace. Currently the <code class="highlighter-rouge">devel space is merged with the source space</code> as you can seen in the layout graph.</p>

<p>###Install Space
This is the default directory in to which the components in current workspace will get installed along with generated docs. This directory contains a file named <em>.rc_install</em> which marks this as an install space. Please note that the robocomp install path <em>/opt/robocomp</em> is also an install space by default.</p>

<hr />
<p>Nithin Murali</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/06/17/mercedes3/">
        <i>GSoC,</i> Symbolic planning techniques for recognizing objects domestic <p>#3</p>, Visual Inverse Kinematics
      </a>
    </h1>

    <span class="post-date">17 Jun 2015</span>

    <p><strong>Visual inverse kinematics, Basic understanding :</strong> In the previous post we anticipate the problems caused by the gaps and inaccuracies of motors in the inverse kinematics of the robot. Now, in this third post we will talk about the solution implemented during the GSoC15 project.</p>

<p>So, with the inverse kinematics component that we have implemented in Robocomp, we had the problem of inaccuracies and gaps in the robotic arm motors, problems that made the robot believed reach the target position without having actually achieved it. To solve this problem it was decided to implement a solution inside the visual field (which is what concerns us throughout this project), whose aim is to provide the inverse kinematics component a visual feedback that allows correct its mistakes. The operation of the algorithm is very simple and takes as its starting point the investigations of Seth Hutchinson, Greg Hager and Peter Corke, collected in <code class="highlighter-rouge">A Tutorial on Visual Servo Control</code> [1].</p>

<p>##’Looking’, then ‘moving’</p>

<p>As Hutchinson, Hager and Corke reflect in their work:</p>

<blockquote>
  <p>Vision is a useful robotic sensor since it mimics the human sense of vision and allows for non-contact measurement of the environment. […] Typically visual sensing and manipulation are combined in a open-loop fashion, ‘looking’ then ‘moving’.</p>
</blockquote>

<p>So the goal of <code class="highlighter-rouge">Visual servo control</code> is to control the movement and location of the robot using visual techniques (detection and recognition of objects in an image). To get an idea how it works, we must have clear some fundamental concepts in this field</p>

<p>###Kinematics of a robot</p>

<p>We need to know what a kinematic chain is, what reference system and transformation coordinate are anda what algorithm is executed inside the robot kinematic. These concepts were explained in the second post of this collection. If you have doubts, consult it.</p>

<p>If we link the kinematic chains concept with visual techniques (ie, now, in addition to the chain formed by the motors of the robotic arm, we have a camera in the chain looking one of the chain ends), we have two types of systems:</p>

<ol>
  <li>Endpoint open-loop (EOL): Systems which only observed the target object. These systems don’t need to look at his end effector so normally the camera is on the end effector (hand-eye).</li>
  <li>Endpoint closed-loop (ECL): Systems which observed the target object and the end effector of the arm.</li>
</ol>

<p>The visual inverse kinematics that we implemented in Robocomp uses this last configuration because is independent of hand-eye calibration errors (precisely, the clearances errors and inaccuracies that bother us in the inverse kinematics), although often requires solution of a more demanding vision problem, because we need to track the end effector.</p>

<p>###Camera Projection Models</p>

<p>We need to understand the geometric aspects of the imaging process if we want to understand how the information provided by the vision system is used to control the movement of the robot. The first thing to consider is that an image taken by a camera is always in 2D, so we’re losing spatial information (the depth of the scene).</p>

<p><img src="http://masters.donntu.org/2012/etf/nikitin/library/article10.files/image10.01.png" alt="Alt text" /></p>

<p>To resolve this issue we have several options:
1. We can use multiple cameras that capture the studio space from different positions.
2. We can obtain multiple views with a single camera.
3. We can have previously stored the geometric relationship between certain characteristics of the target or the elements in the studio space.</p>

<p>In any case, we must keep in mind certain things common to all cameras. For example the system of axes: the <code class="highlighter-rouge">X</code> and <code class="highlighter-rouge">Y</code> axes form the basis of the image plane and the <code class="highlighter-rouge">Z</code> axis is perpendicular to the image plane, along the optical axis of the camera. The origin is located on the <code class="highlighter-rouge">Z</code> axis at a distance <code class="highlighter-rouge">λ</code> of the image plane. That distance <code class="highlighter-rouge">λ</code> is what we call focal length.</p>

<p><img src="http://www.hitl.washington.edu/artoolkit/documentation/images/ch03-17.gif" alt="ALt text" /></p>

<p>We can map the position and the orientation of the end effector in space calculating the projective geometry of the camera. But this method, complicated in itself, increases their difficulty because we need <code class="highlighter-rouge">recognize</code> the end effector in the picture, in addition to deriving the speed from the changes observed in each frame that the camera capture. For these reasons, in our visual inverse kinematic component, we use the algorithm proposed by Edwin Olson, <code class="highlighter-rouge">Apriltags</code> [2] a visual fiducial system that uses a 2D barcode style <code class="highlighter-rouge">tag</code> (binary, black and white synthetic brands), allowing full 6 DOF localization of features from a single image. Thus, if we put a apriltag in the end effector, we can get its position and orientation in a very simple way.</p>

<p>##visualBIK component</p>

<p>Having already some clear concepts, let us study how the component developed in this project, <code class="highlighter-rouge">visualBIK</code>, works.</p>

<p>Our component implements a simple state machine where waits the reception of a target position (a vector with traslations and rotations: [tx, ty, tz,    rx, ry, rz]) through its interface. When a target is received, the visualBIK send it to the inverse kinematics component like a <code class="highlighter-rouge">POSE6D</code> target, and waits for him to finish running the target and placing the arm. As the end effector will be a little out of the target position (due to inaccuracies), the visualBIK will be prepared to correct this error:</p>

<ol>
  <li>It calculates the visual pose of the end effector (through apriltags, visualBIK receives the position of the end effector mark that the camera head sees).</li>
  <li>After, it compute the error vector between the visual pose and the target pose.</li>
  <li>With this error vector, visualBIK corrects the target pose and sends the new position to the inverse kinematics component.</li>
  <li>This process is repeated until the error achieved in translation and rotation is less than a predetermined threshold.</li>
</ol>

<p>In this way we can correct the errors introduced by the inaccuracies of the joints.</p>

<p>This component (like component inverse kinematics) is still in the testing phase and is more than likely suffer some changes that improve its operation.</p>

<p>Bye!</p>

<hr />
<p>[1] Hutchinson, S., Hager, G., Corke, P. <code class="highlighter-rouge">A Tutorial on Visual Servo Control</code>, IEEE Trans. Robot. Automat., 12(5):651–670, Oct. 1996. Download in http://www-cvr.ai.uiuc.edu/~seth/ResPages/pdfs/HutHagCor96.pdf</p>

<p>[2] OLson, E. <code class="highlighter-rouge">AprilTag: A robust and flexible visual fiducial system</code>, Robotics and Automation (ICRA), 2011 IEEE International Conference on, 3400-3407</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/06/15/mercedes2/">
        <i>GSoC,</i> Symbolic planning techniques for recognizing objects domestic <p>#2</p>, Inverse Kinematics
      </a>
    </h1>

    <span class="post-date">15 Jun 2015</span>

    <p><strong>What is inverse kinematics?</strong> : In this second post, although it may seem begin the house from the roof, let’s talk about how a robot moves its arms and hands in order to manipulate daily objects.</p>

<p>The ultimate goal of this work is make the robot to be able to recognize certain daily objects in a house (for example a mug), and to manipulate these objects with its effectors (hands). To do this, one of the things we need to implement is the inverse kinematics of the robot. Although this is the last step, we start by inverse kinematics to be easier and more intuitive than object recognition (besides that we have almost finalized the cinematic component in Robocomp).</p>

<p>###What does the inverse kinematics?</p>

<p>A recurring problem in robotics is to give to robots a certain autonomy in terms of movement. Focusing on a practical and realistic example, as is the trajectory of a robotic arm from an initial position to a target point, the question is how does the robot move its arm from the starting pose to the final pose? or what values take its engines arm to reach the final position? This is the typical problem of inverse kinematics, which is responsible for calculating the angular values of a kinematic chain composed engines (joints) of the arm to reach a target position.</p>

<p>But before we get down to work, we need to review a few concepts.</p>

<p>###Previous concepts</p>

<p>####kinematic chains</p>

<p>The first concept that we should be clear is the <code class="highlighter-rouge">kinematic chain</code>. The kinematic chain is a set of elements that produce motion, deforming the chain to adapt it to movement. Kinematic chains are composed of two elements:</p>

<ol>
  <li><code class="highlighter-rouge">joints</code>: joints or motors that produce the movement. Each joint gives a degree of freedom.</li>
  <li><code class="highlighter-rouge">links</code>: rigid segments that connect the joints together.</li>
</ol>

<p>An example of kinematic chain in robotic is the arm of the robot, that is composed by all the motors that the robot has and the segments that connect this motors in order to create the arm form.</p>

<p><img src="http://www.sitenordeste.com/mecanica/images/cadena_cinematica.JPG" alt="Alt text" /></p>

<p>####Reference systems and Transformation coordinate.</p>

<p>One of the problems of robot manipulators is to know where their structural elements are arranged in the space in which they move. We therefore need a referral system that puts or position the elements of the robot in the workspace. So, a <code class="highlighter-rouge">reference system</code> is a set of agreements or conventions used by an observer to measure positions, rotations and other physical parameters of the system being studied. In our case, the arm of the robot is into the three-dimensional workspace (R³, with the axis X, Y and Z), where each components (for example, each joint) has one traslation (tx, ty, tz) and one rotation (rx, ry, rz). Therefore, the position of each component is given by a vector of six elements: <code class="highlighter-rouge">P=[tx, ty, tz,   rx, ry, rz]</code> (the first three translational and three rotational recent). Normally, we represent the poses by homogeneous trasnformation matrices, which are of the form:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>    | R  T |
P = | 0  1 |
</code></pre>
</div>

<p>where <code class="highlighter-rouge">R</code> is the rotation matrix and <code class="highlighter-rouge">T</code> the traslation coordenates.</p>

<p>One of the kinematic problems is that each motor (which can be moved and/or rotated with respect to the previous motor of the chain) has his own reference system, so if we want to calculate the position of a particular point or joint, we will have to make a number of changes (<code class="highlighter-rouge">transformations</code>) to move from one reference system to another. For example, if we have the newt arm:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>X_1--------------X_2--------X_3-----O
       M_1&gt;2          M_2&gt;3    M_3&gt;O
</code></pre>
</div>

<p>where <code class="highlighter-rouge">X_n</code> represents the position of the joints, <code class="highlighter-rouge">-</code> is the link that connects the joints, <code class="highlighter-rouge">o</code> is the end effector of the arm and <code class="highlighter-rouge">M_n&gt;m</code> are the transformation matrices to change the reference system n to the system m, and we want to calculate the position of the end effector in the reference system of the joint X_1, we have to calculate this equation:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Po_inX_1 = M_2&gt;1 * M_3&gt;2 * M_o&gt;3 * Po_inO.
</code></pre>
</div>

<p>###Problems to solve the inverse kinematics.</p>

<p>If the forward kinematics is responsible for calculating the position of the end effector in a kinematic chain, given some angular values for the joints, the inverse kinematics is just the opposite: it is responsible for calculating the angle values of the joints so the end effector reaches a position. This last problem is much more difficult to solve. So difficult that we are forced to use generic mathematical methods that try to approach an optimal solution iteratively within a reasonable time. We have opted for an iterative method known as the <code class="highlighter-rouge">Levenberg-Marquardt</code> or <code class="highlighter-rouge">damped least squares</code> algorithm. This method is used for solving nonlinear least squares problems where a solution to decrease an error function is sought.</p>

<p>###Inverse kinematics in Robocomp</p>

<p>As a result of the TFG, <code class="highlighter-rouge">Inverse kinematics in Social Robots</code> [1], since 2014 Robocomp has a component [2] that is responsible for calculating the inverse kinematics of the social robot Ursus [3], developed by Robolab. This component has undergone a big evolution, since it was created last year to now, and is more than likely to continue evolving to achieve inverse kinematics each finer and in less time.</p>

<p>Originally, this component receives three types of targets:</p>

<ol>
  <li>POSE6D: It is the typical target with translations and rotations in the X, Y and Z axis. The end effector has to be positioned at coordinates (tx, ty, tz) of the target and align their rotation axes with the target, specified in (rx, ry, rz).</li>
  <li>ADVANCEAXIS: its goal is to move the end effector of the robot along a vector. This is useful for improving the outcome of the above problem, for example, imagine that the hand has been a bit away from a mug. With this feature we can calculate the error vector between the end effector and the mug, and move the effector along the space to place it in an optimal position, near the mug.</li>
  <li>ALIGNAXIS: Its goal is that the end effector is pointing to target without moving to it but rotated as the target. It may be useful in certain cases where we are more interested in oriented the end effector with the same rotation of the target.</li>
</ol>

<p>To solve these various inverse kinematic problems, the component uses as main base the <code class="highlighter-rouge">Levenberg-Marquardt</code> algorithm proposed in the article <code class="highlighter-rouge">SBA: A Software Package for Generic Sparse Bundle Adjustment</code> by Lourakis and Argyros:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Input: A vector functon f: R^m → R^n with n≥m, a measurement vector x ∈ R^n and an initial parameters estimate p_0 ∈ R^m.
Output: A vector p+ ∈ R^m minimizing ||x-f(p)||^2.
Algorithm:
    k:=0;                 v:=2;                     p:=p0;
    A:=transposed(J)·J;   error:=x-f(p);            g:=transposed(J)·error;
    stop:=(||g||∞ ≤ ε1);  μ:=t*max_i=1,...,m (Aii)
    
    while(!stop) and (k&lt;k_max)
         k:=k+1;
         repeat
               SOLVE (A+μ·I)·δ_p=g;
               if(||δ_p||≤ ε2·(||p||+ε2))
                    stop:=true;
               else
                    p_new:=p+δ_p
                    ρ:=(||error||^2-||x-f(p_new)||^2)/(transposed(δ_p)·(μ·δ_p+g));
                    if ρ&gt;0
                        stop:=(||error||-||x-f(p_new)||&lt;ε4·||error||);
                        p:=p_new;
                        A:=transposed(J)·J;    error:=x-f(p);    g:=transposed(J)·error;
                        stop:=(stop) or (||g||∞ ≤ ε1);
                        μ:=μ*max(1/3, 1-(2·ρ-1)^3);
                        v:=2;
                    else
                        μ:=μ*v;
                        v:=2*v;
                    endif
               endif
         until(ρ&gt;0) or (stop)
         stop:=(||error||≤ ε3);
    endwhile
    p+:=p;
</code></pre>
</div>

<p>Where <code class="highlighter-rouge">A</code> is the hessian matrix, <code class="highlighter-rouge">J</code> is the jacobian matrix, <code class="highlighter-rouge">g</code> is the gradient descent, <code class="highlighter-rouge">δ_p</code> is the increments, <code class="highlighter-rouge">ρ</code> is the ratio of profit that tells us if we are approaching a minimum or not, <code class="highlighter-rouge">μ</code> is the damping factor, and <code class="highlighter-rouge">t</code> and <code class="highlighter-rouge">ε1, ε2, ε3, ε4</code> are different thresholds. But the IK component of Robocomp adds several concepts to the original L-M algorithm, in order to complete the proper operation of the component:</p>

<ol>
  <li>Weight matrix: that controls the relevance between the translations (in meters) and rotations (in radians) of the target. So, where <code class="highlighter-rouge">g</code> was calculated as <code class="highlighter-rouge">transposed(J)·error</code>, now <code class="highlighter-rouge">g</code> is <code class="highlighter-rouge">transposed(J)·(W·error)</code></li>
  <li>Motors lock: when a motor reachs its minimun or maximun limit, we modified the jacobian matrix.</li>
</ol>

<p>The new version of the inverse kinematics component simplifies the code of the old version and adds some more functionality:</p>

<ol>
  <li>Executes more than once a target. The inverse kinematic result is not the same if the start point of the effector is the robot’s home or a point B near tho the goal point.</li>
  <li>Executes the traslations without the motors of the wrisht (only for Ursus). This makes possible to move the arm with stiff wrist, and then we can rotate easely the wrist when the end effectos is near the target.</li>
</ol>

<p>Another improvement being studied is to include a small planner responsible for planning the trajectories of the robot arm, in order to facilitate the work of the IK component and reduce its execution time. However, one of the problems that the inverse kinematics can not solve by itself is the problem of gaps and imperfections of the robot. These gaps and inaccuracies make the robot move its arm toward the target position improperly, so that the robot “thinks” that the end effector has reached the target but in reality has fallen far short of the target pose.</p>

<p>In order to solve this last problem, we need visual feedback to correct the errors and mistakes introduced for the gaps and inaccuracies in the kinematic chain. The visualBIK component, developed during this project, is responsible for solve this visual feedback and correct the inverse kinematic, but we’ll talk about it in the next post.</p>

<p>Bye!</p>

<hr />
<p>[1] Master Thesis, Universidad de Extremadura, Escuela Politécnica de Cáceres. Mercedes Paoletti Ávila. <code class="highlighter-rouge">Cinemática Inversa en Robots Sociales</code>. Directed by Pablo Bustos and Luis Vicente Calderita. July 2014. Download in https://robolab.unex.es/index.php?option=com_remository&amp;Itemid=53&amp;func=startdown&amp;id=143</p>

<p>[2] inverse kinematics component repository: https://github.com/robocomp/robocomp-ursus/tree/master/components/inversekinematics</p>

<p>[3] C. Suárez Mejías, C. Echevarría, P. Núñez, L. Manso, P. Bustos, S. Leal and C. Parra. <code class="highlighter-rouge">Ursus: A Robotic Assistant for Training of Patients with Motor Impairments</code>. Book, Converging Clinical and Engineering Research on Neurorehabilitation, Springer series on BioSystems and BioRobotics, Editors, J.L Pons, D. Torricelli and Marta Pajaro. Springer, ISBN 978-3-642-34545-6, pages 249-254. January 2012. Download in https://robolab.unex.es/index.php?option=com_remository&amp;Itemid=53&amp;func=startdown&amp;id=128</p>

<p>[4] Lourakis, M. I., Argyros, A. (2009). <code class="highlighter-rouge">SBA: A Software Package for Generic Sparse Bundle Adjustment</code>. Article of ACM Transactions on Mathematical Software, volume 36, issue 1, pages 1-30. Download in http://doi.acm.org/10.1145/1486527</p>


  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/website/2015/06/12/nithin8/">
        Debian Packaging 2
      </a>
    </h1>

    <span class="post-date">12 Jun 2015</span>

    <p>##What is a binary package?
A binary package in a is an application package which contains (pre-built) executables, as opposed to source code. Basically a binary package is an archive which contains executables some other info like rules on how to install them, dependencies etc. debian binary package is also a type of binary package. You can use a package manger to install these packages.I have explained I have explained it in tutorial <a href="http://robocomp.github.io/website/2015/05/23/nithin1.html">Debian packaging</a>.</p>

<p>##Implementation in Robocomp
For binary packages i was left with two options. whether i could use the same cmake script i used for creating source packages or i could use the <code class="highlighter-rouge">CPACK</code> packaging tool. Finally i decided to go with CPACK because - 1)less code , that means less messing up 2)its an well known tool so is expected to perform better than a script i am writing. CPACK has so many configuration options so i made a seperate cmake file <code class="highlighter-rouge">package_details.cmake</code> for configuring cpack so that its easier for users to change any configuration. CPACK will add a target <code class="highlighter-rouge">package</code> for generating binary package. so you could run <code class="highlighter-rouge">make package</code> for generating the package.</p>

<p>##Source packages and ppa
A Personal Package Archive (PPA) s a special software repository for uploading source packages to be built and published as an APT repository by Launchpad.So basically if a software has a ppa then users can just add the pa to their sources and they will be able to install the software package and will also get updates automatically. As alreasy mentioned we can only upload source packages into a ppa, by definition <em>Source packages provide you with all of the necessary files to compile or otherwise, build the desired piece of software.</em> now the next question is how can we create source packages. I have explained it in tutorial <a href="http://robocomp.github.io/website/2015/05/23/nithin1.html">Debian packaging</a>.</p>

<p>##Implementation in Robocomp
For creating source package for robocomp i wrote a cmake module <em>source_package</em>.The module will basically copy the source in to another directory (currently <em>Debian</em> in build folder) and will create the source tar.Then it will create all debian/ files dynamically. The script will be executed when the target <code class="highlighter-rouge">spackage</code> is made.</p>

<p>After creating the source packages one trouble i faces was in setting up (registering) the PGP keys. Once you have created a launchpad account you should sign the Ubuntu Code of Conduct.Then you can upload the package using <code class="highlighter-rouge">dput</code> utility.</p>

<p>###NB
* Unfortunately CPACK has a bug in it, its not changing the control files permission correctly which throws a warning during installation. so i have create a bash script which will fix the control file permissions.</p>

<ul>
  <li>
    <p>You cant upload a package with same name into same ppa again, launchpad will reject it. so you need to change the package name and hence the version, every time you upload. This is automatically taken care off by the script.</p>
  </li>
  <li>
    <p>If there are any changes to the source, then you should upload the whole source into the ppa. Well, any changes to fies in <em>debian</em> folder is not considered a source change. In Robocomp its implemented in such a way that if there is any changes to the source, then you have to change the Robocomp version.</p>
  </li>
  <li>
    <p>Right now we are not generating the changelog automatically. But that is a feature we could add, generating changelog from the git commit messages.</p>
  </li>
</ul>

<hr />
<p>Nithin Murali</p>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/website/page5">Older</a>
  
  
    
      <a class="pagination-item newer" href="/website/page3">Newer</a>
    
  
</div>
    </div>

  </body>
</html>
